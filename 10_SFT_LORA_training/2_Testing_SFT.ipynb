{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57b97d6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbba44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, torch, re, io, contextlib, sys, warnings\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer, AddedToken\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16670a56",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb61c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN = Path(\"../fine_tuning_small/checkpoints\") # if you're testing small one, comment out this one and comment the below one\n",
    "RUN = Path(\"../fine_tuning/checkpoints\")\n",
    "\n",
    "BASE = Path(\"../pretrain_checkpoint/best_base.pt\") # --base-ckpt used for SFT\n",
    "#BEST = RUN / \"lora_best\" # (or lora_best_ema if you prefer)\n",
    "BEST = RUN / \"lora_best_ema\"\n",
    "\n",
    "\n",
    "CONF = Path(\"../configs/model_config_124M_sft.json\")\n",
    "VOCAB = Path(\"../bpe/bpe_model-vocab.json\")\n",
    "MERGES = Path(\"../bpe/bpe_model-merges.txt\")\n",
    "\n",
    "\n",
    "# for sanity check (we did this but i had a problem with generated output)\n",
    "SFT_IDS = Path(\"../final_npy/train_input_ids.npy\")\n",
    "SFT_MSK = Path(\"../final_npy/train_loss_mask.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b461c72",
   "metadata": {},
   "source": [
    "# ENV configuration stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b099e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1e6567597f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "else:\n",
    "    AMP_DTYPE = torch.float32\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ed973",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776f120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_decode = ByteLevelBPETokenizer(str(VOCAB), str(MERGES), lowercase=False, add_prefix_space=True)\n",
    "bpe_prompt = ByteLevelBPETokenizer(str(VOCAB), str(MERGES), lowercase=False, add_prefix_space=True)\n",
    "\n",
    "# read IDs from the tokenizer; do not add/append anything\n",
    "SPECIALS = [\"<|PAD|>\", \"<|UNKNOWN|>\", \"<|START|>\", \"<|END|>\",\n",
    "            \"<|SYSTEM|>\", \"<|INFOSTART|>\", \"<|INFOEND|>\", \"<|USER|>\", \"<|ASSISTANT|>\"]\n",
    "SID = {t: bpe_decode.token_to_id(t) for t in SPECIALS}\n",
    "\n",
    "# hard assertions to catch if there's anything wrong\n",
    "assert SID[\"<|PAD|>\"] == 0, f\"PAD_ID mismatch: {SID['<|PAD|>']}\"\n",
    "assert SID[\"<|END|>\"] == 3, f\"END_ID mismatch: {SID['<|END|>']}\"\n",
    "assert SID[\"<|ASSISTANT|>\"] == 6, \"ASSISTANT_ID mismatch\"\n",
    "\n",
    "PAD_ID = SID[\"<|PAD|>\"]\n",
    "END_ID = SID[\"<|END|>\"]\n",
    "ASSISTANT_ID = SID[\"<|ASSISTANT|>\"]\n",
    "for _tag in (\"<|SYSTEM|>\", \"<|USER|>\", \"<|ASSISTANT|>\"):\n",
    "    assert SID[_tag] is not None, f\"Missing token in vocab: {_tag}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd75f1",
   "metadata": {},
   "source": [
    "# Model config & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8555d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../python_files\")\n",
    "from Dummy_Model_sft import DummyModel\n",
    "\n",
    "# Accepts raw dict or {'model':...}/{ 'state_dict':... } and strips common prefixes\n",
    "def load_state_dict_safely(model: torch.nn.Module, raw_state, strict: bool=False, verbose: bool=True):\n",
    "    state = raw_state.get(\"model\", raw_state.get(\"state_dict\", raw_state))\n",
    "    clean = {}\n",
    "    for k, v in state.items():\n",
    "        if k.startswith(\"module.\"): k = k[7:]\n",
    "        if k.startswith(\"model.\"): k = k[6:]\n",
    "        clean[k] = v\n",
    "    missing, unexpected = model.load_state_dict(clean, strict=strict)\n",
    "    if verbose:\n",
    "        if missing: print(f\"[load] missing ({len(missing)}): {missing[:5]} ...\")\n",
    "        if unexpected: print(f\"[load] unexpected ({len(unexpected)}): {unexpected[:5]} ...\")\n",
    "        if not missing and not unexpected:\n",
    "            print(\"[load] state dict loaded cleanly.\")\n",
    "    return model\n",
    "\n",
    "cfg = json.loads(Path(CONF).read_text())\n",
    "\n",
    "\n",
    "def _safe_torch_load(path: Path):\n",
    "    try:\n",
    "        return torch.load(path, map_location=DEVICE, weights_only=True)\n",
    "    except TypeError:\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"You are using `torch.load` with `weights_only=False`\",\n",
    "            category=FutureWarning,\n",
    "        )\n",
    "        return torch.load(path, map_location=DEVICE)\n",
    "\n",
    "\n",
    "def _load_base_only() -> torch.nn.Module:\n",
    "    base = DummyModel(cfg).to(DEVICE).eval()\n",
    "    raw  = _safe_torch_load(BASE)\n",
    "    load_state_dict_safely(base, raw, strict=False, verbose=True)\n",
    "    return base\n",
    "\n",
    "\n",
    "def load_with_adapter(adapter_dir: Path, merge: bool=False) -> torch.nn.Module:\n",
    "    base = _load_base_only()\n",
    "    model = PeftModel.from_pretrained(base, str(adapter_dir), is_trainable=False).to(DEVICE).eval()\n",
    "    lp = [n for n,_ in model.named_parameters() if \"lora_\" in n]\n",
    "    print(f\"[PEFT] attached → LoRA tensors: {len(lp)}\")\n",
    "    if merge:\n",
    "        model.merge_and_unload()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ae3d5",
   "metadata": {},
   "source": [
    "# Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203bff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be exactly the same we used when training for SFT\n",
    "SYSTEM_PROMPT_DEFAULT = (\n",
    "    \"Be a helpful, concise assistant with a light, friendly tone. \"\n",
    "    \"Answer directly in 1–3 sentences. Don’t use steps or bullet lists unless the user asks. \"\n",
    "    \"Use the content between INFOSTART/INFOEND only as context. Do not mention it, ‘memory,’ or any internal tags. \"\n",
    "    \"Avoid speculation and say when unsure. Keep replies safe, accurate, and on-topic.\"\n",
    ")\n",
    "\n",
    "# autodetect whether training had leading space/newline after <|ASSISTANT|>\n",
    "def _detect_assistant_prefix(ids_path: Path, sample_k: int = 256) -> str:\n",
    "    if not ids_path.exists():\n",
    "        return \"\"\n",
    "    arr = np.load(ids_path, mmap_mode=\"r\")\n",
    "    spaces = newlines = letters = 0\n",
    "    for i in range(min(sample_k, arr.shape[0])):\n",
    "        row = arr[i].tolist()\n",
    "        txt = bpe_decode.decode([t for t in row if t != PAD_ID])\n",
    "        j = txt.find(\"<|ASSISTANT|>\")\n",
    "        if j == -1:\n",
    "            continue\n",
    "        j += len(\"<|ASSISTANT|>\")\n",
    "        ch = txt[j:j+1]\n",
    "        if ch == \" \": spaces += 1\n",
    "        elif ch == \"\\n\": newlines += 1\n",
    "        elif ch: letters += 1\n",
    "    if spaces >= max(newlines, letters): return \" \"\n",
    "    if newlines > 0: return \"\\n\"\n",
    "    return \"\"\n",
    "\n",
    "ASSIST_PREFIX = _detect_assistant_prefix(SFT_IDS)\n",
    "\n",
    "\n",
    "def build_prompt_ids(user_text: str, info_text: str, system_text: str = SYSTEM_PROMPT_DEFAULT):\n",
    "    \"\"\"\n",
    "    Builds:\n",
    "      <|START|><|SYSTEM|>{SYSTEM}\\n\n",
    "      <|INFOSTART|>{INFO}<|INFOEND|>\\n\n",
    "      <|USER|>{USER}\\n\n",
    "      <|ASSISTANT|>{ASSIST_PREFIX}\n",
    "    \"\"\"\n",
    "    txt = (\n",
    "        \"<|START|><|SYSTEM|>\" + system_text + \"\\n\"\n",
    "        \"<|INFOSTART|>\" + (info_text if info_text else \"\") + \"<|INFOEND|>\\n\"\n",
    "        \"<|USER|>\" + user_text.strip() + \"\\n\"\n",
    "        \"<|ASSISTANT|>\" + ASSIST_PREFIX\n",
    "    )\n",
    "    return bpe_prompt.encode(txt, add_special_tokens=False).ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c55b7",
   "metadata": {},
   "source": [
    "# Prepare Sampling #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4847261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOS stop + small sentence cap + strong tag-leak/style guards\n",
    "SENTENCE_LIMIT = 3\n",
    "MIN_BEFORE_EOS = 1\n",
    "EOS_BOOST = 2.0\n",
    "\n",
    "_STEP_PAT = re.compile(r\"(?i)\\bstep\\s*\\d+\")\n",
    "_SENT_PAT = re.compile(r\"(?i)(identify the (given )?sentence|the sentence is)\")\n",
    "\n",
    "_SENT_END_RE = re.compile(r'[.!?](?=\\s|$)')\n",
    "\n",
    "def _count_sents(s: str) -> int:\n",
    "    return len(_SENT_END_RE.findall(s))\n",
    "\n",
    "\n",
    "def _trim_sents(s: str, limit: int) -> str:\n",
    "    if limit <= 0: return \"\"\n",
    "    if _count_sents(s) <= limit: return s.strip()\n",
    "    idx = 0; kept = 0\n",
    "    for m in _SENT_END_RE.finditer(s):\n",
    "        kept += 1; idx = m.end()\n",
    "        if kept >= limit: break\n",
    "    return s[:idx].strip()\n",
    "\n",
    "VOCAB_SIZE = int(cfg.get(\"vocab_size\", bpe_decode.get_vocab_size()))\n",
    "TOKEN_STR = {i: bpe_decode.decode([i]) for i in range(VOCAB_SIZE)}\n",
    "\n",
    "# guard tag words tolerant to separators/mixed case\n",
    "_TAG_PAT = re.compile(\n",
    "    r\"(?i)(i[^a-z0-9]?n[^a-z0-9]?f[^a-z0-9]?o[^a-z0-9]?s[^a-z0-9]?t[^a-z0-9]?a[^a-z0-9]?r[^a-z0-9]?t\"\n",
    "    r\"|i[^a-z0-9]?n[^a-z0-9]?f[^a-z0-9]?o[^a-z0-9]?e[^a-z0-9]?n[^a-z0-9]?d\"\n",
    "    r\"|s[^a-z0-9]?y[^a-z0-9]?s[^a-z0-9]?t[^a-z0-9]?e[^a-z0-9]?m\"\n",
    "    r\"|u[^a-z0-9]?s[^a-z0-9]?e[^a-z0-9]?r\"\n",
    "    r\"|a[^a-z0-9]?s[^a-z0-9]?s[^a-z0-9]?i[^a-z0-9]?s[^a-z0-9]?t[^a-z0-9]?a[^a-z0-9]?n[^a-z0-9]?t\"\n",
    "    r\"|s[^a-z0-9]?t[^a-z0-9]?a[^a-z0-9]?r[^a-z0-9]?t\"\n",
    "    r\"|e[^a-z0-9]?n[^a-z0-9]?d)\"\n",
    ")\n",
    "\n",
    "_TAG_WORDS = {\"info\", \"start\", \"end\", \"system\", \"user\", \"assistant\", \"infost\", \"fost\"}\n",
    "\n",
    "\n",
    "def _would_form_taglike(prev_tail: str, cand_str) -> bool:\n",
    "    cand = cand_str if isinstance(cand_str, str) else str(cand_str)\n",
    "    s = (prev_tail + cand)[-96:]\n",
    "    if \"<\" in s or \">\" in s or \"|\" in s:\n",
    "        return True\n",
    "    return bool(_TAG_PAT.search(s))\n",
    "\n",
    "\n",
    "def _has_emoji_or_symbol(s: str) -> bool:\n",
    "    return any(\n",
    "        (0x1F300 <= ord(ch) <= 0x1FAFF) or\n",
    "        (0x2600  <= ord(ch) <= 0x26FF) or\n",
    "        (0x2700  <= ord(ch) <= 0x27BF)\n",
    "        for ch in s\n",
    "    )\n",
    "\n",
    "\n",
    "# ban tokens likely to spark tag leakage or junk style\n",
    "def _build_ban_ids() -> set[int]:\n",
    "    bad = set()\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        s  = TOKEN_STR[i]\n",
    "        sl = s.lower()\n",
    "        if (\"<\" in s) or (\">\" in s) or (\"|\" in s):\n",
    "            bad.add(i); continue\n",
    "        if _has_emoji_or_symbol(s):\n",
    "            bad.add(i); continue\n",
    "        ns = s.replace(\" \", \"\")\n",
    "        if (\"#\" in ns) or (\"http\" in sl) or (\"www\" in sl) or (\".com\" in sl) or (\".net\" in sl) or (\".org\" in sl):\n",
    "            bad.add(i); continue\n",
    "        \n",
    "        # also ban tokens that *by themselves* look like tag words or their stubs\n",
    "        if _TAG_PAT.fullmatch(sl):\n",
    "            bad.add(i); continue\n",
    "    bad.discard(END_ID) # never ban EOS\n",
    "    bad.discard(PAD_ID) # never ban PAD\n",
    "    return bad\n",
    "\n",
    "\n",
    "def _violates_ngram(seq, cand, n):\n",
    "    if n <= 1 or len(seq) < n-1: return False\n",
    "    s = seq + [cand]; last = tuple(s[-n:])\n",
    "    for i in range(len(s)-n):\n",
    "        if tuple(s[i:i+n]) == last:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# negation-aware steering\n",
    "NEG_WINDOW_TOKENS = 6\n",
    "NEG_CONT_PENALTY  = 0.8\n",
    "POS_PIVOT_BONUS   = 0.6\n",
    "\n",
    "_NEG_HIT_RE = re.compile(r\"\\b(?:not|no|never|cannot|can['’]t|won['’]t|don['’]t|doesn['’]t|didn['’]t|isn['’]t|aren['’]t|wasn['’]t|weren['’]t)\\b\", re.I)\n",
    "\n",
    "_NEG_CONT_PHRASES = [\n",
    "    \" I can't\", \" I cannot\", \" can't\", \" cannot\", \" unable to\", \" I won't\", \" I do not\",\n",
    "    \" I don't\", \" I am not able\", \" unfortunately\", \" sorry\", \" I cannot help\",\n",
    "]\n",
    "_POS_PIVOTS = [\n",
    "    \" but\", \" however\", \" still\", \" instead\", \" here's\", \" here is\",\n",
    "    \" I can\", \" we can\", \" let's\", \" try\", \" you can\",\n",
    "]\n",
    "\n",
    "def _ids_for_phrases(phrases):\n",
    "    out = []\n",
    "    for p in phrases:\n",
    "        ids = bpe_decode.encode(p, add_special_tokens=False).ids\n",
    "        if ids:\n",
    "            out.append(ids)\n",
    "    return out\n",
    "\n",
    "NEG_CONT_IDS = _ids_for_phrases(_NEG_CONT_PHRASES)\n",
    "POS_PIVOT_IDS = _ids_for_phrases(_POS_PIVOTS)\n",
    "MAX_NEG_CONT = max((len(s) for s in NEG_CONT_IDS), default=0)\n",
    "MAX_POS_PVT  = max((len(s) for s in POS_PIVOT_IDS), default=0)\n",
    "\n",
    "def _would_complete_any(prev_ids: list[int], cand_id: int, seqs: list[list[int]], max_len: int) -> bool:\n",
    "    if not seqs:\n",
    "        return False\n",
    "    ctx = prev_ids[-(max_len-1):] if max_len > 1 else []\n",
    "    test = ctx + [cand_id]\n",
    "    for seq in seqs:\n",
    "        L = len(seq)\n",
    "        if len(test) >= L and test[-L:] == seq:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e4d7b",
   "metadata": {},
   "source": [
    "# Prepare Sampling #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293eefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper used mid-loop to block unsafe tails before they fully appear\n",
    "def _unsafe_tail_would_appear(seq, candidate_id):\n",
    "    test = bpe_decode.decode(seq + [candidate_id])\n",
    "    tail = test[-128:].lower()\n",
    "\n",
    "    if (\"<\" in tail) or (\">\" in tail) or (\"|\" in tail):\n",
    "        return True\n",
    "\n",
    "    # also block tag-like substrings without brackets (e.g., \"fostart\", \"infostart\")\n",
    "    if _TAG_PAT.search(tail):\n",
    "        return True\n",
    "    if any(w in tail for w in _TAG_WORDS | {\"fostart\", \"infostart\", \"infoend\"}):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# generating token ids\n",
    "@torch.no_grad()\n",
    "def generate_ids(model, prompt_ids, user_words: set[str] | None = None,\n",
    "                 max_new_tokens=120, temperature=0.0, top_p=1.0, top_k=0,\n",
    "                 repetition_penalty=1.07, no_repeat_ngram_size=3, seed=None,\n",
    "                 ban_token_ids: set[int] | None = None,\n",
    "                 eos_token_id=None, pad_token_id=None, generator=None,\n",
    "                 first_token_boost: dict[int, float] | None = None):\n",
    "\n",
    "    dev = next(model.parameters()).device\n",
    "    STOP = int(eos_token_id if eos_token_id is not None else END_ID)\n",
    "    PAD  = int(pad_token_id if pad_token_id is not None else PAD_ID)\n",
    "    g = generator if generator is not None else (torch.Generator(device=dev).manual_seed(int(seed)) if seed is not None else None)\n",
    "\n",
    "    x = torch.tensor([prompt_ids], dtype=torch.long, device=dev)\n",
    "    gen = []\n",
    "    neg_window = 0\n",
    "    min_before_eos = MIN_BEFORE_EOS\n",
    "    eos_boost      = EOS_BOOST\n",
    "    user_word_tids = None\n",
    "    if user_words:\n",
    "        user_word_tids = {\n",
    "            tid for tid, s in TOKEN_STR.items()\n",
    "            if s and (ss := s.strip().lower()) in user_words and 3 <= len(ss) <= 12\n",
    "        }\n",
    "\n",
    "    # discourage scaffolds unless the prompt explicitly asks\n",
    "    want_steps = False\n",
    "    if user_words:\n",
    "        hints = {\"steps\", \"step\", \"bullet\", \"list\", \"instructions\", \"how-to\", \"procedure\", \"outline\"}\n",
    "        want_steps = any(h in user_words for h in hints)\n",
    "\n",
    "    step_tids = None\n",
    "    if not want_steps:\n",
    "        # catches \"Step 1:\" and simple bullets\n",
    "        candidates = {\"step\", \"Step\", \":\", \"1\", \"2\", \"3\", \"-\", \"•\", \"Identify\", \"sentence\"}\n",
    "        step_tids = {tid for tid, s in TOKEN_STR.items() if s and s.strip() in candidates}\n",
    "\n",
    "    # nudge away from \"the sentence is\" rut\n",
    "    tsi_tids = {tid for tid, s in TOKEN_STR.items() if s and s.strip().lower() in {\"the\", \"sentence\", \"is\"}}\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=(dev.type==\"cuda\")):\n",
    "            logits, _ = model(input_ids=x)\n",
    "        next_logits = logits[:, -1, :].float()\n",
    "        if first_token_boost and len(gen) == 0:\n",
    "            for tid, bonus in first_token_boost.items():\n",
    "                next_logits[0, tid] += float(bonus)\n",
    "\n",
    "        # light scaffold nudges\n",
    "        if step_tids:\n",
    "            next_logits[0, list(step_tids)] -= 0.40 # stronger push from \"Step 1:\" unless asked\n",
    "        if tsi_tids:\n",
    "            next_logits[0, list(tsi_tids)] -= 0.10 # nudge away from \"the sentence is\"\n",
    "\n",
    "        # hard bans (keep EOS allowed)\n",
    "        next_logits[0, PAD] = float(\"-inf\")\n",
    "        if ban_token_ids:\n",
    "            idx = [t for t in ban_token_ids if t != STOP]\n",
    "            if idx:\n",
    "                next_logits[0, idx] = float(\"-inf\")\n",
    "\n",
    "        # EOS shaping\n",
    "        if len(gen) < min_before_eos:\n",
    "            next_logits[0, STOP] = float(\"-inf\")\n",
    "        else:\n",
    "            next_logits[0, STOP] += eos_boost\n",
    "\n",
    "        if len(gen) < 3:\n",
    "            scaffold_words = {\"Q\", \"Q:\", \"Question\", \"Question:\", \"A\", \"A:\", \"Answer\", \"Answer:\"}\n",
    "            scaffold_tids = {tid for tid,s in TOKEN_STR.items() if s and s.strip() in scaffold_words}\n",
    "            if scaffold_tids:\n",
    "                next_logits[0, list(scaffold_tids)] -= 1.5 # stronger push away at start\n",
    "\n",
    "        # repetition penalty\n",
    "        if gen and repetition_penalty and repetition_penalty != 1.0:\n",
    "            recent = gen[-64:]\n",
    "            for tid in set(recent):\n",
    "                val = next_logits[0, tid]\n",
    "                next_logits[0, tid] = torch.where(val > 0, val / repetition_penalty, val * repetition_penalty)\n",
    "\n",
    "        # temperature / top-k / top-p\n",
    "        do_sample = (temperature and temperature > 0) or (top_p and 0 < top_p < 1.0) or (top_k and top_k > 0)\n",
    "\n",
    "        if temperature and temperature > 0:\n",
    "            next_logits /= float(temperature)\n",
    "\n",
    "        # top-k\n",
    "        if top_k is not None and top_k > 0:\n",
    "            v, _ = torch.topk(next_logits, k=min(top_k, next_logits.size(-1)))\n",
    "            next_logits[next_logits < v[:, [-1]]] = float(\"-inf\")\n",
    "\n",
    "        # top-p\n",
    "        if top_p is not None and 0 < top_p < 1.0:\n",
    "            probs = torch.softmax(next_logits, dim=-1)\n",
    "            sp, si = torch.sort(probs, descending=True)\n",
    "            csum = torch.cumsum(sp, dim=-1)\n",
    "            keep = csum <= top_p\n",
    "            keep[..., 0] = True\n",
    "            filt = torch.full_like(sp, float(\"-inf\"))\n",
    "\n",
    "            # store filtered logits (log-probs) back into next_logits in original index order\n",
    "            filt[keep] = torch.log(sp[keep] + 1e-12)\n",
    "            next_logits = torch.full_like(next_logits, float(\"-inf\"))\n",
    "            next_logits[0, si[0]] = filt[0]\n",
    "\n",
    "        # entropy-based nudge toward EOS when degenerate\n",
    "        with torch.no_grad():\n",
    "            p = torch.softmax(next_logits, dim=-1)\n",
    "            ent = float(-(p * torch.log(p + 1e-12)).sum())\n",
    "        if len(gen) >= 6 and ent < 3.4:\n",
    "            next_logits[0, STOP] += 1.5\n",
    "\n",
    "        # guard n-gram pass over top candidates\n",
    "        running_tail = bpe_decode.decode(gen[-64:]) if gen else \"\"\n",
    "        if _NEG_HIT_RE.search((running_tail or \"\").lower()):\n",
    "            neg_window = max(neg_window, NEG_WINDOW_TOKENS)\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        _, cand_idx = torch.sort(probs, descending=True)\n",
    "        tmp_logits = next_logits.clone()\n",
    "        max_scan = min(256, cand_idx.size(-1))\n",
    "        for j in range(max_scan):\n",
    "            cid = int(cand_idx[0, j])\n",
    "            if cid == STOP:\n",
    "                continue # let EOS through\n",
    "            cand_str = TOKEN_STR.get(cid) or bpe_decode.decode([cid])\n",
    "            if _would_form_taglike(running_tail, cand_str) or (\n",
    "                no_repeat_ngram_size and _violates_ngram(gen, cid, no_repeat_ngram_size)\n",
    "            ):\n",
    "                tmp_logits[0, cid] = float(\"-inf\")\n",
    "\n",
    "            # steer away from continuing refusals; nudge toward constructive pivots\n",
    "            if neg_window > 0:\n",
    "                if _would_complete_any(gen, cid, NEG_CONT_IDS, MAX_NEG_CONT):\n",
    "                    tmp_logits[0, cid] -= NEG_CONT_PENALTY\n",
    "                if _would_complete_any(gen, cid, POS_PIVOT_IDS, MAX_POS_PVT):\n",
    "                    tmp_logits[0, cid] += POS_PIVOT_BONUS\n",
    "\n",
    "\n",
    "        # choose next token\n",
    "        if torch.isneginf(tmp_logits).all():\n",
    "            # soft fallback if we banned everything\n",
    "            fallback = next_logits.clone()\n",
    "            fallback[0, PAD] = float(\"-inf\")\n",
    "            if len(gen) < min_before_eos:\n",
    "                fallback[0, STOP] = float(\"-inf\")\n",
    "            next_id = int(torch.argmax(fallback, dim=-1))\n",
    "        else:\n",
    "            if do_sample:\n",
    "                # proper sampling from filtered distribution\n",
    "                samp_p = torch.softmax(tmp_logits, dim=-1)\n",
    "                next_id = int(torch.multinomial(samp_p[0], num_samples=1, generator=g))\n",
    "            else:\n",
    "                # fall back to greedy\n",
    "                next_id = int(torch.argmax(tmp_logits, dim=-1))\n",
    "\n",
    "        # if choosing the argmax still forms unsafe tail, pick the next safe\n",
    "        if _unsafe_tail_would_appear(gen, next_id):\n",
    "            ok = False\n",
    "            for j2 in range(min(32, cand_idx.size(-1))):\n",
    "                alt = int(cand_idx[0, j2])\n",
    "                if alt == next_id or alt == PAD:\n",
    "                    continue\n",
    "                if len(gen) < MIN_BEFORE_EOS and alt == STOP:\n",
    "                    continue\n",
    "                if not _unsafe_tail_would_appear(gen, alt):\n",
    "                    next_id = alt\n",
    "                    ok = True\n",
    "                    break\n",
    "            if not ok:\n",
    "                fb = next_logits.clone()\n",
    "                fb[0, PAD] = float(\"-inf\")\n",
    "                if len(gen) < min_before_eos:\n",
    "                    fb[0, STOP] = float(\"-inf\")\n",
    "                next_id = int(torch.argmax(fb, dim=-1))\n",
    "\n",
    "        gen.append(next_id)\n",
    "        x = torch.cat([x, torch.tensor([[next_id]], device=dev)], dim=1)\n",
    "\n",
    "        # secondary stops\n",
    "        text_so_far = bpe_decode.decode(gen)\n",
    "        if next_id == STOP: break\n",
    "        if \"<|END|>\" in text_so_far: break\n",
    "        if _count_sents(text_so_far) >= SENTENCE_LIMIT: break\n",
    "        if _STEP_PAT.search(text_so_far) or _SENT_PAT.search(text_so_far):\n",
    "            min_before_eos = 0\n",
    "            eos_boost = 3.0\n",
    "\n",
    "        if len(gen) >= 10 and len(set(gen[-10:])) <= 3: break\n",
    "        if neg_window > 0:\n",
    "            neg_window -= 1\n",
    "\n",
    "\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51ffd9",
   "metadata": {},
   "source": [
    "# Strip for clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc18417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_to_last_period(s: str) -> str:\n",
    "    s = s.rstrip()\n",
    "    if s.endswith('.'):\n",
    "        return s\n",
    "    i = s.rfind('.')\n",
    "    return \"\" if i == -1 else s[:i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37495b",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8a98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def _fallback_answer(info_text: str, user_text: str) -> str:\n",
    "    # try to honor name if provided in INFO block\n",
    "    m = re.search(r\"(?i)\\bmy name is\\s+([\\w .'-]{2,64})\", info_text or \"\")\n",
    "    name = m.group(1).strip() if m else None\n",
    "    if re.search(r\"(?i)\\bwho am i\\b|\\bwhat'?s my name\\b|\\bwho am i to you\\b\", user_text or \"\"):\n",
    "        if name:\n",
    "            return f\"You're {name}.\"\n",
    "        return \"You're the user chatting with me.\"\n",
    "    # generic safe fallback\n",
    "    return \"I'm a small chat model. How can I help in a sentence or two?\"\n",
    "\n",
    "def _first_person_bias() -> dict[int, float]:\n",
    "    # include leading-space variants (because we're using ByteLevel BPE)\n",
    "    seeds = [\" I\", \" I'm\", \" I’m\", \" I can\", \"I\"]\n",
    "    bias = {}\n",
    "    for s in seeds:\n",
    "        tid = bpe_decode.token_to_id(s)\n",
    "        if tid is not None and tid >= 0:\n",
    "            bias[tid] = 0.8 # small, not domineering\n",
    "    return bias\n",
    "\n",
    "def _score_answer(user_text: str, out: str) -> float:\n",
    "    score = 0.0\n",
    "    sents = _count_sents(out)\n",
    "    if 1 <= sents <= 3: score += 2.0\n",
    "    elif sents == 0:    score -= 2.0\n",
    "    else:               score -= 1.0\n",
    "\n",
    "    ut = user_text.lower(); ol = out.lower()\n",
    "    # encourage first-person & capability verbs for openers like \"Who are you / what can you do?\"\n",
    "    if re.search(r\"(?i)\\bwho\\s+are\\s+you\\b|\\bwhat\\s+can\\s+you\\s+do\\b|\\bhow\\s+can\\s+you\\s+help\\b\", user_text):\n",
    "        if \" i \" in f\" {ol} \" or \"i'm\" in ol or \"i’m\" in ol: score += 1.2\n",
    "        if \" can \" in f\" {ol} \": score += 0.8\n",
    "\n",
    "    # discourage common boilerplate\n",
    "    for bp in (\"i don't have access to the latest news\", \"news or news\", \"business business\"):\n",
    "        if bp in ol: score -= 1.5\n",
    "\n",
    "    # numbered list unless explicitly requested\n",
    "    if not re.search(r\"(?i)\\b(list|steps|bullets|outline|numbered)\\b\", user_text):\n",
    "        if re.search(r\"(?m)^\\s*\\d+\\.\\s+\", out): score -= 1.0\n",
    "\n",
    "    # very long tail\n",
    "    if len(out) > 400: score -= 0.5\n",
    "    return score\n",
    "\n",
    "# generate(info_text, user_text) -> str\n",
    "# args:\n",
    "#     info_text: anything you'd like to pass inside <|INFOSTART|>...<|INFOEND|>\n",
    "#     user_text: the user's prompt\n",
    "#     greedy: kept for API compatibility\n",
    "#     clean_style: ban emoji/links/hashtags tokens during decode\n",
    "# returns:\n",
    "#     string, trimmed to a few sentences; never includes tags\n",
    "\n",
    "def generate(info_text: str, user_text: str, greedy: bool=False, clean_style: bool=True) -> str:\n",
    "    model = load_with_adapter(BEST, merge=False)\n",
    "    prompt_ids = build_prompt_ids(user_text=user_text, info_text=info_text, system_text=SYSTEM_PROMPT_DEFAULT)\n",
    "\n",
    "    user_words = {w for w in re.findall(r\"\\w+\", user_text.lower()) if len(w) >= 3}\n",
    "\n",
    "    # decode settings\n",
    "    decode = dict(\n",
    "        max_new_tokens=40, # maximum tokens\n",
    "        temperature=0.60, # crisp wording level (0-1)\n",
    "        top_p=1.0, # disabled nucleus (we're completely using top_k since it's a small model)\n",
    "        top_k=2, # tiny randomness (so, like choose 1 between 2 top probabilities)\n",
    "        repetition_penalty=1.16, # penalty for repetition\n",
    "        no_repeat_ngram_size=4, # reduce short phrase echoes\n",
    "        seed=None, # randomness (but g = generator if generator is not None else (torch.Generator(device=dev).manual_seed(int(seed)) if seed is not None else None) handles that)\n",
    "        eos_token_id=END_ID, # END token\n",
    "        pad_token_id=PAD_ID, # PAD token\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # tiny budget for very short prompts\n",
    "    if len(user_text.strip()) <= 20:\n",
    "        decode[\"max_new_tokens\"] = min(decode[\"max_new_tokens\"], 80)\n",
    "\n",
    "    ban_ids = _build_ban_ids() if clean_style else set()\n",
    "    first_boost = _first_person_bias()\n",
    "\n",
    "    cands = []\n",
    "    for _ in range(3):\n",
    "        g = torch.Generator(device=DEVICE).manual_seed(int(time.time_ns()) & 0xFFFFFFFF)\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            gen_ids = generate_ids(\n",
    "                model, prompt_ids, user_words,\n",
    "                ban_token_ids=ban_ids, generator=g, first_token_boost=first_boost, **decode\n",
    "            )\n",
    "        cands.append(bpe_decode.decode(gen_ids).split(\"<|END|>\")[0])\n",
    "\n",
    "    best_raw = max(cands, key=lambda t: _score_answer(user_text, t))\n",
    "    out = best_raw  # already a string; do not re-decode last gen_ids\n",
    "\n",
    "    # # detokenize + cleanup (clip at END, strip leaked tags)\n",
    "    out = re.sub(r\"\\s+\", \" \", out)\n",
    "    # strip any tag tokens that leaked as text\n",
    "    out = re.sub(r\"<\\|[^>]+?\\|>\", \"\", out).strip().strip('\"')\n",
    "    out = re.sub(r\"\\b(?:START|END|SYSTEM|USER|ASSISTANT|INFOSTART|INFOEND)\\b[:\\\"]?\", \"\", out, flags=re.I)\n",
    "    # remove obvious scaffold headers if any slipped through\n",
    "    out = re.sub(r\"(?i)\\bstep\\s*\\d+\\s*:\\s*\", \"\", out)\n",
    "    out = re.sub(r\"(?i)(identify the (given )?sentence|the sentence is)\\.?\", \"\", out)\n",
    "    # dedupe repeated words\n",
    "    out = re.sub(r\"\\b(\\w+)(?:\\s+\\1){2,}\\b\", r\"\\1\", out)\n",
    "    out = re.sub(r\"\\s{2,}\", \" \", out).strip()\n",
    "    # strip numbered lists unless explicitly requested\n",
    "    if not re.search(r\"(?i)\\b(list|steps|bullets|outline|numbered)\\b\", user_text):\n",
    "        out = re.sub(r\"(?m)^\\s*\\d+\\.\\s+\", \"\", out)\n",
    "\n",
    "\n",
    "    # final trim & fallback if too weird/empty\n",
    "    out = _trim_sents(out, SENTENCE_LIMIT)\n",
    "    if (not out) or len(out) < 3 or _STEP_PAT.search(out) or _SENT_PAT.search(out) or any(t in out for t in (\"FOST\", \"Pplanator\")):\n",
    "        out = _fallback_answer(info_text, user_text)\n",
    "\n",
    "    out = trim_to_last_period(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553c0b4",
   "metadata": {},
   "source": [
    "# Sanity check & generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89ceb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] ids: (1161960, 512) | mask ones%: 29.003%\n",
      "[ASSIST_PREFIX detected: ' '] peek row0 after tag: '  Sure, here'\n",
      "\n",
      "\n",
      "Generated #1:\n",
      "[load] state dict loaded cleanly.\n",
      "[PEFT] attached → LoRA tensors: 96\n",
      "I am a professional player who has been a prominent player in the field of tennis. He is known for his skills and skills, and he has also played an important role in winning tennis.\n",
      "\n",
      "\n",
      "Generated #2:\n",
      "[load] state dict loaded cleanly.\n",
      "[PEFT] attached → LoRA tensors: 96\n",
      "I am a person who is very interested in learning about the world. You can learn from a book by the author \"The World of Life\" by George G. D.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if SFT_IDS.exists() and SFT_MSK.exists():\n",
    "        ids = np.load(SFT_IDS, mmap_mode=\"r\"); msk = np.load(SFT_MSK, mmap_mode=\"r\")\n",
    "        ones = float(msk.sum())/msk.size if msk.size else 0.0\n",
    "        print(f\"[DATA] ids: {ids.shape} | mask ones%: {ones:.3%}\")\n",
    "        # Peek what actually follows <|ASSISTANT|> in row 0:\n",
    "        txt0 = bpe_decode.decode([t for t in ids[0].tolist() if t != PAD_ID])\n",
    "        j0 = txt0.find(\"<|ASSISTANT|>\"); post = txt0[j0+len('<|ASSISTANT|>'): j0+len('<|ASSISTANT|>')+12] if j0!=-1 else \"\"\n",
    "        print(f\"[ASSIST_PREFIX detected: {repr(ASSIST_PREFIX)}] peek row0 after tag: {repr(post)}\")\n",
    "        if \"<|END|>\" in post:\n",
    "            print(\"[warn] Row0 assistant begins near EOS; many rows might be ultra-short. Decoding will lean on fallbacks.\")\n",
    "\n",
    "    for i in range(2):\n",
    "        print(f\"\\n\\nGenerated #{i+1}:\")\n",
    "        print(generate(info_text=\"You are helpful artificial intelligence\", user_text=\"Who are you and what can you do?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
