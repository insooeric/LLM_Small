{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import json, numpy as np, torch, re, io, contextlib, sys, warnings\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from peft import PeftModel\n",
    "from numbers import Real\n",
    "import tempfile, shutil\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "BASE = Path(\"./for_llm_app/best_base.pt\")\n",
    "ADAPTER_CONFIG = Path(\"./for_llm_app/adapter_config.json\")\n",
    "ADAPTER_WEIGHTS = Path(\"./for_llm_app/adapter_model.safetensors\")\n",
    "\n",
    "CONF = Path(\"./for_llm_app/model_config_124M_sft.json\")\n",
    "VOCAB = Path(\"./for_llm_app/bpe_model-vocab.json\")\n",
    "MERGES = Path(\"./for_llm_app/bpe_model-merges.txt\")\n",
    "\n",
    "TEMPERATURE = 0.25\n",
    "TOP_P = 0.95\n",
    "TOP_K = 0\n",
    "NUM_CANDIDATES = 3\n",
    "TYPICAL_TAU = 0.9\n",
    "\n",
    "REPETITION_PENALTY = 1.1\n",
    "NO_REPEAT_NGRAM_SIZE = 3\n",
    "\n",
    "NEG_CONT_PENALTY = 1.6\n",
    "NEG_WINDOW_TOKENS = 12\n",
    "POS_PIVOT_BONUS = 0.9\n",
    "\n",
    "SENTENCE_LIMIT = 3\n",
    "MIN_BEFORE_EOS = 14\n",
    "EOS_BOOST = 0.2\n",
    "\n",
    "ASSIST_PREFIX = \" \"\n",
    "DISABLE_SHAPING = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    AMP_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "else:\n",
    "    AMP_DTYPE = torch.float32\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "bpe_tokenizer = ByteLevelBPETokenizer(str(VOCAB), str(MERGES), lowercase=False, add_prefix_space=True)\n",
    "\n",
    "SPECIALS = [\n",
    "    \"<|PAD|>\", \"<|UNKNOWN|>\", \"<|START|>\", \"<|END|>\",\n",
    "    \"<|SYSTEM|>\", \"<|USER|>\", \"<|ASSISTANT|>\", \"<|EOT|>\",\n",
    "    \"<|INFOSTART|>\", \"<|INFOEND|>\",\n",
    "]\n",
    "SID = {tok: bpe_tokenizer.token_to_id(tok) for tok in SPECIALS}\n",
    "\n",
    "_missing = [t for t, i in SID.items() if i is None]\n",
    "assert not _missing, f\"Missing special tokens in vocab: {_missing}\"\n",
    "\n",
    "_EXPECTED = {\n",
    "    \"<|PAD|>\": 0,\n",
    "    \"<|UNKNOWN|>\": 1,\n",
    "    \"<|START|>\": 2,\n",
    "    \"<|END|>\": 3,\n",
    "    \"<|SYSTEM|>\": 4,\n",
    "    \"<|USER|>\": 5,\n",
    "    \"<|ASSISTANT|>\": 6,\n",
    "    \"<|EOT|>\": 7,\n",
    "    \"<|INFOSTART|>\": 8,\n",
    "    \"<|INFOEND|>\": 9\n",
    "}\n",
    "\n",
    "_MODEL_CACHE = {\"model\": None}\n",
    "def get_model(merge: bool=False):\n",
    "    m = _MODEL_CACHE[\"model\"]\n",
    "    if m is None:\n",
    "        m = load_with_adapter_from_files(ADAPTER_CONFIG, ADAPTER_WEIGHTS, merge=merge)\n",
    "        _MODEL_CACHE[\"model\"] = m\n",
    "    return m\n",
    "\n",
    "_mismatch = {t: (SID[t], want) for t, want in _EXPECTED.items() if SID[t] != want}\n",
    "assert not _mismatch, f\"Special token ID mismatch: {_mismatch}\"\n",
    "\n",
    "(\n",
    "    PAD_ID, UNKNOWN_ID, START_ID, END_ID,\n",
    "    SYSTEM_ID, USER_ID, ASSISTANT_ID, EOT_ID,\n",
    "    INFOSTART_ID, INFOEND_ID\n",
    ") = (SID[t] for t in SPECIALS)\n",
    "\n",
    "_CAP = re.compile(r\"\\b[A-Z][a-z]+(?:[-'][A-Z][a-z]+)*\\b\")\n",
    "\n",
    "def _cap_words(s: str) -> set[str]:\n",
    "    common = {\"I\",\"You\",\"We\",\"It\",\"The\",\"A\",\"An\"}\n",
    "    months = {\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"}\n",
    "    words = {w for w in _CAP.findall(s)}\n",
    "    return {w for w in words if w not in common | months}\n",
    "\n",
    "assert len({SID[t] for t in SPECIALS}) == len(SPECIALS), \"Special token ID collision detected\"\n",
    "import sys\n",
    "sys.path.append(\"./for_llm_app\")\n",
    "from Dummy_Model_sft import DummyModel\n",
    "\n",
    "def load_state_dict_safely(model: torch.nn.Module, raw_state, strict: bool=False, verbose: bool=True):\n",
    "    state = raw_state.get(\"model\", raw_state.get(\"state_dict\", raw_state))\n",
    "    clean = {}\n",
    "    for k, v in state.items():\n",
    "        if k.startswith(\"module.\"): k = k[7:]\n",
    "        if k.startswith(\"model.\"): k = k[6:]\n",
    "        clean[k] = v\n",
    "    missing, unexpected = model.load_state_dict(clean, strict=strict)\n",
    "    if verbose:\n",
    "        if missing: print(f\"[load] missing ({len(missing)}): {missing[:5]} ...\")\n",
    "        if unexpected: print(f\"[load] unexpected ({len(unexpected)}): {unexpected[:5]} ...\")\n",
    "    return model\n",
    "\n",
    "cfg = json.loads(Path(CONF).read_text())\n",
    "\n",
    "\n",
    "def _safe_torch_load(path: Path):\n",
    "    try:\n",
    "        return torch.load(path, map_location=DEVICE, weights_only=True)\n",
    "    except TypeError:\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"You are using `torch.load` with `weights_only=False`\",\n",
    "            category=FutureWarning,\n",
    "        )\n",
    "        return torch.load(path, map_location=DEVICE)\n",
    "\n",
    "\n",
    "def _load_base_only() -> torch.nn.Module:\n",
    "    base = DummyModel(cfg).to(DEVICE).eval()\n",
    "    raw = _safe_torch_load(BASE)\n",
    "    load_state_dict_safely(base, raw, strict=False, verbose=True)\n",
    "    return base\n",
    "\n",
    "\n",
    "def load_with_adapter_from_files(config_json: Path, weights_file: Path, merge: bool=False) -> torch.nn.Module:\n",
    "    base = _load_base_only()\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tdir = Path(tmpdir)\n",
    "        cfg_dst = tdir / \"adapter_config.json\"\n",
    "        wts_dst = tdir / \"adapter_model.safetensors\"\n",
    "\n",
    "        shutil.copyfile(config_json, cfg_dst)\n",
    "        shutil.copyfile(weights_file, wts_dst)\n",
    "\n",
    "        model = PeftModel.from_pretrained(base, str(tdir), is_trainable=False).to(DEVICE).eval()\n",
    "        if merge:\n",
    "            model.merge_and_unload()\n",
    "        return model\n",
    "\n",
    "SYSTEM_PROMPT_DEFAULT = (\n",
    "    \"Be a helpful, concise assistant with a light, friendly tone. \"\n",
    "    \"Answer directly in 1–3 sentences. Don’t use steps or bullet lists unless the user asks. \"\n",
    "    \"Use the content between INFOSTART/INFOEND only as context. Do not mention it, 'memory', or any internal tags. \"\n",
    "    \"Avoid speculation and say when unsure. Keep replies safe, accurate, and on-topic. \"\n",
    "    \"Write in first person. Don’t mention being a language model, training data, or lack of browsing unless asked.\"\n",
    ")\n",
    "\n",
    "def build_prompt_ids(user_text: str, info_text: str, system_text: str = SYSTEM_PROMPT_DEFAULT, assist_prefix: str = \" \"):\n",
    "    txt = (\n",
    "        \"<|START|><|SYSTEM|>\" + system_text + \"\\n\" +\n",
    "        \"<|INFOSTART|>\" + (info_text if info_text else \"\") + \"<|INFOEND|>\\n\" +\n",
    "        \"<|USER|>\" + user_text.strip() + \"\\n\" +\n",
    "        \"<|ASSISTANT|>\" + assist_prefix\n",
    "    )\n",
    "    return bpe_tokenizer.encode(txt, add_special_tokens=False).ids\n",
    "\n",
    "_STEP_PAT = re.compile(r\"(?i)\\bstep\\s*\\d+\")\n",
    "_SENT_PAT = re.compile(r\"(?i)(identify the (given )?sentence|the sentence is)\")\n",
    "_SENT_END_RE = re.compile(r'[.!?](?=\\s|$)')\n",
    "\n",
    "def _count_sents(s: str) -> int:\n",
    "    return len(_SENT_END_RE.findall(s))\n",
    "\n",
    "\n",
    "def _trim_sents(s: str, limit: int) -> str:\n",
    "    if limit <= 0: return \"\"\n",
    "    if _count_sents(s) <= limit: return s.strip()\n",
    "    idx = 0; kept = 0\n",
    "    for m in _SENT_END_RE.finditer(s):\n",
    "        kept += 1; idx = m.end()\n",
    "        if kept >= limit: break\n",
    "    return s[:idx].strip()\n",
    "\n",
    "VOCAB_SIZE = int(cfg.get(\"vocab_size\", bpe_tokenizer.get_vocab_size()))\n",
    "TOKEN_STR = {i: bpe_tokenizer.decode([i]) for i in range(VOCAB_SIZE)}\n",
    "\n",
    "def _has_emoji_or_symbol(s: str) -> bool:\n",
    "    return any(\n",
    "        (0x1F300 <= ord(ch) <= 0x1FAFF) or\n",
    "        (0x2600 <= ord(ch) <= 0x26FF) or\n",
    "        (0x2700 <= ord(ch) <= 0x27BF)\n",
    "        for ch in s\n",
    "    )\n",
    "\n",
    "def _dedupe_close(texts, tol=0.92):\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        if not any(len(set(t.lower().split()) & set(u.lower().split())) /\n",
    "                   max(1, len(set(t.lower().split()) | set(u.lower().split()))) > tol\n",
    "                   for u in out):\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def _build_ban_ids() -> set[int]:\n",
    "    bad = set()\n",
    "    literal_specials = {\n",
    "        \"<|PAD|>\", \"<|UNKNOWN|>\", \"<|START|>\", \"<|END|>\",\n",
    "        \"<|SYSTEM|>\", \"<|USER|>\", \"<|ASSISTANT|>\", \"<|EOT|>\",\n",
    "        \"<|INFOSTART|>\", \"<|INFOEND|>\"\n",
    "    }\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        s = TOKEN_STR[i]\n",
    "        if not s:\n",
    "            continue\n",
    "        if s in literal_specials:\n",
    "            bad.add(i); continue\n",
    "        if _has_emoji_or_symbol(s):\n",
    "            bad.add(i); continue\n",
    "        if s.strip() in {\"<\", \">\"}:\n",
    "            bad.add(i); continue\n",
    "        \n",
    "    bad.discard(END_ID)\n",
    "    bad.discard(PAD_ID)\n",
    "    return bad\n",
    "\n",
    "\n",
    "\n",
    "def _violates_ngram(seq, cand, n):\n",
    "    if n <= 1 or len(seq) < n-1: return False\n",
    "    s = seq + [cand]; last = tuple(s[-n:])\n",
    "    for i in range(len(s)-n):\n",
    "        if tuple(s[i:i+n]) == last:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "_NEG_HIT_RE = re.compile(r\"\\b(?:not|no|never|cannot|can['’]t|won['’]t|don['’]t|doesn['’]t|didn['’]t|isn['’]t|aren['’]t|wasn['’]t|weren['’]t)\\b\", re.I)\n",
    "_NEG_CONT_PHRASES = [\n",
    "    \" I can't\", \" I cannot\", \" can't\", \" cannot\", \" unable to\", \" I won't\", \" I do not\",\n",
    "    \" I don't\", \" I am not able\", \" unfortunately\", \" sorry\", \" I cannot help\",\n",
    "    \" I don't have the ability\", \" I don't have the ability to\", \" I do not have the ability to\",\n",
    "    \" I don't have access to the latest\", \" I do not have access to the latest\",\n",
    "    \" I don't have access to real-time\", \" I do not have access to real-time\",\n",
    "    \" I cannot browse\", \" I can't browse\"\n",
    "]\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"a\",\"an\",\"the\",\"and\",\"or\",\"but\",\"if\",\"then\",\"so\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\n",
    "    \"is\",\"are\",\"am\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
    "    \"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\n",
    "    \"my\",\"your\",\"his\",\"hers\",\"our\",\"their\",\n",
    "    \"what\",\"which\",\"who\",\"whom\",\"whose\",\"when\",\"where\",\"why\",\"how\",\n",
    "    \"do\",\"does\",\"did\",\"can\",\"could\",\"should\",\"would\",\"may\",\"might\",\"will\",\"shall\"\n",
    "}\n",
    "\n",
    "_POS_PIVOTS = [\n",
    "    \" but\", \" however\", \" still\", \" instead\", \" here's\", \" here is\",\n",
    "    \" I can\", \" we can\", \" let's\", \" try\", \" you can\",\n",
    "]\n",
    "\n",
    "def _ids_for_phrases(phrases):\n",
    "    out = []\n",
    "    for p in phrases:\n",
    "        ids = bpe_tokenizer.encode(p, add_special_tokens=False).ids\n",
    "        if ids:\n",
    "            out.append(ids)\n",
    "    return out\n",
    "\n",
    "NEG_CONT_IDS = _ids_for_phrases(_NEG_CONT_PHRASES)\n",
    "POS_PIVOT_IDS = _ids_for_phrases(_POS_PIVOTS)\n",
    "MAX_NEG_CONT = max((len(s) for s in NEG_CONT_IDS), default=0)\n",
    "MAX_POS_PVT  = max((len(s) for s in POS_PIVOT_IDS), default=0)\n",
    "COLON_TIDS = {tid for tid, s in TOKEN_STR.items() if s and s.strip() == ':'}\n",
    "TUTORY = {\"here's\", \"here\", \"how\", \"let\", \"explain\"}\n",
    "TUTORY_TIDS = {tid for tid, s in TOKEN_STR.items() if s and s.strip().lower() in TUTORY}\n",
    "PERIOD_TIDS = {tid for tid, s in TOKEN_STR.items() if s and s.strip() in {\".\", \"!\", \"?\"}}\n",
    "PHRASEY = {\"identify\", \"key\", \"elements\", \"use\", \"information\"}\n",
    "PHRASEY_TIDS = {tid for tid, s in TOKEN_STR.items() if s and s.strip().lower() in PHRASEY}\n",
    "DRIFT_WORDS = {\"human\",\"body\",\"biology\",\"animal\",\"animals\",\"cell\",\"cells\"}\n",
    "DRIFT_TIDS  = {tid for tid, s in TOKEN_STR.items() if s and s.strip().lower() in DRIFT_WORDS}\n",
    "\n",
    "def typical_filter(logits, tau=0.9):\n",
    "    p = torch.softmax(logits, dim=-1)\n",
    "    H = -(p * (p+1e-12).log()).sum(dim=-1, keepdim=True)\n",
    "    self_info = -torch.log(p + 1e-12)\n",
    "    dev = torch.abs(self_info - H)\n",
    "    idx = torch.argsort(dev, dim=-1)\n",
    "    cum = torch.cumsum(p.gather(-1, idx), dim=-1)\n",
    "    keep_mask = cum <= tau\n",
    "    keep_mask[..., 0] = True\n",
    "    kept = torch.full_like(p, float(\"-inf\"))\n",
    "    kept[0, idx[0, keep_mask[0]]] = torch.log(p[0, idx[0, keep_mask[0]]] + 1e-12)\n",
    "    return kept\n",
    "\n",
    "def _would_complete_any(prev_ids: list[int], cand_id: int, seqs: list[list[int]], max_len: int) -> bool:\n",
    "    if not seqs:\n",
    "        return False\n",
    "    ctx = prev_ids[-(max_len-1):] if max_len > 1 else []\n",
    "    test = ctx + [cand_id]\n",
    "    for seq in seqs:\n",
    "        L = len(seq)\n",
    "        if len(test) >= L and test[-L:] == seq:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _soft_tail_trim(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    cut = re.search(r'[.!?](?!.*[.!?])', s)\n",
    "    return s if not cut else s[:cut.end()]\n",
    "\n",
    "TAG_TOKEN_RE = re.compile(\n",
    "    r\"<\\|?(PAD|UNKNOWN|START|END|SYSTEM|USER|ASSISTANT|EOT|INFOSTART|INFOEND)\\|?>\"\n",
    ")\n",
    "\n",
    "def _unsafe_tail_would_appear(seq, candidate_id):\n",
    "    test = bpe_tokenizer.decode(seq + [candidate_id])\n",
    "    tail = test[-128:]\n",
    "    return bool(TAG_TOKEN_RE.search(tail))\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_ids(model, prompt_ids, user_words: set[str] | None = None, min_tokens: int = 40,\n",
    "                 max_new_tokens=120, temperature=0.0, top_p=1.0, top_k=0,\n",
    "                 repetition_penalty=1.07, no_repeat_ngram_size=3, seed=None,\n",
    "                 ban_token_ids: set[int] | None = None,\n",
    "                 eos_token_id=None, pad_token_id=None, generator=None, \n",
    "                 known_caps: set[str] | None = None):\n",
    "\n",
    "    dev  = next(model.parameters()).device\n",
    "    STOP = int(eos_token_id if eos_token_id is not None else END_ID)\n",
    "    PAD  = int(pad_token_id if pad_token_id is not None else PAD_ID)\n",
    "    g    = generator if generator is not None else (\n",
    "        torch.Generator(device=dev).manual_seed(int(seed)) if seed is not None else None\n",
    "    )\n",
    "\n",
    "    x = torch.tensor([prompt_ids], dtype=torch.long, device=dev)\n",
    "    gen: list[int] = []\n",
    "    neg_window = 0\n",
    "    min_before_eos = MIN_BEFORE_EOS\n",
    "    eos_boost = EOS_BOOST\n",
    "\n",
    "    CORE_WORDS_HARD = {\"step\", \"Step\", \"•\", \"-\", \"Identify\", \"keywords\", \"phrases\", \"Use\", \"Here's\", \"how\"}\n",
    "    SOFT_WORDS      = {\":\", \"1\", \"2\", \"3\", \"4\", \"5\"}\n",
    "    CORE_TIDS = {tid for tid,s in TOKEN_STR.items() if s and s.strip() in CORE_WORDS_HARD}\n",
    "    SOFT_TIDS = {tid for tid,s in TOKEN_STR.items() if s and s.strip() in SOFT_WORDS}\n",
    "    QA_SCAFF_WORDS = {\"Q\",\"Q:\",\"Question\",\"Question:\",\"A\",\"A:\",\"Answer\",\"Answer:\"}\n",
    "    QA_SCAFF_TIDS  = {tid for tid, s in TOKEN_STR.items() if s and s.strip() in QA_SCAFF_WORDS}\n",
    "\n",
    "    want_steps = False\n",
    "    if user_words:\n",
    "        hints = {\"steps\",\"step\",\"bullet\",\"list\",\"instructions\",\"how-to\",\"procedure\",\"outline\"}\n",
    "        want_steps = any(h in user_words for h in hints)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE, enabled=(dev.type==\"cuda\")):\n",
    "            logits, _ = model(input_ids=x)\n",
    "        next_logits = logits[:, -1, :].float()\n",
    "\n",
    "        if not want_steps:\n",
    "            if CORE_TIDS:\n",
    "                next_logits[0, list(CORE_TIDS)] -= 0.15\n",
    "            if len(gen) < 18 and SOFT_TIDS:\n",
    "                next_logits[0, list(SOFT_TIDS)] -= 0.20\n",
    "\n",
    "            recent = (bpe_tokenizer.decode(gen[-64:]) if gen else \"\").lower()\n",
    "            looks_listy = (\n",
    "                \"step\" in recent[-24:]\n",
    "                or \"•\" in recent[-24:]\n",
    "                or re.search(r\"(^|\\s)-\\s?$\", recent[-24:])\n",
    "                or re.search(r\"\\bhere\\s+(are|is|\\'?s)\\b\", recent)\n",
    "                or \"keyword\" in recent[-32:] or \"phrase\" in recent[-32:]\n",
    "            )\n",
    "            if looks_listy and SOFT_TIDS:\n",
    "                next_logits[0, list(SOFT_TIDS)] -= 0.30\n",
    "\n",
    "        partial_text = bpe_tokenizer.decode(gen)\n",
    "        sents_done = _count_sents(partial_text) >= 1\n",
    "\n",
    "        if not sents_done and known_caps is not None:\n",
    "            ctx_words_lower = {w.lower() for w in re.findall(r\"[A-Za-z][\\w'-]*\", bpe_tokenizer.decode(prompt_ids) or \"\")}\n",
    "            with torch.no_grad():\n",
    "                _, cand = torch.topk(next_logits, k=min(64, next_logits.size(-1)), dim=-1)\n",
    "            for tid in cand[0].tolist():\n",
    "                if tid in (END_ID, PAD_ID):\n",
    "                    continue\n",
    "                ts = (TOKEN_STR.get(tid) or \"\").strip().lower()\n",
    "                if ts.isalpha() and ts in ctx_words_lower:\n",
    "                    next_logits[0, tid] -= 0.35\n",
    "\n",
    "        if not sents_done:\n",
    "            tail = (partial_text[-40:] if partial_text else \"\").lower()\n",
    "            if re.search(r\"\\bit\\s+is\\s+a\\s+$\", tail):\n",
    "                BOILER = {\"that\",\"which\",\"uses\",\"language\",\"words\"}\n",
    "                BOILER_TIDS = {tid for tid,s in TOKEN_STR.items() if s and s.strip().lower() in BOILER}\n",
    "                if BOILER_TIDS:\n",
    "                    next_logits[0, list(BOILER_TIDS)] -= 0.4\n",
    "\n",
    "        if sents_done:\n",
    "            if COLON_TIDS:\n",
    "                next_logits[0, list(COLON_TIDS)] -= 1.2\n",
    "            if TUTORY_TIDS:\n",
    "                next_logits[0, list(TUTORY_TIDS)] -= 0.8\n",
    "            if not want_steps and PHRASEY_TIDS:\n",
    "                next_logits[0, list(PHRASEY_TIDS)] -= 0.15\n",
    "\n",
    "            mem_words = set(re.findall(r\"[A-Za-z][\\w'-]*\", (bpe_tokenizer.decode(prompt_ids) or \"\")))\n",
    "            if mem_words:\n",
    "                with torch.no_grad():\n",
    "                    _, cand = torch.topk(next_logits, k=min(48, next_logits.size(-1)), dim=-1)\n",
    "                mem_lower = {w.lower() for w in mem_words}\n",
    "                for tid in cand[0].tolist():\n",
    "                    if tid in (END_ID, PAD_ID):\n",
    "                        continue\n",
    "                    ts = (TOKEN_STR.get(tid) or \"\").strip().lower()\n",
    "                    if ts and ts in mem_lower:\n",
    "                        next_logits[0, tid] -= 0.12\n",
    "\n",
    "            APOLOGY = {\"sorry\",\"unfortunately\"}\n",
    "            APO_TIDS = {tid for tid,s in TOKEN_STR.items() if s and s.strip().lower() in APOLOGY}\n",
    "            if APO_TIDS:\n",
    "                next_logits[0, list(APO_TIDS)] -= 0.2\n",
    "        \n",
    "        if _count_sents(bpe_tokenizer.decode(gen)) >= 2 and PERIOD_TIDS:\n",
    "            next_logits[0, list(PERIOD_TIDS)] += 0.10\n",
    "\n",
    "        if user_words:\n",
    "            with torch.no_grad():\n",
    "                _, cand = torch.topk(next_logits, k=min(64, next_logits.size(-1)), dim=-1)\n",
    "            uw_lower = {w.lower() for w in user_words}\n",
    "            for tid in cand[0].tolist():\n",
    "                ts = (TOKEN_STR.get(tid) or \"\").strip().lower()\n",
    "                if len(ts) >= 3 and ts.isalpha() and ts in uw_lower:\n",
    "                    next_logits[0, tid] += 0.2\n",
    "\n",
    "        if DRIFT_TIDS:\n",
    "            if not any(w in user_words for w in {\"human\",\"biology\",\"body\",\"animal\",\"animals\",\"cell\",\"cells\"}):\n",
    "                penalty = 0.6\n",
    "                if \"humanity\" in user_words:\n",
    "                    penalty += 0.25\n",
    "                next_logits[0, list(DRIFT_TIDS)] -= penalty\n",
    "\n",
    "        if len(gen) < 12:\n",
    "            REFUSAL_SURF = {\"don't\", \"cannot\", \"can't\", \"unable\", \"sorry\", \"unfortunately\"}\n",
    "            REFUSAL_TIDS = {tid for tid,s in TOKEN_STR.items() if s and s.strip().lower() in REFUSAL_SURF}\n",
    "            if REFUSAL_TIDS:\n",
    "                next_logits[0, list(REFUSAL_TIDS)] -= 1.2\n",
    "\n",
    "        next_logits[0, PAD] = float(\"-inf\")\n",
    "        if ban_token_ids:\n",
    "            idx = [t for t in ban_token_ids if t != STOP]\n",
    "            if idx:\n",
    "                next_logits[0, idx] = float(\"-inf\")\n",
    "\n",
    "        if len(gen) < max(min_before_eos, min_tokens):\n",
    "            next_logits[0, STOP] = float(\"-inf\")\n",
    "        else:\n",
    "            next_logits[0, STOP] += eos_boost\n",
    "\n",
    "        if len(gen) < 3 and QA_SCAFF_TIDS:\n",
    "            next_logits[0, list(QA_SCAFF_TIDS)] -= 1.5\n",
    "\n",
    "        if gen and repetition_penalty and repetition_penalty != 1.0:\n",
    "            recent_ids = gen[-64:]\n",
    "            for tid in set(recent_ids):\n",
    "                val = next_logits[0, tid]\n",
    "                next_logits[0, tid] = torch.where(val > 0, val / repetition_penalty, val * repetition_penalty)\n",
    "\n",
    "        if len(gen) >= 8:\n",
    "            window = gen[-8:]\n",
    "            uniq, counts = np.unique(window, return_counts=True)\n",
    "            for tid_i, cnt in zip(uniq.tolist(), counts.tolist()):\n",
    "                if tid_i in (STOP, PAD): \n",
    "                    continue\n",
    "                next_logits[0, tid_i] -= 0.03 * cnt\n",
    "\n",
    "        if len(gen) >= 2:\n",
    "            last1, last2 = gen[-1], gen[-2]\n",
    "            with torch.no_grad():\n",
    "                _, cand = torch.topk(next_logits, k=min(64, next_logits.size(-1)), dim=-1)\n",
    "            for tid in cand[0].tolist():\n",
    "                if tid == last1 == last2:\n",
    "                    next_logits[0, tid] -= 0.6\n",
    "                if tid == last1:\n",
    "                    next_logits[0, tid] -= 0.12\n",
    "\n",
    "        do_sample = (temperature and temperature > 0) or (top_p and 0 < top_p < 1.0) or (top_k and top_k > 0)\n",
    "\n",
    "        if temperature and temperature > 0:\n",
    "            next_logits /= float(temperature)\n",
    "\n",
    "        if known_caps and _count_sents(partial_text) >= 2:\n",
    "            with torch.no_grad():\n",
    "                _, cand = torch.topk(next_logits, k=min(64, next_logits.size(-1)), dim=-1)\n",
    "            for tid in cand[0].tolist():\n",
    "                ts = TOKEN_STR.get(tid) or \"\"\n",
    "                if ts[:1].isupper() and ts.isalpha() and ts not in known_caps and tid not in (STOP, PAD):\n",
    "                    next_logits[0, tid] -= 0.35\n",
    "        \n",
    "        use_typical = (len(gen) >= 12)\n",
    "                \n",
    "        if use_typical:\n",
    "            filt = typical_filter(next_logits, tau=TYPICAL_TAU)\n",
    "            if not torch.isneginf(filt).all():\n",
    "                next_logits = filt\n",
    "        else:\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(next_logits, k=min(top_k, next_logits.size(-1)))\n",
    "                next_logits[next_logits < v[:, [-1]]] = float(\"-inf\")\n",
    "            if top_p is not None and 0 < top_p < 1.0:\n",
    "                probs = torch.softmax(next_logits, dim=-1)\n",
    "                sp, si = torch.sort(probs, descending=True)\n",
    "                csum = torch.cumsum(sp, dim=-1)\n",
    "                keep = csum <= top_p\n",
    "                keep[..., 0] = True\n",
    "                filt = torch.full_like(sp, float(\"-inf\"))\n",
    "                filt[keep] = torch.log(sp[keep] + 1e-12)\n",
    "                next_logits = torch.full_like(next_logits, float(\"-inf\"))\n",
    "                next_logits[0, si[0]] = filt[0]\n",
    "        \n",
    "        if sents_done:\n",
    "            _, top8 = torch.topk(next_logits, k=8, dim=-1)\n",
    "            top8_ids = top8[0].tolist()\n",
    "            if any((TOKEN_STR.get(t) or '').strip() == ':' for t in top8_ids):\n",
    "                if len(gen) >= max(min_before_eos, min_tokens):\n",
    "                    next_logits[0, STOP] += 0.9\n",
    "\n",
    "        loop12 = (len(gen) >= 12 and len(set(gen[-12:])) <= 4)\n",
    "        loop10 = (len(gen) >= 10 and len(set(gen[-10:])) <= 3)\n",
    "\n",
    "        if loop12:\n",
    "            next_logits[0, STOP] += 1.6\n",
    "        elif loop10:\n",
    "            next_logits[0, STOP] += 0.8\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                p = torch.softmax(next_logits, dim=-1)\n",
    "                ent = float(-(p * torch.log(p + 1e-12)).sum())\n",
    "            if len(gen) >= 6 and ent < 3.4:\n",
    "                next_logits[0, STOP] += 0.8\n",
    "\n",
    "        if len(gen) >= max(min_before_eos, min_tokens) - 4 and PERIOD_TIDS:\n",
    "            next_logits[0, list(PERIOD_TIDS)] += 0.45\n",
    "\n",
    "        running_tail = bpe_tokenizer.decode(gen[-64:]) if gen else \"\"\n",
    "        if _NEG_HIT_RE.search((running_tail or \"\").lower()):\n",
    "            neg_window = max(neg_window, NEG_WINDOW_TOKENS)\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        _, cand_idx = torch.sort(probs, descending=True)\n",
    "        tmp_logits = next_logits.clone()\n",
    "        max_scan = min(256, cand_idx.size(-1))\n",
    "        for j in range(max_scan):\n",
    "            cid = int(cand_idx[0, j])\n",
    "            if cid == STOP:\n",
    "                continue\n",
    "\n",
    "            cand_str = TOKEN_STR.get(cid) or bpe_tokenizer.decode([cid])\n",
    "\n",
    "            if ANGLE_NOISE_PREFIX_RE.search(running_tail) and re.match(r\"[A-Za-z0-9/|>]\", cand_str):\n",
    "                tmp_logits[0, cid] = float(\"-inf\")\n",
    "                continue\n",
    "\n",
    "            if _unsafe_tail_would_appear(gen, cid) or (\n",
    "                no_repeat_ngram_size and _violates_ngram(gen, cid, no_repeat_ngram_size)\n",
    "            ):\n",
    "                tmp_logits[0, cid] = float(\"-inf\")\n",
    "                continue\n",
    "\n",
    "            if _would_complete_any(gen, cid, NEG_CONT_IDS, MAX_NEG_CONT):\n",
    "                penalty = NEG_CONT_PENALTY * (1.25 if neg_window > 0 else 1.0)\n",
    "                tmp_logits[0, cid] -= penalty\n",
    "            if _would_complete_any(gen, cid, POS_PIVOT_IDS, MAX_POS_PVT):\n",
    "                bonus = POS_PIVOT_BONUS * (1.25 if neg_window > 0 else 1.0)\n",
    "                tmp_logits[0, cid] += bonus\n",
    "\n",
    "        if torch.isneginf(tmp_logits).all():\n",
    "            fallback = next_logits.clone()\n",
    "            fallback[0, PAD] = float(\"-inf\")\n",
    "            if len(gen) < min_before_eos:\n",
    "                fallback[0, STOP] = float(\"-inf\")\n",
    "            next_id = int(torch.argmax(fallback, dim=-1))\n",
    "        else:\n",
    "            if do_sample:\n",
    "                samp_p = torch.softmax(tmp_logits, dim=-1)\n",
    "                next_id = int(torch.multinomial(samp_p[0], num_samples=1, generator=g))\n",
    "            else:\n",
    "                next_id = int(torch.argmax(tmp_logits, dim=-1))\n",
    "\n",
    "        if _unsafe_tail_would_appear(gen, next_id):\n",
    "            ok = False\n",
    "            for j2 in range(min(32, cand_idx.size(-1))):\n",
    "                alt = int(cand_idx[0, j2])\n",
    "                if alt == next_id or alt == PAD:\n",
    "                    continue\n",
    "                if len(gen) < MIN_BEFORE_EOS and alt == STOP:\n",
    "                    continue\n",
    "                if not _unsafe_tail_would_appear(gen, alt):\n",
    "                    next_id = alt; ok = True; break\n",
    "            if not ok:\n",
    "                fb = next_logits.clone()\n",
    "                fb[0, PAD] = float(\"-inf\")\n",
    "                if len(gen) < min_before_eos:\n",
    "                    fb[0, STOP] = float(\"-inf\")\n",
    "                next_id = int(torch.argmax(fb, dim=-1))\n",
    "\n",
    "        gen.append(next_id)\n",
    "        x = torch.cat([x, torch.tensor([[next_id]], device=dev)], dim=1)\n",
    "\n",
    "        text_so_far = bpe_tokenizer.decode(gen)\n",
    "        if len(gen) >= min_tokens:\n",
    "            if next_id == STOP: break\n",
    "            if \"<|END|>\" in text_so_far or \"<|EOT|>\" in text_so_far: break\n",
    "            if _count_sents(text_so_far) >= SENTENCE_LIMIT: break\n",
    "            if _STEP_PAT.search(text_so_far) or _SENT_PAT.search(text_so_far):\n",
    "                min_before_eos = 0\n",
    "                eos_boost = 3.0\n",
    "        if len(gen) >= max(min_tokens, 10) and len(set(gen[-10:])) <= 3:\n",
    "            break\n",
    "        if neg_window > 0:\n",
    "            neg_window -= 1\n",
    "\n",
    "    return gen\n",
    "\n",
    "\n",
    "def trim_to_last_period(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return s\n",
    "    if s.endswith((\".\", \"!\", \"?\")):\n",
    "        return s\n",
    "    i = s.rfind(\".\")\n",
    "    return s if i == -1 else s[:i+1]\n",
    "\n",
    "ANGLE_NOISE_PREFIX_RE = re.compile(r\"<[A-Za-z/|]{0,24}$\")\n",
    "\n",
    "_REFUSAL_RE = re.compile(\n",
    "    r\"(?i)\\b(i\\s+(?:don['’]t|cannot|can['’]?t(?!\\s+wait)|won['’]?t|am\\s+not\\s+able))\\b|\"\n",
    "    r\"\\b(unfortunately|sorry|unable to)\\b|\"\n",
    "    r\"\\b(as an?\\s+(?:ai|language model))\\b|\"\n",
    "    r\"access to the latest|real[-\\s]?time|browsing\"\n",
    ")\n",
    "\n",
    "def _score_answer(user_text: str, out: str, info_text: str = \"\") -> float:\n",
    "    score = 0.0\n",
    "    ol = out.strip()\n",
    "    ll = ol.lower()\n",
    "\n",
    "    if _REFUSAL_RE.search(ll): score -= 3.5\n",
    "\n",
    "    sents = len(re.findall(r'[.!?](?=\\s|$)', ol))\n",
    "    if   1 <= sents <= 3: score += 2.0\n",
    "    elif sents == 0: score -= 2.0\n",
    "    elif sents >= 6: score -= 0.8\n",
    "\n",
    "    nchar = len(ol)\n",
    "    if 40 <= nchar <= 320: score += 0.6\n",
    "    elif nchar < 25: score -= 0.6\n",
    "    elif nchar > 600: score -= 0.6\n",
    "\n",
    "    if re.search(r':\\s*\\d+\\.\\s*', ol): score -= 0.6\n",
    "    if re.search(r\"(?m)^\\s*\\d+\\.\\s+\", ol): score -= 1.0\n",
    "    if re.search(r\"\\b(\\w+)(?:\\s+\\1){2,}\\b\", ol): score -= 0.7\n",
    "    if re.search(r\"\\b(i\\s+can\\s+(help|do|explain|summarize|guide))\\b\", ll): score += 0.6\n",
    "    if re.search(r'(?i)\\b(identify|keyword|phrase|here(?:\\'|)s how)\\b', ol): score -= 1.0\n",
    "    if re.search(r'(?i)\\b(step\\s*\\d+|^\\s*[-•]\\s+)', ol, flags=re.M): score -= 1.0\n",
    "    if re.search(r\"(?i)\\bi am\\b|\\bi’m\\b|\\bi can\\b\", ol): score += 0.4\n",
    "    if re.search(r\":\\s*$\", ol): score -= 0.8\n",
    "    if \"<|END|>\" in ol or \"<|EOT|>\" in ol: score -= 1.0\n",
    "    if re.search(r'(?i)\\b(use this information|identify (the )?key (points|elements)|here(?:\\'|)s how)\\b', ol):\n",
    "        score -= 1.2\n",
    "    if re.search(r'(?i)\\b(language|words)\\b.*\\b(use[s]?\\b|\\bused\\b)', ol):\n",
    "        score -= 0.9\n",
    "\n",
    "    u_tokens = set(re.findall(r\"[a-zA-Z][\\w'-]*\", user_text))\n",
    "    i_tokens = set(re.findall(r\"[a-zA-Z][\\w'-]*\", info_text))\n",
    "    a_tokens = set(re.findall(r\"[a-zA-Z][\\w'-]*\", ol))\n",
    "    overlap = len((u_tokens | i_tokens) & a_tokens)\n",
    "    score += 0.02 * overlap\n",
    "    known_caps = _cap_words(user_text) | _cap_words(info_text)\n",
    "    out_caps   = _cap_words(ol)\n",
    "    novel_caps = [w for w in out_caps if w not in known_caps]\n",
    "    score -= 0.6 * len(novel_caps)\n",
    "\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "def _salvage_if_all_refusals(cands: list[str]) -> str | None:\n",
    "    bad = [c for c in cands if _REFUSAL_RE.search(c)]\n",
    "    if len(bad) == len(cands):\n",
    "        return min(cands, key=lambda t: len(_REFUSAL_RE.findall(t)))\n",
    "    return None\n",
    "\n",
    "def generate(info_text: str, user_text: str, greedy: bool=False, clean_style: bool=True, seed: int | None = None) -> str:\n",
    "\n",
    "    model = get_model(merge=True)\n",
    "    prompt_ids = build_prompt_ids(\n",
    "        user_text=user_text,\n",
    "        info_text=info_text,\n",
    "        system_text=SYSTEM_PROMPT_DEFAULT,\n",
    "        assist_prefix=ASSIST_PREFIX\n",
    "    )\n",
    "\n",
    "\n",
    "    user_words = {w for w in re.findall(r\"\\w+\", user_text.lower()) if len(w) >= 3 and w not in STOPWORDS}\n",
    "\n",
    "    decode = dict(\n",
    "        max_new_tokens=124,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "        seed=seed,\n",
    "        eos_token_id=END_ID,\n",
    "        pad_token_id=PAD_ID,\n",
    "    )\n",
    "\n",
    "    if len(user_text.strip()) <= 20:\n",
    "        decode[\"max_new_tokens\"] = min(decode[\"max_new_tokens\"], 80)\n",
    "\n",
    "    ban_ids = _build_ban_ids() if clean_style else set()\n",
    "\n",
    "    cands: list[str] = []\n",
    "    for i in range(NUM_CANDIDATES):\n",
    "        if seed is None:\n",
    "            g = torch.Generator(device=DEVICE).manual_seed(\n",
    "                (int(time.time_ns() & 0xFFFFFFFF) ^ 0xA5A5A5A5 ^ i) & 0xFFFFFFFF\n",
    "            )\n",
    "        else:\n",
    "            g = torch.Generator(device=DEVICE).manual_seed(int(seed) + i)\n",
    "\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            known_caps = _cap_words(bpe_tokenizer.decode(prompt_ids)) | _cap_words(\n",
    "                f\"{info_text or ''} {user_text or ''}\"\n",
    "            )\n",
    "\n",
    "            gen_ids = generate_ids(\n",
    "                model, prompt_ids, user_words,\n",
    "                min_tokens=16,\n",
    "                ban_token_ids=ban_ids,\n",
    "                generator=g,\n",
    "                known_caps=known_caps,\n",
    "                **decode\n",
    "            )\n",
    "        cands.append(bpe_tokenizer.decode(gen_ids).split(\"<|END|>\")[0])\n",
    "\n",
    "    cands = _dedupe_close(cands)\n",
    "    fallback = _salvage_if_all_refusals(cands)\n",
    "    if fallback is not None:\n",
    "        out = fallback\n",
    "    else:\n",
    "        out = max(cands, key=lambda t: _score_answer(user_text, t, info_text))\n",
    "\n",
    "    out = out.split(\"<|EOT|>\")[0]\n",
    "    out = re.sub(r\"\\s+\", \" \", out)\n",
    "    out = re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", out)\n",
    "    out = re.sub(r\"<\\|[^>]+?\\|>\", \"\", out).strip().strip('\"')\n",
    "    out = re.sub(\n",
    "    r\"<\\|?(?:PAD|UNKNOWN|START|END|SYSTEM|USER|ASSISTANT|EOT|INFOSTART|INFOEND)\\|?>\",\n",
    "        \"\",\n",
    "        out\n",
    "    )\n",
    "    out = re.sub(r'\\b(\\w+)\\s+and\\s+\\1\\b', r'\\1', out, flags=re.I)\n",
    "    out = re.sub(r\"<[A-Za-z0-9/|<>]{1,40}>?\", \"\", out)\n",
    "    out = re.sub(r\":\\s*(?:\\d+\\.\\s*)+\", \": \", out)\n",
    "    out = re.sub(r\"(?i)\\s*here(?:'|)s\\s+how.*?:\\s*\", \"\", out)\n",
    "    out = re.sub(r\"(?i)^\\s*(the (answer|sentence) is)\\s*[:\\-]?\\s*\", \"\", out)\n",
    "    out = re.sub(r\"\\b(\\w+)(?:\\s+\\1){2,}\\b\", r\"\\1\", out)\n",
    "    out = re.sub(r\"\\s{2,}\", \" \", out).strip()\n",
    "    out = re.sub(r\"(?i)\\s*(?:let me explain.*)$\", \"\", out).strip()\n",
    "    out = re.sub(r\"(?i)\\s*(?:i hope this (?:information )?helps!?).*$\", \"\", out).strip()\n",
    "    out = re.sub(r'(?i)\\b(?:identify|use)\\s+the\\s+(?:key\\s*)?(?:word|phrase)s?\\b.*?:\\s*', '', out)\n",
    "    out = re.sub(r\":\\s*$\", \".\", out)\n",
    "    out = re.sub(r':\\s*(?:\\d+\\.\\s*){1,6}', ': ', out)\n",
    "    out = re.sub(r'(?i)\\s*(?:i hope that helps!?|let me know if you have (?:any )?other questions.*)$', '', out)\n",
    "    out = re.sub(r'(?m)^\\s*(?:[-•]|\\d+\\.)\\s+.*$', '', out)\n",
    "    out = re.sub(r'\\b(\\w+)(?:\\s*,\\s*\\1){1,}\\b', r'\\1', out)\n",
    "    out = re.sub(r'\\b(\\w+)(?:\\s+\\1){1,}\\b', r'\\1', out)\n",
    "\n",
    "    if not re.search(r\"(?i)\\b(list|steps|bullets|outline|numbered)\\b\", user_text):\n",
    "        out = re.sub(r\"(?m)^\\s*\\d+\\.\\s+\", \"\", out)\n",
    "    \n",
    "    out = _soft_tail_trim(out)\n",
    "    out = _trim_sents(out, SENTENCE_LIMIT)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User typed: Who are you and what can you do?\n",
      "Memory has: You are a supportive artificial intelligence.\n",
      "Model answer: I am a supportive artificial intelligence. I can help you learn about the language of the people who are in need. Please provide me with information on what you are doing.\n",
      "\n",
      "User typed: Hello, I made you. It's been a month and half since I start this project making small LLM, and I'm so glad that I finished it!\n",
      "Memory has: \n",
      "Model answer: I'm glad you found it.\n",
      "\n",
      "User typed: What's your thought on the humanity?\n",
      "Memory has: \n",
      "Model answer: \"I am a man.\" The humanity is a group of people who have been working for years in the world. They are not like people who work together and do not work together. They are like people who are different, but they are different.\n",
      "\n",
      "User typed: Hi. What's my name?\n",
      "Memory has: User's name is Insoo Son.\n",
      "Model answer: Insoo Son. Insoo Son is a name that is used to describe a person's actions. It is a name that is used to describe a person's actions or actions.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m model_answer = generate(info_text=extra_information, user_text=user_question)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43ms\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "user_question = \"Who are you and what can you do?\"\n",
    "extra_information = \"You are a supportive artificial intelligence.\"\n",
    "print(f\"User typed: {user_question}\")\n",
    "print(f\"Memory has: {extra_information}\")\n",
    "model_answer = generate(info_text=extra_information, user_text=user_question)\n",
    "print(f\"Model answer: {model_answer}\\n\")\n",
    "\n",
    "user_question = \"Hello, I made you. It's been a month and half since I start this project making small LLM, and I'm so glad that I finished it!\"\n",
    "extra_information = \"\"\n",
    "print(f\"User typed: {user_question}\")\n",
    "print(f\"Memory has: {extra_information}\")\n",
    "model_answer = generate(info_text=extra_information, user_text=user_question)\n",
    "print(f\"Model answer: {model_answer}\\n\")\n",
    "\n",
    "user_question = \"What's your thought on the humanity?\"\n",
    "extra_information = \"\"\n",
    "print(f\"User typed: {user_question}\")\n",
    "print(f\"Memory has: {extra_information}\")\n",
    "model_answer = generate(info_text=extra_information, user_text=user_question)\n",
    "print(f\"Model answer: {model_answer}\\n\")\n",
    "\n",
    "user_question = \"Hi. What's my name?\"\n",
    "extra_information = \"User's name is Insoo Son.\"\n",
    "print(f\"User typed: {user_question}\")\n",
    "print(f\"Memory has: {extra_information}\")\n",
    "model_answer = generate(info_text=extra_information, user_text=user_question)\n",
    "print(f\"Model answer: {model_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
