{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bceabb",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16378e",
   "metadata": {},
   "source": [
    "Ok. We basically have 3 types of level for tokenizer <br/>\n",
    "- Word-level\n",
    "- Character-level\n",
    "- Byte-level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6508a63",
   "metadata": {},
   "source": [
    "## Word-level\n",
    "- What it does?\n",
    "    - Take each whole word as a token; unseen words becomes <|UNKNOWN|> (or something else if you were to customize it)\n",
    "- Pros\n",
    "    - Short sequences on clean text since it takes whole word as a token and we grab them\n",
    "- Cons\n",
    "    - Huge vocab \n",
    "        - english has over one million words, meaning we'll have to tokenize one million words... (that's huge)\n",
    "        \n",
    "    - Weak for multilingual text \n",
    "        - say for example, we need to tokenize Korean. \"안녕하세요\" then we need to tokenize every Korean words as well\n",
    "        - and that's gonna be one million (from english) + 100,000 (from korean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18109a83",
   "metadata": {},
   "source": [
    "## Character-level\n",
    "- What it does?\n",
    "    - Parse each character to Unicode character; then, tokenize it\n",
    "- Pros\n",
    "    - Vocab size is tinier than Word-level\n",
    "- Cons\n",
    "    - VERY long sequence\n",
    "        - Think of sentence \"Hello, World! I got a call from mom...\"\n",
    "        - Then, we tokenize every letter (we're not considering repeats like \"ll\" from \"Hello\" and \"call\")\n",
    "        - And that's gonna be pretty lot of work\n",
    "        - Which is gonna result slow training speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f555",
   "metadata": {},
   "source": [
    "# Byte-level\n",
    "- What it does?\n",
    "    - Iterate through the text, find repeats, tokenize them into byte (0-255)\n",
    "- Pros\n",
    "    - Tiny fixed vocab size covers every word.\n",
    "- Cons\n",
    "    - Long sequence; needs bigger models to match subword efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb439baf",
   "metadata": {},
   "source": [
    "# For this project, we'll be using Byte-level with subword family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451edf7",
   "metadata": {},
   "source": [
    "## Idea\n",
    "Think of months from January to December.\n",
    "- january = j + an + uary\n",
    "- february = febr + uary\n",
    "- march = m + a + rch\n",
    "- april = a + pril\n",
    "- may = m + a + y\n",
    "- june = j + une\n",
    "- july = j + uly\n",
    "- august = a + ugust\n",
    "- september = sept + em + ber\n",
    "- october = oct + o + ber\n",
    "- november = nov + em + ber\n",
    "- december = dec + em + ber.\n",
    "\n",
    "then sequence: <br/>\n",
    "J, nov, dec, febr, m, sept, oct, em, uly, a, une, an, ber, y, uary, rch, pril, ugust<br/>\n",
    "can cover all of those\n",
    "\n",
    "Then we're gonna assign unique id for each of those, save it into .json\n",
    "Then we're gonna save what to merge in .txt\n",
    "\n",
    "Below is a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f29eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Example small corpus\n",
    "corpus = [\n",
    "    \"january\",\n",
    "    \"february\",\n",
    "    \"march\",\n",
    "    \"april\",\n",
    "    \"may\",\n",
    "    \"june\",\n",
    "    \"july\",\n",
    "    \"august\",\n",
    "    \"september\",\n",
    "    \"october\",\n",
    "    \"november\",\n",
    "    \"december\",\n",
    "]\n",
    "\n",
    "# Build vocabulary: word → frequency of symbol sequence\n",
    "# We append <|EOW|> to mark end-of-word\n",
    "def build_vocab(corpus):\n",
    "    vocab = Counter()\n",
    "    for word in corpus:\n",
    "        symbols = list(word) + ['<|EOW|>']\n",
    "        vocab[tuple(symbols)] += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d28b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_stats(vocab):\n",
    "    \"\"\"Count all adjacent symbol-pair frequencies.\"\"\"\n",
    "    pairs = Counter()\n",
    "    for symbols, freq in vocab.items():\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pairs[pair] += freq\n",
    "    return pairs\n",
    "\n",
    "stats = get_pair_stats(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e533bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pair, vocab):\n",
    "    \"\"\"\n",
    "    Merge all occurrences of `pair` in the vocab into a single symbol.\n",
    "    E.g. ('j','a') → 'j a' becomes 'ja'\n",
    "    \"\"\"\n",
    "    merged_vocab = {}\n",
    "    bigram = pair\n",
    "    replacement = ''.join(pair)\n",
    "    for symbols, freq in vocab.items():\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            # if the pair matches at this position, merge it\n",
    "            if i < len(symbols)-1 and (symbols[i], symbols[i+1]) == bigram:\n",
    "                new_symbols.append(replacement)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        merged_vocab[tuple(new_symbols)] = freq\n",
    "    return merged_vocab\n",
    "\n",
    "# merge the top pair\n",
    "best_pair = stats.most_common(1)[0][0]\n",
    "vocab = merge_pair(best_pair, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a861b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned merges: [('b', 'e'), ('be', 'r'), ('ber', '<|EOW|>'), ('a', 'r'), ('e', 'm'), ('em', 'ber<|EOW|>'), ('u', 'ar'), ('uar', 'y<|EOW|>'), ('j', 'u'), ('j', 'a')]\n"
     ]
    }
   ],
   "source": [
    "def learn_bpe(vocab, num_merges):\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        stats = get_pair_stats(vocab)\n",
    "        if not stats:\n",
    "            break\n",
    "        best_pair, _ = stats.most_common(1)[0]\n",
    "        merges.append(best_pair)\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "    return merges\n",
    "\n",
    "# Learn, say, 10 merges on our tiny corpus:\n",
    "bpe_merges = learn_bpe(vocab, num_merges=10)\n",
    "print(\"Learned merges:\", bpe_merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ja', 'n', 'uar', 'y', '<|EOW|>']\n"
     ]
    }
   ],
   "source": [
    "def apply_bpe(token, merges):\n",
    "    # start from character sequence + <|EOW|>\n",
    "    symbols = list(token) + ['<|EOW|>']\n",
    "    # repeatedly apply merges in order\n",
    "    for pair in merges:\n",
    "        i = 0\n",
    "        new_symbols = []\n",
    "        replacement = ''.join(pair)\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols)-1 and (symbols[i], symbols[i+1]) == pair:\n",
    "                new_symbols.append(replacement)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        symbols = new_symbols\n",
    "    return symbols\n",
    "\n",
    "# Example:\n",
    "print(apply_bpe(\"january\", bpe_merges))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d39e331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "september → ['s', 'e', 'p', 't', 'ember<|EOW|>']\n",
      "october → ['o', 'c', 't', 'o', 'ber<|EOW|>']\n",
      "november → ['n', 'o', 'v', 'ember<|EOW|>']\n",
      "december → ['d', 'e', 'c', 'ember<|EOW|>']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    corpus = [\n",
    "    \"january\",\n",
    "    \"february\",\n",
    "    \"march\",\n",
    "    \"april\",\n",
    "    \"may\",\n",
    "    \"june\",\n",
    "    \"july\",\n",
    "    \"august\",\n",
    "    \"september\",\n",
    "    \"october\",\n",
    "    \"november\",\n",
    "    \"december\",\n",
    "    ]\n",
    "    vocab = build_vocab(corpus)\n",
    "    merges = learn_bpe(vocab, num_merges=10)\n",
    "    \n",
    "    for word in corpus[8:]: # print last 4 for example\n",
    "        tokens = apply_bpe(word, merges)\n",
    "        print(word, \"→\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ff436",
   "metadata": {},
   "source": [
    "# Using ByteLevelBPETokenizer from tokenizers library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b70cf5",
   "metadata": {},
   "source": [
    "To make our life easier, we'll use ByteLevelBPETokenizer from tokenizers library. <br/>\n",
    "The only difference is: <br/>\n",
    "- Units are bytes, not Unicode characters\n",
    "- Instead of <|EOW|>, it's using Ġ\n",
    "- It doesn't normalize text; leading space is consistently captured\n",
    "- It uses Rust (fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18701e6c",
   "metadata": {},
   "source": [
    "First, we need books for text. <br/>\n",
    "I'm gonna use books in Gutenburg.org (since they're licence free) <br/>\n",
    "For the real one, i fed 75,000 books. But that's like 35 GB. <br/>\n",
    "So, i'm gonna be using 100 books instead. <br/>\n",
    "(You can download the full version in here: https://www.kaggle.com/datasets/lokeshparab/gutenberg-books-and-metadata-2025?resource=download&select=books)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9f804",
   "metadata": {},
   "source": [
    "## **If you're testing full, turn this to true**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e380eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfull=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aa640",
   "metadata": {},
   "source": [
    "First, we're gona merge all books into text file called \"all_books.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d900d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating books: 100%|██████████| 72082/72082 [16:10<00:00, 74.29it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import glob, os\n",
    "from pathlib import Path\n",
    "\n",
    "if isfull:\n",
    "    out_dir = Path(\"../materials\") # use this for full dataset\n",
    "else:\n",
    "    out_dir = Path(\"../materials_small\")\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if isfull:\n",
    "    files = [f for f in glob.glob(\"../books/*\") if os.path.isfile(f)] # use this for full dataset\n",
    "else: \n",
    "    files = [f for f in glob.glob(\"../books_small/*\") if os.path.isfile(f)]\n",
    "\n",
    "with open(out_dir.__str__()+\"/all_books.txt\", \"w\", encoding=\"utf-8\", errors=\"ignore\") as out:\n",
    "    for fn in tqdm(files, desc=\"Concatenating books\"):\n",
    "        with open(fn, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            out.write(f.read())\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "# You'll get all_books.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71586daa",
   "metadata": {},
   "source": [
    "The real one took 17m 10s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9935db",
   "metadata": {},
   "source": [
    "Then use ByteLevelBPETokenizer from tokenizers to generate bpe_model-vocab.json and bpe_model-merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b365249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total lines in the file for tqdm progress bar\n",
      "Counting lines in the file...\n",
      "start training BPE tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding lines: 100%|██████████| 583802534/583802534 [26:59<00:00, 360589.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE tokenizer training complete\n",
      "BPE model saved to bpe(_small) directory\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Counting total lines in the file for tqdm progress bar\")\n",
    "if isfull:\n",
    "    file_path = \"../materials/all_books.txt\" # use this for full dataset\n",
    "else:\n",
    "    file_path = \"../materials_small/all_books.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    print(\"Counting lines in the file...\")\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "def line_iterator(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in tqdm(f, total=total_lines, desc=\"Feeding lines\"):\n",
    "            yield line\n",
    "\n",
    "print(\"start training BPE tokenizer\")\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    line_iterator(file_path),\n",
    "    vocab_size=60000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"<|PAD|>\", \"<|UNKNOWN|>\", \"<|START|>\", \"<|END|>\", \"<|SYSTEM|>\", \"<|USER|>\", \"<|ASSISTANT|>\", \"<|EOT|>\",\"<|INFOSTART|>\",\"<|INFOEND|>\"]\n",
    ")\n",
    "print(\"BPE tokenizer training complete\")\n",
    "\n",
    "if isfull:\n",
    "    out_dir = Path(\"../bpe\") # use this for full dataset\n",
    "else:\n",
    "    out_dir = Path(\"../bpe_small\")\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if isfull:\n",
    "    tokenizer.save_model(\"../bpe\", \"bpe_model\")\n",
    "else:\n",
    "    tokenizer.save_model(\"../bpe_small\", \"bpe_model\") \n",
    "print(\"BPE model saved to bpe(_small) directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f023c1",
   "metadata": {},
   "source": [
    "Ok. small one took around 7m 30s <br/>\n",
    "6 minutes for feeding phase <br/>\n",
    "rest for merging phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807c7a4",
   "metadata": {},
   "source": [
    "The real one took: <br/>\n",
    "<br/>\n",
    "RTX3080: <br/>\n",
    "total: 43m 30.6s<br/>\n",
    "feeding phase: 28m 3s<br/>\n",
    "merging phase: around 15m<br/>\n",
    "<br/>\n",
    "RTX3060: <br/>\n",
    "total: around 2 hours <br/>\n",
    "feeding phase: 1h 27m 15s <br/>\n",
    "merging phase: around 33m <br/>\n",
    "\n",
    "starting next one, we're gonna be using the one I've done with RTX3080 <br/>\n",
    "(saving time. ⭐ Life Good ⭐)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079193a",
   "metadata": {},
   "source": [
    "## DONE! If you see bpe folder, and you see vocab.json and materials(_small)/all_books.txt, you're good to go"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
