{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89110280",
   "metadata": {},
   "source": [
    "# Pipe line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fe280",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Instantiate & Sanity Check Tokenizer\n",
    "2. Clean & Normalize Raw Text\n",
    "3. Encode Entire Corpus into Token IDs\n",
    "4. Split into Train / Validation\n",
    "5. Chunk into fixed length blocks\n",
    "6. Prepare for smoke test\n",
    "7. Define & Save model config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb8665",
   "metadata": {},
   "source": [
    "# 1. **Instantiate & Sanity-Check Tokenizer**\n",
    "\n",
    "   * Load your vocab/merges into a `ByteLevelBPETokenizer`.\n",
    "   * Encode a few hand-picked sentences to verify you get reasonable token IDs and that special tokens (in the previous \"tokenizer.ipynb,\" I've customized to `<|UNKNOWN|>`, `<|START|>`, `<|END|>`) are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29d521",
   "metadata": {},
   "source": [
    "## **If you're testing full, turn this to true**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc809eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfull=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c514e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = \"../bpe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9988efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 60000\n",
      "<|PAD|>     ‚Üí id 0\n",
      "<|UNKNOWN|> ‚Üí id 1\n",
      "<|START|>   ‚Üí id 2\n",
      "<|END|>     ‚Üí id 3\n",
      "<|SYSTEM|>  ‚Üí id 4\n",
      "<|USER|>    ‚Üí id 5\n",
      "<|ASSISTANT|>‚Üí id 6\n",
      "<|EOT|>     ‚Üí id 7\n",
      "<|INFOSTART|>‚Üí id 8\n",
      "<|INFOEND|> ‚Üí id 9\n",
      "\n",
      "--------LOG--------\n",
      "NOTE: ƒ† is a leading space\n",
      "\n",
      "Tokens: ['<|START|>', 'ƒ†H', 'ello', ',', 'ƒ†world', '!', 'ƒ†', '<|END|>']\n",
      "Decoded:  Hello, world! \n",
      "Original: <|START|> Hello, world! <|END|>\n",
      "(it shouldn't print special tokens)\n",
      "\n",
      "\n",
      "Tokens: ['ƒ†This', 'ƒ†is', 'ƒ†a', 'ƒ†test', '.']\n",
      "Decoded:  This is a test.\n",
      "Original: This is a test.\n",
      "\n",
      "\n",
      "Tokens: ['ƒ†', '√∞', 'ƒø', 'ƒÆ', 'ƒ®', 'ƒ†This', '-', 'is', '√¢ƒ¢ƒµ', 'we', 'ird', '?!']\n",
      "Decoded:  ùåÜ This-is‚Äìweird?!\n",
      "Original: ùåÜ This-is‚Äìweird?!\n",
      "\n",
      "\n",
      "Tokens: ['ƒ†', '√∞', '≈Å', 'ƒ∫', 'ƒ¢', 'ƒ†em', 'oj', 'i', 'ƒ†becomes', 'ƒ†√É¬∞', 'ƒ†√Ö', 'ƒ£', 'ƒ†√Ñ', '¬∫', 'ƒ†√Ñ', '¬¢']\n",
      "Decoded:  üòÄ emoji becomes √∞ ≈Å ƒ∫ ƒ¢\n",
      "Original: üòÄ emoji becomes √∞ ≈Å ƒ∫ ƒ¢\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    bpe_path + \"/bpe_model-vocab.json\",\n",
    "    bpe_path + \"/bpe_model-merges.txt\",\n",
    "    lowercase=False,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "# special tokens\n",
    "tokenizer.add_special_tokens([\n",
    "    \"<|PAD|>\", \n",
    "    \"<|UNKNOWN|>\", \n",
    "    \"<|START|>\", \n",
    "    \"<|END|>\", \n",
    "    \"<|SYSTEM|>\", \n",
    "    \"<|USER|>\", \n",
    "    \"<|ASSISTANT|>\", \n",
    "    \"<|EOT|>\",\n",
    "    \"<|INFOSTART|>\",\n",
    "    \"<|INFOEND|>\"\n",
    "])\n",
    "\n",
    "# sanity check\n",
    "print(\"Vocab size:\", tokenizer.get_vocab_size())\n",
    "for t in [\"<|PAD|>\", \"<|UNKNOWN|>\", \"<|START|>\", \"<|END|>\", \"<|SYSTEM|>\", \"<|USER|>\", \"<|ASSISTANT|>\", \"<|EOT|>\",\"<|INFOSTART|>\",\"<|INFOEND|>\"]:\n",
    "    print(f\"{t:12s}‚Üí id {tokenizer.token_to_id(t)}\")\n",
    "\n",
    "# logs\n",
    "print(\"\\n--------LOG--------\")\n",
    "print(\"NOTE: ƒ† is a leading space\\n\")\n",
    "\n",
    "enc = tokenizer.encode(\"<|START|> Hello, world! <|END|>\")\n",
    "print(\"Tokens:\", enc.tokens)\n",
    "dec1 = tokenizer.decode(enc.ids)\n",
    "print(\"Decoded:\", dec1)\n",
    "print(\"Original: <|START|> Hello, world! <|END|>\")\n",
    "print(\"(it shouldn't print special tokens)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "enc2 = tokenizer.encode(\"This is a test.\")\n",
    "print(\"Tokens:\", enc2.tokens)\n",
    "dec2 = tokenizer.decode(enc2.ids)\n",
    "print(\"Decoded:\", dec2)\n",
    "print(\"Original: This is a test.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "enc3 = tokenizer.encode(\"ùåÜ This-is‚Äìweird?!\")\n",
    "print(\"Tokens:\", enc3.tokens)\n",
    "dec3 = tokenizer.decode(enc3.ids)\n",
    "print(\"Decoded:\", dec3)\n",
    "print(\"Original: ùåÜ This-is‚Äìweird?!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "enc4 = tokenizer.encode(\"üòÄ emoji becomes √∞ ≈Å ƒ∫ ƒ¢\")\n",
    "print(\"Tokens:\", enc4.tokens)\n",
    "dec4 = tokenizer.decode(enc4.ids)\n",
    "print(\"Decoded:\", dec4)\n",
    "print(\"Original: üòÄ emoji becomes √∞ ≈Å ƒ∫ ƒ¢\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447cf02",
   "metadata": {},
   "source": [
    "# 2. **Clean & Normalize Raw Text**\n",
    "   * Unicode-normalize (NFC), strip control characters, normalize punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad31f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 27.1G/27.6G [14:02<00:16, 34.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Done cleaning ‚Üí ../materials/all_books_clean.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# stuffs\n",
    "if isfull:\n",
    "    INPUT  = \"../materials/all_books.txt\"\n",
    "    OUTPUT = \"../materials/all_books_clean.txt\"\n",
    "else:\n",
    "    INPUT  = \"../materials_small/all_books.txt\"\n",
    "    OUTPUT = \"../materials_small/all_books_clean.txt\"\n",
    "\n",
    "# prog bar\n",
    "filesize = os.path.getsize(INPUT)\n",
    "\n",
    "# configure control characters\n",
    "# i used \"https://www.ascii-code.com/\" for below\n",
    "CTRL_CHARS = re.compile(\n",
    "    \"[\" +\n",
    "    # 0-31 is ASCII control range. But we want to exclude \n",
    "    # tabs (\\t = 9), newlines (\\n = 10), and carriage returns (\\r = 13)\n",
    "    \"\".join(chr(c) for c in range(0,32) if c not in (9,10,13)) +\n",
    "    # also, add all characters in 127-159\n",
    "    \"\".join(chr(c) for c in range(127,160)) +\n",
    "    \"]\"\n",
    ")\n",
    "\n",
    "# cleaning things\n",
    "with open(INPUT, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin, \\\n",
    "     open(OUTPUT, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    # prog bar\n",
    "    pbar = tqdm(total=filesize,\n",
    "                unit=\"B\", unit_scale=True, unit_divisor=1024,\n",
    "                desc=\"Cleaning\")\n",
    "\n",
    "    in_body = True\n",
    "    for line in fin:\n",
    "        pbar.update(len(line.encode(\"utf-8\")))\n",
    "\n",
    "        text = unicodedata.normalize(\"NFC\", line)\n",
    "        # if we ever find CTRL_CHARS, replace that to empty string\n",
    "        text = CTRL_CHARS.sub(\"\", text)\n",
    "\n",
    "        fout.write(text)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "print(f\"‚úì Done cleaning ‚Üí {OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47195e1e",
   "metadata": {},
   "source": [
    "Real run:\n",
    "\n",
    "RTX3080: <br/>\n",
    "runtime: 14m 44s<br/>\n",
    "\n",
    "RTX3060: <br/>\n",
    "runtime: 38m 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd56696",
   "metadata": {},
   "source": [
    "# 3. **Encode Entire Corpus into Token IDs**\n",
    "\n",
    "   * Read your cleaned text in chunks (per line).\n",
    "   * Run your tokenizer over each chunk (e.g. `encode_batch`) to produce a flat 1D array of token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1b5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_vocab_path = \"../bpe/bpe_model-vocab.json\"\n",
    "bpe_merges_path = \"../bpe/bpe_model-merges.txt\"\n",
    "\n",
    "if isfull:\n",
    "    clean_books_path = \"../materials/all_books_clean.txt\"\n",
    "    clean_books_id_path = \"../materials/all_books_ids.npy\"\n",
    "else:\n",
    "    clean_books_path = \"../materials_small/all_books_clean.txt\"\n",
    "    clean_books_id_path = \"../materials_small/all_books_ids.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ce7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total tokens...\n",
      "Total tokens: 7,373,891,517ens\n",
      "Streaming tokenization into memmap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 29.1G/29.6G [1:16:49<01:32, 6.30MB/s, tokens=Processed 7,373,891,517 tokens]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Wrote 7,373,891,517 token IDs to all_books_ids.npy\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# life-good code\n",
    "# if there's existing all_books_ids.npy, remove that\n",
    "if os.path.exists(clean_books_id_path):\n",
    "    print(f\"Removing existing file: {clean_books_id_path}\")\n",
    "    os.remove(clean_books_id_path)\n",
    "\n",
    "# load tokenizer & configure special tokens\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    bpe_vocab_path,\n",
    "    bpe_merges_path,\n",
    "    lowercase=False,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "tokenizer.add_special_tokens([\"<|PAD|>\", \"<|UNKNOWN|>\", \"<|START|>\", \"<|END|>\", \"<|SYSTEM|>\", \"<|USER|>\", \"<|ASSISTANT|>\", \"<|EOT|>\",\"<|INFOSTART|>\",\"<|INFOEND|>\"])\n",
    "\n",
    "# settings\n",
    "INPUT       = clean_books_path # we're going to process with the cleaned version\n",
    "BATCH_LINES = 100_000 # encode 100,000 line per iteration\n",
    "DTYPE       = np.int32 # we want tokenid to be an integer type\n",
    "\n",
    "# count total tokens\n",
    "print(\"Counting total tokens...\")\n",
    "total_tokens = 0\n",
    "with open(INPUT, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    while True:\n",
    "        batch = list(islice(f, BATCH_LINES))\n",
    "        if not batch:\n",
    "            break\n",
    "        encs = tokenizer.encode_batch(batch)\n",
    "        total_tokens += sum(len(enc.ids) for enc in encs)\n",
    "        print(f\"Processed {total_tokens:,} tokens\", end=\"\\r\")\n",
    "\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "\n",
    "# allocate memmap\n",
    "# since the real one has over billions of tokens, we'll use memmap to prevent RAM crash (due to memory)\n",
    "mm = np.lib.format.open_memmap(\n",
    "    clean_books_id_path,\n",
    "    mode=\"w+\",\n",
    "    dtype=DTYPE,\n",
    "    shape=(total_tokens,)\n",
    ")\n",
    "\n",
    "# fill memmap with streaming tokenization\n",
    "print(\"Streaming tokenization into memmap...\")\n",
    "filesize = os.path.getsize(INPUT)\n",
    "pbar = tqdm(total=filesize, unit=\"B\", unit_scale=True, desc=\"Tokenizing\")\n",
    "\n",
    "# just for progress bar\n",
    "offset = 0\n",
    "\n",
    "# main loop\n",
    "with open(INPUT, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    while True:\n",
    "        # grab 100,000 tokens in each iteration\n",
    "        batch = list(islice(f, BATCH_LINES))\n",
    "        if not batch:\n",
    "            # if we have 0 tokens, that means we're done.\n",
    "            # so end the loop\n",
    "            break\n",
    "        \n",
    "        # encode the tokens\n",
    "        encs = tokenizer.encode_batch(batch)\n",
    "        for enc in encs:\n",
    "            length = len(enc.ids)\n",
    "            mm[offset:offset + length] = enc.ids\n",
    "            offset += length\n",
    "\n",
    "        # update bar by bytes read\n",
    "        pbar.update(sum(len(line.encode(\"utf-8\")) for line in batch))\n",
    "        pbar.set_postfix(tokens=f\"Processed {offset:,} tokens\")\n",
    "\n",
    "pbar.close()\n",
    "del mm # now that we're done, we're gonna terminate memmap\n",
    "\n",
    "print(f\"‚úì Wrote {total_tokens:,} token IDs to all_books_ids.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794544d7",
   "metadata": {},
   "source": [
    "**Small one:** <br/>\n",
    "total tokens: 338,565,967 tokens <br/>\n",
    "RTX3060: <br/>\n",
    "runtime: 12m 13s\n",
    "\n",
    "**Real one:** <br/>\n",
    "total tokens: 7,690,145,918 tokens (30.3GB) <br/>\n",
    "RTX3060: <br/>\n",
    "runtime: 2h 6m 8s\n",
    "\n",
    "RTX3080: <br/>\n",
    "runtime: 2h 10m 47.8s <br/>\n",
    "(well... this is with lots of background applications... so, yup. reasonable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b672359",
   "metadata": {},
   "source": [
    "# 4. **Split into Train / Validation**\n",
    "\n",
    "   * Choose a split (I'm using 95% for train and 10% for validation).\n",
    "   * Slice your ID array accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e105105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving ../materials/train_ids.npy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7006/7006 [03:56<00:00, 29.63tokens/s] \n",
      "Saving ../materials/valid_ids.npy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 369/369 [00:45<00:00,  8.19tokens/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "# load .npy\n",
    "if isfull:\n",
    "    ids = np.load(\"../materials/all_books_ids.npy\")\n",
    "else:\n",
    "    ids = np.load(\"../materials_small/all_books_ids.npy\")\n",
    "\n",
    "# split train & validation\n",
    "split_ratio = 0.95\n",
    "split_index = int(len(ids) * split_ratio)\n",
    "\n",
    "train_ids = ids[:split_index]\n",
    "valid_ids = ids[split_index:]\n",
    "\n",
    "def save_npy_with_progress(filename, array, chunk_size=1_000_000):\n",
    "    mm = np.lib.format.open_memmap(filename, mode='w+', \n",
    "                                   dtype=array.dtype, shape=array.shape)\n",
    "    for start in trange(0, array.shape[0], chunk_size,\n",
    "                        desc=f\"Saving {filename}\", unit=\"tokens\"):\n",
    "        end = min(start + chunk_size, array.shape[0])\n",
    "        mm[start:end] = array[start:end]\n",
    "    del mm\n",
    "\n",
    "# save with progress\n",
    "if isfull:\n",
    "    save_npy_with_progress(\"../materials/train_ids.npy\", train_ids)\n",
    "    save_npy_with_progress(\"../materials/valid_ids.npy\", valid_ids)\n",
    "else:\n",
    "    save_npy_with_progress(\"../materials_small/train_ids.npy\", train_ids)\n",
    "    save_npy_with_progress(\"../materials_small/valid_ids.npy\", valid_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdf338",
   "metadata": {},
   "source": [
    "# 5. **Chunk into Fixed-Length Blocks**\n",
    "\n",
    "   * context will be 1024 (tokens).\n",
    "   * Break each subset into non-overlapping blocks of exactly that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd53c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking train blocks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6841012/6841012 [01:53<00:00, 60291.39it/s]\n",
      "Chunking valid blocks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 360053/360053 [00:05<00:00, 60106.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 6841012 train blocks and 360053 valid blocks.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# param\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "# chunk Train IDs into BLOCK_SIZE\n",
    "if isfull:\n",
    "    train_ids = np.load(\"../materials/train_ids.npy\", mmap_mode=\"r\")\n",
    "else:\n",
    "    train_ids = np.load(\"../materials_small/train_ids.npy\", mmap_mode=\"r\")\n",
    "num_train_blocks = len(train_ids) // BLOCK_SIZE\n",
    "\n",
    "if isfull: \n",
    "    writetrain = \"../materials/train_blocks.npy\"\n",
    "else:\n",
    "    writetrain = \"../materials_small/train_blocks.npy\"\n",
    "\n",
    "train_blocks = np.lib.format.open_memmap(\n",
    "    writetrain, mode=\"w+\",\n",
    "    dtype=train_ids.dtype, shape=(num_train_blocks, BLOCK_SIZE)\n",
    ")\n",
    "for i in tqdm(range(num_train_blocks), desc=\"Chunking train blocks\"):\n",
    "    start = i * BLOCK_SIZE\n",
    "    train_blocks[i] = train_ids[start:start + BLOCK_SIZE]\n",
    "del train_blocks  # flush to disk\n",
    "\n",
    "# chunk Valid IDs into BLOCK_SIZE\n",
    "if isfull:\n",
    "    valid_ids = np.load(\"../materials/valid_ids.npy\", mmap_mode=\"r\")\n",
    "else:\n",
    "    valid_ids = np.load(\"../materials_small/valid_ids.npy\", mmap_mode=\"r\")\n",
    "\n",
    "num_valid_blocks = len(valid_ids) // BLOCK_SIZE\n",
    "\n",
    "if isfull:\n",
    "    writevalid = \"../materials/valid_blocks.npy\"\n",
    "else:\n",
    "    writevalid = \"../materials_small/valid_blocks.npy\"\n",
    "\n",
    "valid_blocks = np.lib.format.open_memmap(\n",
    "    writevalid, mode=\"w+\",\n",
    "    dtype=valid_ids.dtype, shape=(num_valid_blocks, BLOCK_SIZE)\n",
    ")\n",
    "for i in tqdm(range(num_valid_blocks), desc=\"Chunking valid blocks\"):\n",
    "    start = i * BLOCK_SIZE\n",
    "    valid_blocks[i] = valid_ids[start:start + BLOCK_SIZE]\n",
    "\n",
    "del valid_blocks  # flush to disk\n",
    "\n",
    "print(f\"‚úì Created {num_train_blocks} train blocks and {num_valid_blocks} valid blocks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe504132",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec29936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** START OF THE PROJECT GUTENBERG EBOOK 1 ***\n",
      " \n",
      " ===========================================================\n",
      " \n",
      "     NOTE:  This file combines the first two Project Gutenberg\n",
      "     files, both of which were given the filenumber #1. There are\n",
      "     several duplicate files here. There were many updates over\n",
      "     the years.  All of the original files are included in the\n",
      "     \"old\" subdirectory which may be accessed under the \"More\n",
      "     Files\" listing in the PG Catalog of this file. No changes\n",
      "     h\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "if isfull:\n",
    "    train_blocks = np.load(\"../materials/train_blocks.npy\")\n",
    "else:\n",
    "    train_blocks = np.load(\"../materials_small/train_blocks.npy\")\n",
    "\n",
    "# decode block 0 back to text and eyeball it\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tok = ByteLevelBPETokenizer(\"../bpe/bpe_model-vocab.json\", \"../bpe/bpe_model-merges.txt\", lowercase=False, add_prefix_space=True)\n",
    "tok.add_special_tokens([\"<|PAD|>\", \"<|UNKNOWN|>\", \"<|START|>\", \"<|END|>\", \"<|SYSTEM|>\", \"<|USER|>\", \"<|ASSISTANT|>\", \"<|EOT|>\",\"<|INFOSTART|>\",\"<|INFOEND|>\"])\n",
    "sample_text = tok.decode(train_blocks[0].tolist())\n",
    "print(sample_text[:500])\n",
    "\n",
    "# verify that the output is readable. (it's a part of clean books.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ec42b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train blocks shape: (6841012, 1024)\n",
      "Valid blocks shape: (360053, 1024)\n"
     ]
    }
   ],
   "source": [
    "if isfull:\n",
    "    train_blocks = np.load(\"../materials/train_blocks.npy\", mmap_mode=\"r\")\n",
    "    valid_blocks = np.load(\"../materials/valid_blocks.npy\", mmap_mode=\"r\")\n",
    "else:\n",
    "    train_blocks = np.load(\"../materials_small/train_blocks.npy\", mmap_mode=\"r\")\n",
    "    valid_blocks = np.load(\"../materials_small/valid_blocks.npy\", mmap_mode=\"r\")\n",
    "\n",
    "print(\"Train blocks shape:\", train_blocks.shape)\n",
    "print(\"Valid blocks shape:\", valid_blocks.shape)\n",
    "\n",
    "# the shape should be whatever you see in log after executing first cell in 5th one\n",
    "# and 1024 since BLOCK_SIZE = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b0b75",
   "metadata": {},
   "source": [
    "# 6. **Prepare for smoke test**\n",
    "\n",
    "   * keep a tiny ‚Äúsmoke‚Äêtest‚Äù subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da75881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Smoke-test train blocks: 40 saved as train_smoke.npy\n",
      "‚úì Smoke-test valid blocks: 20 saved as valid_smoke.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load your full block arrays\n",
    "if isfull:\n",
    "    train_blocks = np.load(\"../materials/train_blocks.npy\")\n",
    "    valid_blocks = np.load(\"../materials/valid_blocks.npy\")\n",
    "else:\n",
    "    train_blocks = np.load(\"../materials_small/train_blocks.npy\")\n",
    "    valid_blocks = np.load(\"../materials_small/valid_blocks.npy\")\n",
    "\n",
    "# save a tiny ‚Äúsmoke-test‚Äù subset\n",
    "#  first 40 train blocks, first 20 valid blocks\n",
    "if isfull:\n",
    "    np.save(\"../materials/train_smoke.npy\", train_blocks[:40])\n",
    "    np.save(\"../materials/valid_smoke.npy\", valid_blocks[:20])\n",
    "else:\n",
    "    np.save(\"../materials_small/train_smoke.npy\", train_blocks[:40])\n",
    "    np.save(\"../materials_small/valid_smoke.npy\", valid_blocks[:20])\n",
    "\n",
    "\n",
    "print(f\"‚úì Smoke-test train blocks: {train_blocks[:40].shape[0]} saved as train_smoke.npy\")\n",
    "print(f\"‚úì Smoke-test valid blocks: {valid_blocks[:20].shape[0]} saved as valid_smoke.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51003b8e",
   "metadata": {},
   "source": [
    "# DONE! <br/>\n",
    "If you see followings in materials folder:\n",
    "- all_books_clean.txt\n",
    "- all_books_ids.npy\n",
    "- all_books.txt\n",
    "- train_blocks.npy\n",
    "- train_ids.npy\n",
    "- train_smoke.npy\n",
    "- valid_blocks.npy\n",
    "- valid_ids.npy\n",
    "- valid_smoke.npy\n",
    "\n",
    "you're good to go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
