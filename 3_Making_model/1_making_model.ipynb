{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6414f925",
   "metadata": {},
   "source": [
    "# Making model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939b994",
   "metadata": {},
   "source": [
    "<img src=\"./diagram_1.PNG\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849dd8b",
   "metadata": {},
   "source": [
    "Yup... that's the entire diagram for model <br/>\n",
    "(without checkpoints. i'll be covering this in this ipynb as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70516397",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2e4ac",
   "metadata": {},
   "source": [
    "<img src=\"./GELU.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a5f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# note: i tried GELU, ReLU, Swish, etc \n",
    "# but GELU gave me the fastest runtime (idk the reason...)\n",
    "# so, i'm using GELU for this project\n",
    "def build_activation(name: str):\n",
    "    name = (name or \"gelu\").lower() \n",
    "    if name == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    raise ValueError(f\"Unsupported activation: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10f1c1",
   "metadata": {},
   "source": [
    "# MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qkv_bias, attn_drop, resid_drop, max_ctx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # in multi-head attention, we split the full embedding vector of size d_model into n_heads smaller chunks (one chunk per head)\n",
    "        # so, d_model should be divisible by n_heads\n",
    "        # if not, we'll end up with fractional head dimensions, which is impossible for tensor reshaping\n",
    "        assert d_model % n_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads # dimension per attention head\n",
    "\n",
    "        # linear layers to project input [B, T, d_model] into queries, keys, values\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=qkv_bias) # Q projection\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=qkv_bias) # K projection\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=qkv_bias) # V projection\n",
    "        # think of\n",
    "        # Q: asks \"what am i looking for?\"\n",
    "        # K: says \"what do i have to offer?\"\n",
    "        # V: the actual content we'll aggregate once we decide \"who to listen to\"\n",
    "\n",
    "        # output projection: combines all head outputs back into a single vector per token\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        # dropout stuffs\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.resid_drop = nn.Dropout(resid_drop)\n",
    "\n",
    "        # masking (see fig 2_1)\n",
    "        # so, precompute upper triangular causal mask [max_ctx, max_ctx]\n",
    "        # determine true entries & indicate positions that should be masked \n",
    "        # that masked ones will be future tokens\n",
    "        mask = torch.triu(torch.ones(max_ctx, max_ctx, dtype=torch.bool), diagonal=1)\n",
    "        self.register_buffer(\"causal_mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # project input to Q, K, V, then reshape into [B, n_heads, T, head_dim]\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # compute attention output\n",
    "        if hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            # if SDPA is found, use that \n",
    "            y = F.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, is_causal=True,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.0,\n",
    "            )\n",
    "        else:\n",
    "            # manual attention computation\n",
    "            # scale it first\n",
    "            att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "            # apply causal mask\n",
    "            cm = self.causal_mask[:T, :T].to(att.device)\n",
    "            att = att.masked_fill(cm, torch.finfo(att.dtype).min)\n",
    "\n",
    "            # softmax\n",
    "            att = F.softmax(att, dim=-1)\n",
    "\n",
    "            # dropout attention weights\n",
    "            att = self.attn_drop(att)\n",
    "\n",
    "            # weighted sum of values\n",
    "            y = att @ v # [B, h, T, head_dim]\n",
    "\n",
    "        # merge heads\n",
    "        # [B, h, T, head_dim] â†’ [B, T, C]\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # final output projection + dropout\n",
    "        y = self.resid_drop(self.out_proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af5bff",
   "metadata": {},
   "source": [
    "### Figure 2_1\n",
    "<img src=\"./causal_mask_example.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929b70a",
   "metadata": {},
   "source": [
    "# MLP (Feed-Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4810ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the guy used in transformer block\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, drop, activation=\"gelu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # first linear layer: expand embedding dimension to a larger feed-forward size\n",
    "        # this gives the network more capacity to learn richer transformations per token\n",
    "        self.fc = nn.Linear(d_model, d_ff)\n",
    "\n",
    "        # non-linear activation (we're using GELU)\n",
    "        self.act = build_activation(activation)\n",
    "\n",
    "        # second linear layer: project back down to original embedding size\n",
    "        self.proj = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # dropout: randomly zero out some elements during training to prevent overfitting\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flow:\n",
    "        # 1. project up: [B, T, d_model] -> [B, T, d_ff]\n",
    "        # 2. apply non-linear activation\n",
    "        # 3. project down: [B, T, d_ff] -> [B, T, d_model]\n",
    "        # 4. apply dropout to output\n",
    "        return self.drop(self.proj(self.act(self.fc(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbd09f",
   "metadata": {},
   "source": [
    "# TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "348206e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll have multiples of this later\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # dimensions\n",
    "        d_model = cfg[\"emb_dim\"] # embedding dimension\n",
    "        n_heads = cfg[\"n_heads\"] # number of attention heads\n",
    "\n",
    "        # feed-forward dimension\n",
    "        d_ff = cfg.get(\"intermediate_size\", 4 * d_model) # i've setted up as 4 * the embedding size\n",
    "\n",
    "        # dropout \n",
    "        drop = cfg.get(\"drop_rate\", 0.1) # residual & MLP dropout\n",
    "        attn_drop = cfg.get(\"attention_probs_dropout_prob\", drop) # attention weight dropout\n",
    "\n",
    "        # attention config\n",
    "        qkv_bias = cfg.get(\"qkv_bias\", False) # whether Q/K/V projections have bias (we've set this to True in model_config_124M.json)\n",
    "        max_ctx = cfg[\"context_length\"] # maximum sequence length\n",
    "\n",
    "        # activation for MLP\n",
    "        activation = cfg.get(\"activation\", \"gelu\") # we're using GELU overall\n",
    "\n",
    "        # layernorm epsilon (small constant for numerical stability)\n",
    "        eps = cfg.get(\"layer_norm_eps\", 1e-5)\n",
    "\n",
    "        # submodules\n",
    "        self.ln1 = nn.LayerNorm(d_model, eps=eps) # pre-attention layernorm\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, qkv_bias, attn_drop, drop, max_ctx) # multi-head self-attention block\n",
    "        self.ln2 = nn.LayerNorm(d_model, eps=eps) # pre-MLP layer norm\n",
    "        self.mlp = MLP(d_model, d_ff, drop, activation) # feed-forward network\n",
    "\n",
    "        # scaling factor (to stabilize training for deep networks)\n",
    "        # 1 / sqrt(2 * num_layers)\n",
    "        self.res_scale = 1.0 / math.sqrt(2 * cfg[\"n_layers\"])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # multi-head attention sublayer\n",
    "        # pre-norm: normalize inputs before attention\n",
    "        # residual: add attention output back to original x\n",
    "        x = x + self.attn(self.ln1(x)) * self.res_scale\n",
    "\n",
    "        # MLP sublayer\n",
    "        # pre-norm: normalize inputs before MLP\n",
    "        # residual: add MLP output back to updated x\n",
    "        x = x + self.mlp(self.ln2(x)) * self.res_scale\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96407f7f",
   "metadata": {},
   "source": [
    "# Main Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42b2ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initialization (this is used in DummyModel class)\n",
    "def _init_weights(module, std):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c10ce6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main model\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # grab configurations\n",
    "        # in this case, from \"model_config_124M.json\"\n",
    "        self.vocab_size = cfg[\"vocab_size\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        d_model = cfg[\"emb_dim\"]\n",
    "        n_layers = cfg[\"n_layers\"]\n",
    "        drop = cfg.get(\"drop_rate\", 0.1)\n",
    "        eps = cfg.get(\"layer_norm_eps\", 1e-5)\n",
    "        init_std = cfg.get(\"initializer_range\", 0.02)\n",
    "\n",
    "        # embeddings\n",
    "        self.tok_emb = nn.Embedding(self.vocab_size, d_model) # token embeddings\n",
    "        self.pos_emb = nn.Embedding(self.context_length, d_model) # positional embeddings\n",
    "        self.drop = nn.Dropout(drop) # dropout after embeddings\n",
    "\n",
    "        # transformer layers\n",
    "        # remember i told you we'll have multiple of transformer blocks?\n",
    "        # this makes the multiple of it\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg) for _ in range(n_layers) # stack of blocks\n",
    "        ])\n",
    "\n",
    "        # final norm & output projection\n",
    "        self.ln_f = nn.LayerNorm(d_model, eps=eps) # normalize before output\n",
    "        self.lm_head = nn.Linear(d_model, self.vocab_size, bias=False) # projection to vocab\n",
    "\n",
    "        # weight tying\n",
    "        # share token embedding weights with output projection weights\n",
    "        # this reduces parameters\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        # for checkpoint\n",
    "        self.grad_ckpt = bool(cfg.get(\"grad_ckpt\", False))\n",
    "\n",
    "        # apply that to all submodules\n",
    "        self.apply(lambda m: _init_weights(m, init_std))\n",
    "\n",
    "    def forward(self, idx, labels=None):\n",
    "        assert idx.dtype == torch.long\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # clip sequence length if it exceeds context_length\n",
    "        # we would have some tokens missing while training\n",
    "        # but hey. in real training, we have literally billions of tokens.\n",
    "        # so, that wouldn't be a big issue\n",
    "        if T > self.context_length:\n",
    "            idx = idx[:, -self.context_length:]\n",
    "            T = idx.shape[1]\n",
    "\n",
    "        # embedding lookup\n",
    "        T = min(T, self.context_length) # for safety\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        x = self.tok_emb(idx[:, :T]) + self.pos_emb(pos)[None, :, :]\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # checkpoint\n",
    "        use_ckpt = self.grad_ckpt and self.training\n",
    "\n",
    "        # if we have checkpoint, start from there\n",
    "        if use_ckpt:\n",
    "            try:\n",
    "                amp_dtype = torch.get_autocast_gpu_dtype()\n",
    "            except AttributeError:\n",
    "                amp_dtype = torch.bfloat16\n",
    "\n",
    "            def run_block(block, t):\n",
    "                with autocast(device_type=t.device.type, dtype=amp_dtype):\n",
    "                    return block(t)\n",
    "\n",
    "            def _block_forward(bl, u):\n",
    "                return run_block(bl, u)\n",
    "            \n",
    "            for b in self.blocks:\n",
    "                x = checkpoint(_block_forward, b, x, use_reentrant=False)\n",
    "        else:\n",
    "            # transformer stack\n",
    "            for b in self.blocks:\n",
    "                x = b(x)\n",
    "\n",
    "        # final norm + projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # loss (this is for log & displaying graph. it's optional)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels[:, :T]\n",
    "            assert labels.dtype == torch.long\n",
    "            logits_flat = logits[:, :-1, :].contiguous().view(-1, self.vocab_size)\n",
    "            labels_flat = labels[:, 1:].contiguous().view(-1)\n",
    "            loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=-100)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def num_params(self, trainable_only=True):\n",
    "        # count total parameters in the module\n",
    "        ps = (p for p in self.parameters() if (p.requires_grad or not trainable_only))\n",
    "        return sum(p.numel() for p in ps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8f46c",
   "metadata": {},
   "source": [
    "# Tiny Smoke Test\n",
    "(just to verify we're not getting any error after executing above codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a84289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 166144\n",
      "logits: torch.Size([4, 32, 1000]) loss: 6.944150447845459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = dict(\n",
    "    vocab_size=1000, context_length=32, emb_dim=64,\n",
    "    n_heads=8, n_layers=2, drop_rate=0.1, qkv_bias=True, activation=\"gelu\"\n",
    ")\n",
    "m = DummyModel(cfg)\n",
    "print(\"Params:\", m.num_params())\n",
    "B, T = 4, 32\n",
    "idx = torch.randint(0, cfg[\"vocab_size\"], (B, T))\n",
    "labels = torch.randint(0, cfg[\"vocab_size\"], (B, T))\n",
    "with torch.no_grad():\n",
    "    logits, loss = m(idx, labels)\n",
    "print(\"logits:\", logits.shape, \"loss:\", float(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd904071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA input: torch.Size([2, 16, 64]) â†’ output: torch.Size([2, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B, T, d = 2, 16, cfg[\"emb_dim\"]\n",
    "x = torch.randn(B, T, d)\n",
    "attn = MultiHeadAttention(d, cfg[\"n_heads\"], True, 0.0, 0.0, cfg[\"context_length\"])\n",
    "with torch.no_grad():\n",
    "    y = attn(x)\n",
    "print(\"MHA input:\", x.shape, \"â†’ output:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24679f1",
   "metadata": {},
   "source": [
    "if you see input torch.size = output torch.size <br/>\n",
    "you're good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369acbf",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88919f37",
   "metadata": {},
   "source": [
    "in python_files folder, there's Dummy_model.py <br/>\n",
    "they're basically the same (except I tried to use ReLU for test, but i'm sticking with GELU) <br/>\n",
    "I'm naming this as Dummy_model.py <br/>\n",
    "(but you can have different name. Just make sure in the future, you select the correct model file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0cd856",
   "metadata": {},
   "source": [
    "# UPDATES\n",
    "\n",
    "- **Fused QKV**: replaced three Linear projections (Q,K,V) with a single `nn.Linear(d_model, 3*d_model)`.\n",
    "- **SDPA path**: use `torch.nn.functional.scaled_dot_product_attention` when available; fallback to masked softmax.\n",
    "- **Causal masking**: register a boolean upper-tri mask as a buffer; SDPA path uses `is_causal=True`.\n",
    "- **Residual scaling**: scale both residual branches by `1/sqrt(2*n_layers)` for depth stability.\n",
    "- **Pre-Norm**: LayerNorm before attention and MLP (`ln1`, `ln2`) with configurable `eps`.\n",
    "- **Gradient checkpointing**: optional per-block checkpointing (`use_reentrant=False`, `preserve_rng_state=False`) and rely on outer AMP autocast.\n",
    "- **AMP support**: compatible with `torch.amp.autocast` in the training loop.\n",
    "- **Weight tying**: `lm_head.weight = tok_emb.weight` to tie input/output embeddings.\n",
    "- **Learned positional embeddings**: keep learned `pos_emb` (no RoPE/ALiBi).\n",
    "- **Label shift & ignore_index**: next-token CE loss with `ignore_index=-100`.\n",
    "- **Config knobs**: `qkv_bias`, `attention_probs_dropout_prob` separate from `drop_rate`, `layer_norm_eps`, `initializer_range`.\n",
    "- **Context-length safety**: trim long sequences and clamp `T` to `context_length`.\n",
    "- **Shape guards**: assert `d_model % n_heads == 0`.\n",
    "- **Param count helper**: `num_params(trainable_only=True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b0d52",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".LLM_Dummy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
