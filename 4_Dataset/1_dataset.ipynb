{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3603c68",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "in here, we're going to create NpyTokensDataset <br/>\n",
    "our goal is to efficiently stream token sequences from large ```.npy``` files without loading everything into RAM <br/>\n",
    "we'll be using NumPy memmap and PyTorch Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93d2ab",
   "metadata": {},
   "source": [
    "# 1. Imports & skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae729d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NpyTokensDataset_note(Dataset):\n",
    "    def __init__(self, path: str, seq_len: int):\n",
    "        ...\n",
    "    def __getstate__(self):\n",
    "        ...\n",
    "    def __setstate__(self, state):\n",
    "        ...\n",
    "    def __len__(self):\n",
    "        ...\n",
    "    def __getitem__(self, idx):\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a81e2",
   "metadata": {},
   "source": [
    "# 2. ```__init__```\n",
    "what it does?\n",
    "- saves path / seq_len\n",
    "- opens ```.npy``` file with ```mmap_mode``` so it's not all loaded into memory\n",
    "- accepts 1D or 2D arrays (we're mainly using 2D. I've included 1D since i was testing out previous one as well)\n",
    "- asserts that the array length is longer than ```seq_len + 1``` for proper ```(x, y)``` shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95c4957",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNpyTokensDataset_note\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, seq_len: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28mself\u001b[39m.path = \u001b[38;5;28mstr\u001b[39m(path)\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class NpyTokensDataset_note(Dataset):\n",
    "    def __init__(self, path: str, seq_len: int):\n",
    "        self.path = str(path)\n",
    "        self.seq_len = int(seq_len)\n",
    "        # open as memmap (not full load)\n",
    "        arr = np.load(self.path, mmap_mode=\"r\")\n",
    "\n",
    "        # record shape info\n",
    "        self._ndim = arr.ndim\n",
    "        self._shape = arr.shape\n",
    "\n",
    "        # enforce 1D or 2D\n",
    "        assert self._ndim in (1, 2), f\"Expected 1D/2D, got {self._ndim}\"\n",
    "\n",
    "        # ensure we can create (x, y) with a 1-token shift\n",
    "        if self._ndim == 1:\n",
    "            assert self._shape[0] > self.seq_len + 1, \\\n",
    "                f\"1D length {self._shape[0]} must be > seq_len+1={self.seq_len+1}\"\n",
    "        else:\n",
    "            assert self._shape[1] > self.seq_len + 1, \\\n",
    "                f\"2D length {self._shape[1]} must be > seq_len+1={self.seq_len+1}\"\n",
    "\n",
    "        # keep the memmap handle (in main process)\n",
    "        self._arr = arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121c290",
   "metadata": {},
   "source": [
    "#### (TL;DR) WHY MEMMAP?\n",
    "in the actual pretraining, i'm feeding huge data (around 35 GB for text file) <br/>\n",
    "and when we perform calculation, it will require huge memory for each block (idk... around 20 GB for each chunk?)\n",
    "it's not going to fit into our RAM since we also need to remember values & other stuffs <br/>\n",
    "(mine has 40GB RAM but it's still small to fit all 8 chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e93286",
   "metadata": {},
   "source": [
    "# 3. ```__getstate__``` & ```__setstate__```\n",
    "the problem is that: when ```DataLoader(num_workers)``` forks worker process, we don't want to pickle a huge memmap array <br/>\n",
    "so, save only lightweight fields in ```__getstate__```, then reopen the memmap in each worker's ```__setstate__```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f890b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpyTokensDataset_note(Dataset):\n",
    "    ...\n",
    "    def __getstate__(self):\n",
    "        # return only small, picklable state.\n",
    "        return {\n",
    "            \"path\": self.path,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"_ndim\": self._ndim,\n",
    "            \"_shape\": self._shape,\n",
    "        }\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # restore small state\n",
    "        self.__dict__.update(state)\n",
    "        # reopen memmap in the worker process\n",
    "        self._arr = np.load(self.path, mmap_mode=\"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ca65b",
   "metadata": {},
   "source": [
    "# 4. ```__len__```\n",
    "what it does?\n",
    "- for 1D: returns at least 4096; otherwise, an estimate of how many non-overlapping windows fit\n",
    "- for 2D: multiplies row by a per-row window count (at least 4), then capped to at least 4096\n",
    "\n",
    "this is a virtual length to keep random sampling going<br/>\n",
    "we don't actuallly index by ```idx``` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f233f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpyTokensDataset_note(Dataset):\n",
    "    ...\n",
    "    def __len__(self):\n",
    "        if self._ndim == 1:\n",
    "            # rough capacity estimate or a minimum baseline\n",
    "            return max(4096, (self._shape[0] - 1) // self.seq_len)\n",
    "        rows, L = self._shape\n",
    "        # for 2D, ensure each row contributes at least a few samples\n",
    "        return max(4096, rows * max(4, (L - 1) // self.seq_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2215d16",
   "metadata": {},
   "source": [
    "#### (TL;DR) Why 4096?\n",
    "```DataLoader``` progress bars and samplers behave nicely even if your array is short <br/>\n",
    "it also gives enough iterations per epoch to see metrics <br/>\n",
    "(i also tried 8192, but it stretched epoch times too much; progress bars moved slowly and eval checkpoints lagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93845e44",
   "metadata": {},
   "source": [
    "# 5. ```__getitem__```\n",
    "what it does?\n",
    "- ignores idx for true randomization\n",
    "- picks a random start ```s``` so that ```[s : s+se_len]``` is ```x``` and ```[s+1 : s+seq_len+1]``` is ```y```\n",
    "- for 2D, it also picks random row ```r```\n",
    "- zero-copy slices with ```.astype(np.int64, copy=False)``` and converts to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d927eec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNpyTokensDataset_note\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      2\u001b[39m     ...\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class NpyTokensDataset_note(Dataset):\n",
    "    ...\n",
    "    def __getitem__(self, idx):\n",
    "        arr = self._arr\n",
    "        if self._ndim == 1:\n",
    "            L = self._shape[0]\n",
    "            s = np.random.randint(0, L - self.seq_len - 1)\n",
    "            x = arr[s:s+self.seq_len].astype(np.int64, copy=False)\n",
    "            y = arr[s+1:s+self.seq_len+1].astype(np.int64, copy=False)\n",
    "        else:\n",
    "            rows, L = self._shape\n",
    "            r = np.random.randint(0, rows) # pick a random row/document\n",
    "            s = np.random.randint(0, L - self.seq_len - 1) # pick a random start within that row\n",
    "            row = arr[r]\n",
    "            x = row[s:s+self.seq_len].astype(np.int64, copy=False)\n",
    "            y = row[s+1:s+self.seq_len+1].astype(np.int64, copy=False)\n",
    "\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff38f1",
   "metadata": {},
   "source": [
    "# Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b5fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append(\"../python_files\")\n",
    "from npy_datasets import NpyTokensDataset\n",
    "\n",
    "SEQ_LEN = 512\n",
    "PATH = \"../materials/train_smoke.npy\" \n",
    "# for full dataset, use these lines instead\n",
    "# PATH = \"../materials/train_smoke.npy\" \n",
    "\n",
    "train_ds = NpyTokensDataset(PATH, seq_len=SEQ_LEN)\n",
    "\n",
    "# small test\n",
    "loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=12,\n",
    "    shuffle=False,            \n",
    "    num_workers=2, # small number of workers\n",
    "    pin_memory=True, # if using CUDA\n",
    "    persistent_workers=True, # keeps workers alive for speed\n",
    "    prefetch_factor=4, # multiples of batches per worker\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75030d",
   "metadata": {},
   "source": [
    "# Sanity check\n",
    "what to see:\n",
    "- size and type of input and targets should match\n",
    "- Sample x shows some value\n",
    "- Sample y is sample x but shifted one position left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2067b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (x): torch.Size([12, 512]) torch.int64 cpu\n",
      "Target (y): torch.Size([12, 512]) torch.int64 cpu\n",
      "\n",
      "\n",
      "Sample x[0]: [1052, 724, 7314, 322, 35, 208, 3801, 1500, 1933, 20934]\n",
      "Sample y[0]: [724, 7314, 322, 35, 208, 3801, 1500, 1933, 20934, 653]\n"
     ]
    }
   ],
   "source": [
    "# grab one batch\n",
    "xb, yb = next(iter(loader))\n",
    "\n",
    "print(\"Input (x):\", xb.shape, xb.dtype, xb.device)\n",
    "print(\"Target (y):\", yb.shape, yb.dtype, yb.device)\n",
    "\n",
    "# show first sequence of tokens (truncated for readability)\n",
    "print(\"\\n\")\n",
    "print(\"Sample x[0]:\", xb[0, :10].tolist()) # first 20 tokens\n",
    "print(\"Sample y[0]:\", yb[0, :10].tolist()) # shifted by 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbafdbe",
   "metadata": {},
   "source": [
    "# DONE!\n",
    "you'll see ```Datasets.py``` in python_files folder <br/>\n",
    "that's exactly what this notebook has (maybe some names are different)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
