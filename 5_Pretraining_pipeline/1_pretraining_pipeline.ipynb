{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0765bc3a",
   "metadata": {},
   "source": [
    "# Making train pipeline (No-CLI) Notebook\n",
    "**NOTE**: in actual ```train.py```, i'm gonna be implementing CLI + some modifications to increase the calculation speed <br/>\n",
    "but the basic ideas are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7835a7",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Imports & global setups\n",
    "2. Config & runtime knobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef49a8",
   "metadata": {},
   "source": [
    "# OVERALL DIAGRAM\n",
    "<img src=\"./train_diagram.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9cdf8",
   "metadata": {},
   "source": [
    "## **If you're testing full, turn this to true**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541abe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfull = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a61842",
   "metadata": {},
   "source": [
    "# 1. Imports & global setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba213924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os, math, time, json, random, csv, warnings\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# global setups\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\") # just to remove tqdm warnings\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# this defaults to true. but i'm doing it to MAKE SURE it's true\n",
    "# it allows matrix multiplication to run in TF32 mod (in my case, RTX30-series)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "# Values:\n",
    "#   'highest' → strict IEEE FP32 (full precision, slower).\n",
    "#   'high' → allows TF32 (fast, slightly less precise).\n",
    "#   'medium' → allows lower precision modes when possible (even faster).\n",
    "\n",
    "# i've done this just for debugging\n",
    "SEED = 1234\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# setting up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device) # make sure you see \"device: cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100eafb4",
   "metadata": {},
   "source": [
    "# 2. Config, params, runtime knobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807ef7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan → TOTAL_UPDATES=200 (pilot=10, 2×chunks). eff tokens/update ≈ 2,048. warmup=20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model config (later, we'll be grabing model.json instead)\n",
    "cfg = {\n",
    "    \"vocab_size\": 60000,\n",
    "    \"context_length\": 128,      # real one uses 10024\n",
    "    \"emb_dim\": 256,             # real one uses 768\n",
    "    \"n_heads\": 4,               # real one uses 12\n",
    "    \"n_layers\": 4,              # real one uses 12\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"layer_norm_eps\": 1e-5,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 1024,  # real one uses 3072\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"grad_ckpt\": False,\n",
    "}\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# paths\n",
    "RUN_DIR = Path(\"../pretrain_checkpoint_notebook/\")\n",
    "\n",
    "# DIR THINGS; I'VE HANDLED IT HERE EXPLITELY FOR NOTEBOOK (it's not gonna be in our train.py)\n",
    "# remove RUN_DIR if it exists (clear contents)\n",
    "if RUN_DIR.exists():\n",
    "    shutil.rmtree(RUN_DIR)\n",
    "\n",
    "# recreate empty RUN_DIR\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# END OF DIR THINGS\n",
    "\n",
    "# scheduling knobs (small for quick test; this will be scaled up later before real pretrain)\n",
    "TOTAL_UPDATES   = 200\n",
    "PILOT_FRAC      = 0.05\n",
    "CHUNKS          = 2\n",
    "EVAL_EVERY      = 50\n",
    "PERIODIC_EVERY  = 100\n",
    "LOG_EVERY       = 10\n",
    "ACCUM           = 2\n",
    "MICRO_BSZ       = 8\n",
    "BASE_LR         = 5e-4\n",
    "WEIGHT_DECAY    = 0.10\n",
    "\n",
    "# AMP mode: \"fp16\" | \"bf16\" | \"none\"\n",
    "# during the real run, i tested both bf16 and fp16\n",
    "# but the estimate completion time for bf16 was approximately 8 days while fp16 was 5 days\n",
    "# so, i decided to use fp16\n",
    "AMP_MODE = \"fp16\" # feel free to test out bf16 and none (it's gonna default to fp16)\n",
    "\n",
    "# derived knobs\n",
    "CTX = int(cfg[\"context_length\"])\n",
    "EFFECTIVE_TOKENS_PER_UPDATE = CTX * MICRO_BSZ * ACCUM\n",
    "WARMUP_STEPS = max(1, int(0.10 * TOTAL_UPDATES))\n",
    "\n",
    "print(f\"Plan → TOTAL_UPDATES={TOTAL_UPDATES} (pilot={int(PILOT_FRAC*TOTAL_UPDATES)}, \"\n",
    "      f\"{CHUNKS}×chunks). eff tokens/update ≈ {EFFECTIVE_TOKENS_PER_UPDATE:,}. warmup={WARMUP_STEPS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa12c40",
   "metadata": {},
   "source": [
    "later on, some of those constants will be controlled by inline params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd86758",
   "metadata": {},
   "source": [
    "# 3. Helpers (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922874b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before saving/loading state_dict, we want to unwrap a model to its underlying nn.Module\n",
    "def _unwrap_model_for_state_dict(model: nn.Module) -> nn.Module:\n",
    "    if hasattr(model, \"_orig_mod\"): # unwrap torch.compile wrapper\n",
    "        return model._orig_mod\n",
    "    if hasattr(model, \"module\"): # unwrap DDP / DataParallel\n",
    "        return model.module\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa29d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also want to remove prefix from all keys in a state_dict\n",
    "# removes a given prefix (e.g., \"module.\" or \"_orig_mod.\") from \n",
    "# all parameter keys inside a model's state_dict (i.e. ckpt[\"model\"]). \n",
    "# This is useful when loading checkpoints saved with wrappers like \n",
    "# DataParallel (which adds \"module.\") or torch.compile (which adds \"_orig_mod.\").\n",
    "def _strip_prefix_in_state_dict(sd: dict, prefix: str) -> dict:\n",
    "    if not any(k.startswith(prefix) for k in sd.keys()):\n",
    "        return sd\n",
    "    return {k[len(prefix):] if k.startswith(prefix) else k: v for k, v in sd.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa282aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensures each worker gets a unique, deterministic random seed\n",
    "# so, yeah. we call this multiple times to instantiate workers\n",
    "def worker_init_fn(worker_id: int):\n",
    "    seed = (torch.initial_seed() + worker_id) % (2**31 - 1)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b770d33",
   "metadata": {},
   "source": [
    "#### Weight decay\n",
    "<img src=\"./weight_decay.png\" width=\"500\"/>\n",
    "\n",
    "that's the example of graph for weight decay. <br/>\n",
    "concept: as the weight value goes down; learning rate would increase.\n",
    "\n",
    "- without weight decay (blue)\n",
    "    - moving strictly down -> params grow large -> causes overfitting and unstable learning\n",
    "- with weight decay (orange)\n",
    "    - moving down -> but in some point, it will move down slower (never hitting horizontal asymptote)\n",
    "        -> param grows large to small -> prevents overfitting and unstable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d90f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is called before optimizer does the thing\n",
    "def param_groups_weight_decay(module: nn.Module, weight_decay: float):\n",
    "    decay = [] # params that should have weight decay applied\n",
    "    no_decay = [] # params that should not have weight decay applied\n",
    "\n",
    "    for n, p in module.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        is_bias = n.endswith(\"bias\")\n",
    "        is_norm = (\"norm\" in n.lower()) or (\"ln\" in n.lower())\n",
    "\n",
    "        # biases, norms, and 1D tensors -> no decay\n",
    "        (no_decay if (is_bias or is_norm or p.ndim <= 1) else decay).append(p)\n",
    "    return [{\"params\": decay, \"weight_decay\": weight_decay},\n",
    "            {\"params\": no_decay, \"weight_decay\": 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc725d",
   "metadata": {},
   "source": [
    "#### Cosine warmup\n",
    "let's focus on the warmup phase (vertical dotted lines) <br/>\n",
    "##### Without Cosine warmup\n",
    "<img src=\"./without_cosine_warmup.png\" width=\"500\"><br/>\n",
    "- after warmup, the learning rate stays flat at the maximum value.\n",
    "- we're gonna face overfitting problem\n",
    "- this may cause unstable learning\n",
    "\n",
    "<img src=\"./cosine_warmup.png\" width=\"500\"/><br/>\n",
    "- that nice down-curve makes learning rate to drop smoothly\n",
    "- updates get smaller over time, letting the model stable instead of bouncing\n",
    "- this is unlikely to cause overfitting and unstable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a4edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cosine_with_warmup(optimizer, warmup_steps, total_steps, min_lr_ratio=0.10):\n",
    "    # this part is gonna return that nice curve, which is after warmup\n",
    "    def lr_lambda(step):\n",
    "        # linear part (from 0 to warmup)\n",
    "        if step < warmup_steps:\n",
    "            return (step + 1) / max(1, warmup_steps)\n",
    "        \n",
    "        # normalize step to [0, 1] after warmup\n",
    "        t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        t = min(max(t, 0.0), 1.0)\n",
    "\n",
    "        # apply cosine curve using that \"t\"\n",
    "        return min_lr_ratio + 0.5 * (1 - min_lr_ratio) * (1 + math.cos(math.pi * t))\n",
    "    \n",
    "    # then we return that\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546650c",
   "metadata": {},
   "source": [
    "#### Exponential Moving Average\n",
    "<img src=\"./ema.png\" width=\"500\"/><br/>\n",
    "so, what's doing here is:\n",
    "1. take that fluctuating weight values that's moving downwards\n",
    "2. smooth the noises \n",
    "3. we'll end up with some nice curve that's heading downwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b55e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMeter:\n",
    "    def __init__(self, beta=0.9): \n",
    "        self.beta = beta # smoothing factor (closer to 1 = smoother; but slower updates)\n",
    "        self.val = 0.0 # current EMA value\n",
    "        self.inited = False # flag to check if first value has been set\n",
    "        \n",
    "    def update(self, x):\n",
    "        # get weight, loss, etc. as x\n",
    "        x = float(x)\n",
    "\n",
    "        # if first update, just take x as initial value\n",
    "        if not self.inited: \n",
    "            self.val, \n",
    "            self.inited = x, \n",
    "            True\n",
    "        # if not, new_value = smoothing_factor * old_value + (1 - smoothing_factor)\n",
    "        else: \n",
    "            self.val = self.beta*self.val + (1-self.beta)*x\n",
    "\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc207d5",
   "metadata": {},
   "source": [
    "#### Top-K\n",
    "idea: \n",
    "- track top-K best result (like in lowest loss, etc) using a heap\n",
    "- later on, this will be used when we reload checkpoint, fine-tuning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc269f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopK:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.heap = []  # max-heap via negative loss\n",
    "\n",
    "    # adding a new result into the heap\n",
    "    # args:\n",
    "    #   loss (float): validation loss\n",
    "    #   update (int): training step number\n",
    "    #   path (str): checkpoint file path\n",
    "    def add(self, loss, update, path):\n",
    "        import heapq\n",
    "        item = (-float(loss), int(update), str(path))\n",
    "\n",
    "        # if heap is not full, push new item\n",
    "        if len(self.heap) < self.k:\n",
    "            heapq.heappush(self.heap, item)\n",
    "        \n",
    "        # if heap is full, replace the worst\n",
    "        else:\n",
    "            if item > self.heap[0]: # better than current worst?\n",
    "                heapq.heapreplace(self.heap, item) # yeah, replace the worst item with the current item\n",
    "    \n",
    "    # return top-K items sorted by ascending loss\n",
    "    def best(self):\n",
    "        return sorted([(-l, u, p) for (l, u, p) in self.heap], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80619a85",
   "metadata": {},
   "source": [
    "#### ByteTensor, Tensor, Byte, Sequences to tensor\n",
    "some of the Q&A\n",
    "- why cpu instead of GPU?\n",
    "    - that byte tensor isn't for computation. it's just some number that stores metadata\n",
    "- why uint8?\n",
    "    - since RNG states, checkpoint manifests, etc. are often stored as uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3344ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_byte_tensor(x):\n",
    "\n",
    "    # just in case x is nothing (does NOT mean undefined)\n",
    "    if x is None: \n",
    "        return None\n",
    "    \n",
    "    # if type of x is ByteTensor (8-bit unsigned)\n",
    "    if isinstance(x, torch.ByteTensor): \n",
    "        return x.cpu() # just put that into cpu\n",
    "    \n",
    "    # if type of x is any other kind of Tensor\n",
    "    if isinstance(x, torch.Tensor):     \n",
    "        # detach from the graph -> cast to uint8 -> move to cpu\n",
    "        return x.detach().to(dtype=torch.uint8, device=\"cpu\")\n",
    "    \n",
    "    # if type of x is raw bytes or bytearray\n",
    "    if isinstance(x, (bytes, bytearray)): \n",
    "        # convert to list of integers -> cast to uint8\n",
    "        return torch.tensor(list(x), dtype=torch.uint8)\n",
    "    \n",
    "    # if type of x is list, tuple, ndarray\n",
    "    if isinstance(x, (list, tuple, np.ndarray)): \n",
    "        # just cast to uint8\n",
    "        return torch.tensor(x, dtype=torch.uint8)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b15fff",
   "metadata": {},
   "source": [
    "# 4. Logs, visualization, directories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11086559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def make_run_dirs(base):\n",
    "    # we're gonna later \"../pretrain_results_test\") for smoke testing\n",
    "    base = Path(base)\n",
    "\n",
    "    # inside base path,\n",
    "    ckpt = base / \"checkpoints\" # create checkpoints folder\n",
    "    plots = base / \"plots\" # create plots folder\n",
    "    logs = base / \"logs\" # create logs folder\n",
    "\n",
    "    # just in case if we're rerunning the whole thing \n",
    "    # (with different config)\n",
    "    # clear only the three subfolders\n",
    "    for d in (ckpt, plots, logs):\n",
    "        if d.exists() and d.is_dir():\n",
    "            shutil.rmtree(d)\n",
    "\n",
    "    # recreate fresh\n",
    "    for d in (ckpt, plots, logs):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return base, ckpt, plots, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59def976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# about manifest\n",
    "# it's going to create manifest in base\n",
    "# also, it will contain latest update\n",
    "def manifest_load_or_init(pretrain_dir: Path):\n",
    "    # define path\n",
    "    path = pretrain_dir / \"manifest.json\"\n",
    "\n",
    "    # if there's no manifest, create one\n",
    "    if not path.exists():\n",
    "        manifest = {\n",
    "            \"created_utc\": dt.datetime.now(dt.UTC).isoformat() + \"Z\",\n",
    "            \"chunks_completed\": 0,\n",
    "            \"last_update\": 0,\n",
    "            \"best_val\": None,\n",
    "            \"cfg\": cfg,\n",
    "        }\n",
    "        path.write_text(json.dumps(manifest, indent=2))\n",
    "    \n",
    "    # if we have one, load that since we're gonna be updating this (chunks_completed, last_update, best_val)\n",
    "    else:\n",
    "        manifest = json.loads(path.read_text())\n",
    "    return manifest, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e2c6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's going to generate csv for each train (NOT CHUNKS)\n",
    "def history_init(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        with open(csv_path, \"w\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(\n",
    "                [\"update\",\"split\",\"loss\",\"ppl\",\"lr\",\"tps\",\"gnorm\",\"scale\",\"tokens_seen\",\"utc\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb4718",
   "metadata": {},
   "source": [
    "#### Below 2 will be used for plotting training loss & validation loss in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3359a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's going to update the history.csv for each train\n",
    "def log_train_row(csv_path, update, loss, lr, tps, gnorm, scale, tokens_seen):\n",
    "    with open(csv_path, \"a\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\n",
    "            int(update),\"train\",float(loss),\"\",float(lr),float(tps),\n",
    "            float(gnorm),float(scale),int(tokens_seen),dt.datetime.now(dt.UTC).isoformat()+\"Z\"\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e8379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's going to update the validation results to the history.csv\n",
    "def log_val_row(csv_path, update, loss, ppl, tokens_seen):\n",
    "    with open(csv_path, \"a\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\n",
    "            int(update),\"val\",float(loss),float(ppl),\"\",\"\",\"\",\"\",\n",
    "            int(tokens_seen),dt.datetime.now(dt.UTC).isoformat()+\"Z\"\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debad15d",
   "metadata": {},
   "source": [
    "#### Generating graph for training loss & validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "331637e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is optional\n",
    "# i'm using this to visualize results later on\n",
    "def save_plot(history_csv: Path, plots_dir: Path, eff_tokens_per_update: int, save_tag=\"latest\"):\n",
    "    try:\n",
    "        import pandas as pd, matplotlib.pyplot as plt\n",
    "        if not history_csv.exists(): return\n",
    "        df = pd.read_csv(history_csv)\n",
    "        if df.empty: return\n",
    "\n",
    "        df[\"tokens_seen\"] = df[\"update\"] * eff_tokens_per_update\n",
    "        tr = df[df[\"split\"]==\"train\"].copy()\n",
    "        vl = df[df[\"split\"]==\"val\"].copy()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7,3.2), dpi=120)\n",
    "        ax.plot(tr[\"update\"], tr[\"loss\"], label=\"Training loss\")\n",
    "        if not vl.empty:\n",
    "            ax.plot(vl[\"update\"], vl[\"loss\"], linestyle=\"--\", label=\"Validation loss\")\n",
    "        ax.set_xlabel(\"Updates\"); ax.set_ylabel(\"Loss\"); ax.legend(loc=\"best\")\n",
    "\n",
    "        ax2 = ax.twiny()\n",
    "        ax2.set_xlim(ax.get_xlim())\n",
    "        xticks = ax.get_xticks()\n",
    "        ax2.set_xticks(xticks)\n",
    "        ax2.set_xticklabels([f\"{int(x*eff_tokens_per_update/1000):,}k\" for x in xticks])\n",
    "        ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "        out = plots_dir / f\"loss_curve_{save_tag}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(out)\n",
    "        plt.close(fig)\n",
    "        print(f\"🖼️ saved plot → {out}\")\n",
    "\n",
    "    # if it fails...? well, we don't really need graph so, skip that\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] plot failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5bcc3c",
   "metadata": {},
   "source": [
    "# 5. Checkpoint\n",
    "here's the thing<br/>\n",
    "the actual run will take around 6 days WITHOUT PAUSING<br/>\n",
    "i don't really want to burst my computer<br/>\n",
    "i decided to make checkpoint<br/>\n",
    "so that in the future, when we stop and resume, it will start from the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc714c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(ckpt_dir: Path, manifest: dict, manifest_path: Path,\n",
    "                    model, optimizer, scheduler, scaler, update, val_loss=None, tag=\"latest\"):\n",
    "    \n",
    "    base_model = _unwrap_model_for_state_dict(model)\n",
    "\n",
    "    if tag == \"latest\":\n",
    "        path = ckpt_dir / \"latest.pt\"\n",
    "    elif tag == \"best\":\n",
    "        path = ckpt_dir / \"best.pt\"\n",
    "    else:\n",
    "        path = ckpt_dir / f\"{tag}.pt\"\n",
    "\n",
    "    payload = {\n",
    "        \"update\": int(update),\n",
    "        \"val_loss\": (None if val_loss is None else float(val_loss)),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"scaler\": scaler.state_dict() if (scaler is not None and getattr(scaler, \"is_enabled\", lambda: False)()) else None,\n",
    "        \"cfg\": cfg,\n",
    "        \"rng\": {\n",
    "            \"torch_cpu\": torch.random.get_rng_state(),\n",
    "            \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
    "        },\n",
    "        \"timestamp\": time.time(),\n",
    "        \"model\": base_model.state_dict()\n",
    "    }\n",
    "\n",
    "    # you'll see multiples of uXXXXXX.pt in checkpoint\n",
    "    # this is what generates thsoe .pt filess\n",
    "    torch.save(payload, path)\n",
    "\n",
    "    manifest[\"last_update\"] = int(update)\n",
    "    if val_loss is not None:\n",
    "        m_best = manifest.get(\"best_val\")\n",
    "        if (m_best is None) or (val_loss < m_best):\n",
    "            manifest[\"best_val\"] = float(val_loss)\n",
    "    manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "    return str(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbebe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path: Path, model, optimizer, scheduler, scaler, map_location):\n",
    "    # if we don't see any checkpoint.pt?\n",
    "    # ⭐ fresh start ⭐ \n",
    "    # yeah... without checkpoint, if you got crash or OOM during run, you'll have to run all over again...\n",
    "    if not path.exists():\n",
    "        print(\"no checkpoint found, starting fresh\")\n",
    "        return 1, None\n",
    "    \n",
    "    # load that checkpoint.pt\n",
    "    ckpt = torch.load(path, map_location=map_location, weights_only=False)\n",
    "\n",
    "    # and re-configure everything\n",
    "    raw_sd = ckpt[\"model\"]\n",
    "    raw_sd = _strip_prefix_in_state_dict(raw_sd, \"_orig_mod.\")\n",
    "    raw_sd = _strip_prefix_in_state_dict(raw_sd, \"module.\")\n",
    "    model.load_state_dict(raw_sd)\n",
    "    \n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    if scaler is not None and ckpt.get(\"scaler\") is not None:\n",
    "        try: scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "        except Exception as e: print(f\"[warn] AMP scaler restore failed: {e}\")\n",
    "\n",
    "    rng = ckpt.get(\"rng\", {})\n",
    "    try:\n",
    "        cpu_state = _to_byte_tensor(rng.get(\"torch_cpu\"))\n",
    "        if cpu_state is not None: torch.random.set_rng_state(cpu_state)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] CPU RNG restore skipped: {e}\")\n",
    "\n",
    "    try:\n",
    "        cuda_states = rng.get(\"torch_cuda\")\n",
    "        if torch.cuda.is_available() and cuda_states is not None:\n",
    "            if isinstance(cuda_states, (list, tuple)):\n",
    "                cuda_states = [_to_byte_tensor(s) for s in cuda_states]\n",
    "                torch.cuda.set_rng_state_all(cuda_states)\n",
    "            else:\n",
    "                torch.cuda.set_rng_state(_to_byte_tensor(cuda_states))\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] CUDA RNG restore skipped: {e}\")\n",
    "\n",
    "    start = int(ckpt.get(\"update\", 0)) + 1\n",
    "    best  = ckpt.get(\"val_loss\", None)\n",
    "    print(f\"🔄 resumed from {path} @ update {start-1} (best_val={best})\")\n",
    "    return start, best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0209041",
   "metadata": {},
   "source": [
    "# 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e1c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# masked value\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "# THE GREEDIEST METHOD\n",
    "# always return loss (that's not none)\n",
    "# I never did this in train.py (i got lazy on fixing that bug when testing pretraining cell)\n",
    "def forward_with_guaranteed_loss(model, xb, yb):\n",
    "    try:\n",
    "        out = model(xb, labels=yb)\n",
    "    except TypeError:\n",
    "        out = model(xb, yb)\n",
    "\n",
    "    if isinstance(out, (tuple, list)):\n",
    "        logits, maybe_loss = out[0], (out[1] if len(out) > 1 else None)\n",
    "    elif hasattr(out, \"logits\") or hasattr(out, \"loss\"):\n",
    "        logits = getattr(out, \"logits\", out)\n",
    "        maybe_loss = getattr(out, \"loss\", None)\n",
    "    else:\n",
    "        logits, maybe_loss = out, None\n",
    "\n",
    "    if maybe_loss is None:\n",
    "        V = logits.shape[-1]\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, V),\n",
    "            yb.reshape(-1),\n",
    "            ignore_index=IGNORE_INDEX,\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "    else:\n",
    "        loss = maybe_loss\n",
    "\n",
    "    return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c42e9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "@torch.no_grad() # disables gradient tracking\n",
    "def evaluate(model, loader, device, amp_dtype=None, max_batches=None):\n",
    "    model_was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    # amp/autocast context\n",
    "    if amp_dtype in (\"bf16\", \"bfloat16\", torch.bfloat16):\n",
    "        autocast_ctx = torch.autocast(device_type=device.type, dtype=torch.bfloat16)\n",
    "    elif amp_dtype in (\"fp16\", \"float16\", torch.float16):\n",
    "        autocast_ctx = torch.autocast(device_type=device.type, dtype=torch.float16)\n",
    "    else:\n",
    "        autocast_ctx = nullcontext()\n",
    "\n",
    "    total_loss_weighted = 0.0  # sum of (loss * valid_tokens)\n",
    "    total_valid_tokens  = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with autocast_ctx:\n",
    "            for b_idx, batch in enumerate(loader):\n",
    "                if isinstance(batch, (tuple, list)):\n",
    "                    xb, yb = batch[0], batch[1]\n",
    "                elif isinstance(batch, dict):\n",
    "                    xb, yb = batch[\"input_ids\"], batch.get(\"labels\", batch.get(\"targets\"))\n",
    "                else:\n",
    "                    raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
    "\n",
    "                xb = xb.to(device, non_blocking=True)\n",
    "                yb = yb.to(device, non_blocking=True).long()\n",
    "\n",
    "                _, loss = forward_with_guaranteed_loss(model, xb, yb)\n",
    "\n",
    "                # count only valid tokens\n",
    "                valid = (yb != IGNORE_INDEX).sum().item()\n",
    "                if valid == 0:\n",
    "                    # skip completely masked batches\n",
    "                    continue\n",
    "\n",
    "                total_loss_weighted += loss.item() * valid\n",
    "                total_valid_tokens  += valid\n",
    "\n",
    "                if max_batches is not None and (b_idx + 1) >= max_batches:\n",
    "                    break\n",
    "\n",
    "    # Avoid div by zero\n",
    "    if total_valid_tokens == 0:\n",
    "        mean_loss = float(\"nan\")\n",
    "        ppl = float(\"inf\")\n",
    "    else:\n",
    "        mean_loss = total_loss_weighted / total_valid_tokens\n",
    "        ppl = float(\"inf\") if not (mean_loss == mean_loss) else float(torch.exp(torch.tensor(mean_loss)).item())\n",
    "\n",
    "    if model_was_training:\n",
    "        model.train()\n",
    "\n",
    "    return mean_loss, ppl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60c151",
   "metadata": {},
   "source": [
    "# 7. Model & dataset wiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c0d9682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sdpa_kernel\n",
      "params: 18552320\n",
      "batch OK: torch.Size([8, 128]) torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "# grabbing model and dataset\n",
    "# model and sets are in different directory\n",
    "# so, we'll gonna change the path\n",
    "import sys\n",
    "sys.path.append(\"../python_files\")\n",
    "from npy_datasets import NpyTokensDataset\n",
    "from Dummy_Model import DummyModel\n",
    "\n",
    "if isfull:\n",
    "    TRAIN_BLOCKS = \"../materials/train_blocks.npy\"\n",
    "    VALID_BLOCKS = \"../materials/valid_blocks.npy\"\n",
    "else:\n",
    "    TRAIN_BLOCKS = \"../materials_small/train_blocks.npy\"\n",
    "    VALID_BLOCKS = \"../materials_small/valid_blocks.npy\"\n",
    "\n",
    "# datasets\n",
    "train_ds = NpyTokensDataset(TRAIN_BLOCKS, cfg[\"context_length\"])\n",
    "valid_ds = NpyTokensDataset(VALID_BLOCKS, cfg[\"context_length\"])\n",
    "\n",
    "base_kwargs = dict(\n",
    "    batch_size=MICRO_BSZ,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_kwargs = dict(base_kwargs)\n",
    "valid_kwargs[\"shuffle\"] = False\n",
    "\n",
    "# pin to GPU if available\n",
    "try:\n",
    "    base_kwargs[\"pin_memory_device\"] = \"cuda\"\n",
    "    valid_kwargs[\"pin_memory_device\"] = \"cuda\"\n",
    "except TypeError:\n",
    "    pass\n",
    "\n",
    "# loader\n",
    "train_loader = DataLoader(train_ds, **base_kwargs)\n",
    "valid_loader = DataLoader(valid_ds, **valid_kwargs)\n",
    "\n",
    "# model\n",
    "# if you ran all the way down to here from start, you should have cfg and device variables defined\n",
    "model = DummyModel(cfg).to(device)\n",
    "print(\"params:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# check batch sanity\n",
    "xb, yb = next(iter(train_loader))\n",
    "assert xb.dtype == torch.int64 and yb.dtype == torch.int64\n",
    "assert xb.shape[1] == cfg[\"context_length\"] and yb.shape == xb.shape\n",
    "print(\"batch OK:\", xb.shape, yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b4d7b",
   "metadata": {},
   "source": [
    "# 8. Optimizer (AdamW)\n",
    "list of torch.optim:\n",
    "- torch.optim.AdamW\n",
    "- torch.optim.Adadelta\n",
    "- torch.optim.Adafactor\n",
    "- torch.optim.Adagrad\n",
    "- torch.optim.Adam\n",
    "- torch.optim.Adamax\n",
    "- torch.optim.ASGD\n",
    "- torch.optim.Adadelta\n",
    "- torch.optim.LBFGS\n",
    "- torch.optim.NAdam\n",
    "- torch.optim.Optimizer\n",
    "- torch.optim.RAdam\n",
    "- torch.optim.SGD\n",
    "- torch.optim.RMSprop\n",
    "- torch.optim.Rprop\n",
    "- torch.optim.SparseAdam\n",
    "\n",
    "(Adam is short for Adaptive Moment Estimate. Not a person's name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdcdff",
   "metadata": {},
   "source": [
    "##### TOP 2 Optimizers often used (without AdamW):\n",
    "- Adam (this is later upgraded to AdamW, which we're going to use it)\n",
    "    - How it works: adaptive learning rates per parameter (tracks 1st and 2nd moments of gradients)\n",
    "    - Pros:\n",
    "        - it's FAST\n",
    "        - minimal hyper parameter tuning needed\n",
    "    - Cons:\n",
    "        - may cause overfit\n",
    "        - memory hungry (since it keeps 2 extra states per parameter)\n",
    "    - Example: GPT-1\n",
    "- LAMB\n",
    "    - How it works: builds an AdamW but scales the learning rate per layer by the ratio of parameter norm to update norm\n",
    "    - Pros:\n",
    "        - Enables very large batch training without divergence\n",
    "        - designed for scaling BERT/GPT pretraining across hundreds of GPUs\n",
    "    - Cons:\n",
    "        - complex and slower per-step than AdamW\n",
    "        - gains are most relevant only at extreme batch scales\n",
    "    - Example: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef962b6",
   "metadata": {},
   "source": [
    "##### AdamW (we're going to use this guy)\n",
    "<img src=\"./AdamW.png\" width=\"500\"/><br/>\n",
    "- How it works: in big picture same thing with Adam, but it decouples weight decay from gradient updates <br/>\n",
    "    (there's more but i'm trimming in here)\n",
    "- Pros:\n",
    "    - better generalization than Adam (yup. that decoupling weight decay)\n",
    "    - stable convergence with large model\n",
    "- Cons:\n",
    "    - memory cost since it still keeps 1st and 2nd states (like Adam)\n",
    "- Example: GPT-4, DeepMind (Google), LLaMA (Meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d94ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember we've made all those weight dacay & cosine warmup?\n",
    "# yup. we're going to use those here\n",
    "\n",
    "# weight decay\n",
    "opt_groups = param_groups_weight_decay(model, WEIGHT_DECAY)\n",
    "\n",
    "# if torch supports fused, use that\n",
    "try:\n",
    "    # fused=True makes cumputation faster\n",
    "    optimizer = torch.optim.AdamW(opt_groups, lr=BASE_LR, betas=(0.9,0.95), eps=1e-8, fused=True)\n",
    "\n",
    "# if not, just use the defaulted one\n",
    "# computation is slower. but results are identical\n",
    "except TypeError:\n",
    "    optimizer = torch.optim.AdamW(opt_groups, lr=BASE_LR, betas=(0.9,0.95), eps=1e-8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3b861",
   "metadata": {},
   "source": [
    "##### (TL;DR)\n",
    "<img src=\"./AdamW_actual.png\" width=\"300\"/><br/>\n",
    "the actual graph would look something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079687a9",
   "metadata": {},
   "source": [
    "# 9. Scheduler and AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f50b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let scheduler use that cosine stuff\n",
    "scheduler = build_cosine_with_warmup(\n",
    "    optimizer, warmup_steps=WARMUP_STEPS, total_steps=TOTAL_UPDATES, min_lr_ratio=0.10\n",
    ")\n",
    "\n",
    "# define AMP_MODE\n",
    "if AMP_MODE == \"fp16\":\n",
    "    amp_dtype = torch.float16\n",
    "    scaler = GradScaler(enabled=True)\n",
    "elif AMP_MODE == \"bf16\":\n",
    "    amp_dtype = torch.bfloat16\n",
    "    scaler = GradScaler(enabled=False)\n",
    "else:\n",
    "    amp_dtype = None\n",
    "    scaler = GradScaler(enabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97ca95",
   "metadata": {},
   "source": [
    "# 10. Define constants for checkpoints\n",
    "note that if you run this, it will remove everything in pretrain_results_test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3da79d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run dirs & manifest & history ---\n",
    "PRETRAIN_DIR, CKPT_DIR, PLOTS_DIR, LOGS_DIR = make_run_dirs(RUN_DIR)\n",
    "manifest, MANIFEST_PATH = manifest_load_or_init(PRETRAIN_DIR)\n",
    "HISTORY_CSV = LOGS_DIR / \"history.csv\"\n",
    "history_init(HISTORY_CSV)\n",
    "\n",
    "LATEST = CKPT_DIR / \"latest.pt\"\n",
    "BEST   = CKPT_DIR / \"best.pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca577a",
   "metadata": {},
   "source": [
    "# 11. Loading checkpoints\n",
    "after all the way down to pretesting, we'll be running this again<br/>\n",
    "to verify that the resuming works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e1582ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoint found, starting fresh\n",
      "🚀 Starting fresh training run.\n"
     ]
    }
   ],
   "source": [
    "start_update, best_val = load_checkpoint(LATEST, model, optimizer, scheduler, scaler, device)\n",
    "if best_val is None: best_val = float(\"inf\")\n",
    "\n",
    "if start_update > 1:\n",
    "    print(f\"🔄 RESUMING TRAINING from update {start_update - 1} (best={best_val}) in {PRETRAIN_DIR}\")\n",
    "else:\n",
    "    print(\"🚀 Starting fresh training run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f298adf",
   "metadata": {},
   "source": [
    "##### **IF YOUR TESTING RESUME, RUN CODE ALL THE WAY DOWN AGAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6acf18e",
   "metadata": {},
   "source": [
    "# 12. Train helpers & alerts\n",
    "<img src=\"./periodic_register_val.png\" width=\"800\"/><br/>\n",
    "it's basically this part (+other blocks connected to) in the TRAIN block of diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e88133d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = TopK(k=5)\n",
    "\n",
    "def save_periodic(update, val_loss, every=PERIODIC_EVERY):\n",
    "    if update % every == 0:\n",
    "        tag = f\"u{update:07d}\" # for example, u0000100\n",
    "        save_checkpoint(CKPT_DIR, manifest, MANIFEST_PATH, model, optimizer, scheduler, scaler, update, val_loss, tag)\n",
    "        print(f\"💾 saved periodic checkpoint: {tag}\")\n",
    "\n",
    "def register_val_result(update, val_loss):\n",
    "    # always save latest\n",
    "    save_checkpoint(CKPT_DIR, manifest, MANIFEST_PATH, model, optimizer, scheduler, scaler, update, val_loss, \"latest\")\n",
    "\n",
    "    # track and save best so far\n",
    "    current_best = getattr(register_val_result, \"_best\", float(\"inf\"))\n",
    "    if val_loss < current_best:\n",
    "        save_checkpoint(CKPT_DIR, manifest, MANIFEST_PATH, model, optimizer, scheduler, scaler, update, val_loss, \"best\")\n",
    "        register_val_result._best = val_loss\n",
    "        print(f\"⭐ new best: {val_loss:.4f} @ upd {update}\")\n",
    "    tag = f\"u{update:07d}\"\n",
    "    ckpt_path = CKPT_DIR / f\"{tag}.pt\"\n",
    "    topk.add(val_loss, update, str(ckpt_path))\n",
    "\n",
    "# alerts\n",
    "# later, we'll be using these constants in train loop\n",
    "BASELINE_WINDOW       = 50\n",
    "TOKENS_SEC_ALERT_DROP = 0.30\n",
    "LOSS_SPIKE_X          = 2.0\n",
    "CONSEC_FOR_ALERT      = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd4ebf",
   "metadata": {},
   "source": [
    "# 13. Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df33ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm # for progress bar\n",
    "\n",
    "def train_for_updates(num_updates, tag):\n",
    "    global start_update, best_val\n",
    "    end_update = start_update + num_updates - 1\n",
    "\n",
    "    # smoothing meters (using Exponential Moving Average)\n",
    "    tps_ema  = EMAMeter(0.8) # (closer to 1 = smoother; but slower updates)\n",
    "    loss_ema = EMAMeter(0.9)\n",
    "\n",
    "    # baselines used for alerting\n",
    "    base_tps = None\n",
    "    base_loss= None\n",
    "    tps_bad  = 0 # consecutive bad throughput updates\n",
    "    loss_bad = 0 # consecutive bad loss spikes\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    loader_iter = iter(train_loader)\n",
    "\n",
    "    # progress bar\n",
    "    pbar = tqdm(range(start_update, end_update + 1), desc=f\"[{tag}]\", unit=\"upd\")\n",
    "    \n",
    "    for update in pbar:\n",
    "        # forward & backward pass\n",
    "        # reset gradients each step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # start timer\n",
    "        upd_t0 = time.time()\n",
    "\n",
    "        # accumulated loss (for gradient)\n",
    "        loss_accum = 0.0 # track accumulated loss across ACCUM steeps\n",
    "\n",
    "        # gradient accumulation\n",
    "        for _ in range(ACCUM):\n",
    "            try:\n",
    "                xb, yb = next(loader_iter) # get batch\n",
    "            except StopIteration:\n",
    "                # restart dataloader when ephch exhausted\n",
    "                loader_iter = iter(train_loader); xb, yb = next(loader_iter)\n",
    "\n",
    "            # move to device (cuda)\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).long()\n",
    "\n",
    "            # forward pass\n",
    "            # if we're using fp16 or df16\n",
    "            if amp_dtype is not None:\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                    logits, loss = forward_with_guaranteed_loss(model, xb, yb)\n",
    "            # if \"none\"\n",
    "            else:\n",
    "                logits, loss = forward_with_guaranteed_loss(model, xb, yb)\n",
    "\n",
    "            # normalize loss for accumulation\n",
    "            loss = loss / ACCUM\n",
    "\n",
    "            # backward pass\n",
    "            if scaler and scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # accumulate for logging (not for backprop)\n",
    "            loss_accum += float(loss)\n",
    "\n",
    "        # optimizer step\n",
    "        # unscale gradients if AMP is on\n",
    "        try:\n",
    "            if scaler and scaler.is_enabled():\n",
    "                scaler.unscale_(optimizer)\n",
    "        except RuntimeError:\n",
    "            pass # if grads already unscaled or there's an issue, just pass\n",
    "        \n",
    "        # gradient clipping (prevent exploding gradients)\n",
    "        gnorm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "        try:\n",
    "            if scaler and scaler.is_enabled():\n",
    "                scaler.step(optimizer) # AMP step\n",
    "            else:\n",
    "                optimizer.step() # regular step\n",
    "        except AssertionError:\n",
    "            # if AMP scale gets NaN, just skip update\n",
    "            if scaler and scaler.is_enabled(): scaler.update()\n",
    "            scheduler.step()\n",
    "            continue\n",
    "        \n",
    "        # update scale if AMP\n",
    "        if scaler and scaler.is_enabled():\n",
    "            scaler.update()\n",
    "\n",
    "        # update learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # logging & metrics\n",
    "        dt_upd  = max(time.time() - upd_t0, 1e-6) # elapsed time\n",
    "        tps     = EFFECTIVE_TOKENS_PER_UPDATE / dt_upd # throughput\n",
    "        tps_sm  = tps_ema.update(tps) # smoothed tps\n",
    "        loss_sm = loss_ema.update(loss_accum * ACCUM) # smoothed loss\n",
    "        gpu_gb  = (torch.cuda.max_memory_allocated() / 1e9) if torch.cuda.is_available() else 0.0\n",
    "        amp_sc  = float(getattr(scaler, \"get_scale\", lambda: 1.0)()) # AMP scale factor\n",
    "\n",
    "        # track tokens processed\n",
    "        tokens_seen = update * EFFECTIVE_TOKENS_PER_UPDATE\n",
    "\n",
    "        # log training row to CSV\n",
    "        log_train_row(HISTORY_CSV, update, loss_sm, optimizer.param_groups[0]['lr'], tps_sm,\n",
    "                      float(gnorm), float(amp_sc), tokens_seen)\n",
    "\n",
    "        # establish baseline after warmup window\n",
    "        seen = update - start_update + 1\n",
    "        if seen == BASELINE_WINDOW:\n",
    "            base_tps  = tps_sm\n",
    "            base_loss = loss_sm\n",
    "\n",
    "        # detect throughput drop\n",
    "        if base_tps is not None and base_tps > 0:\n",
    "            tps_bad = tps_bad + 1 if tps_sm < (1.0 - TOKENS_SEC_ALERT_DROP) * base_tps else 0\n",
    "\n",
    "        # detect loss spike\n",
    "        if base_loss is not None and base_loss > 0:\n",
    "            loss_bad = loss_bad + 1 if loss_sm > LOSS_SPIKE_X * base_loss else 0\n",
    "\n",
    "        # trigger alert if bad condition sustained\n",
    "        if (tps_bad >= CONSEC_FOR_ALERT) or (loss_bad >= CONSEC_FOR_ALERT):\n",
    "            print(f\"\\n⚠️  ALERT @ upd {update}: \"\n",
    "                  f\"{'tps drop' if tps_bad>=CONSEC_FOR_ALERT else ''}\"\n",
    "                  f\"{' and ' if tps_bad>=CONSEC_FOR_ALERT and loss_bad>=CONSEC_FOR_ALERT else ''}\"\n",
    "                  f\"{'loss spike' if loss_bad>=CONSEC_FOR_ALERT else ''}. \"\n",
    "                  f\"tps≈{tps_sm:,.0f} (base≈{base_tps:,.0f}), loss≈{loss_sm:.4f} (base≈{base_loss:.4f})\")\n",
    "            \n",
    "            # run validation eval\n",
    "            val_loss, val_ppl = evaluate(model, valid_loader, device, amp_dtype, max_batches=15)\n",
    "            print(f\"   immediate eval: val_loss={val_loss:.4f}  ppl={val_ppl:.2f}\")\n",
    "\n",
    "            # save emergency checkpoint & plot\n",
    "            save_checkpoint(CKPT_DIR, manifest, MANIFEST_PATH, model, optimizer, scheduler, scaler, update, val_loss,\n",
    "                            tag=f\"alert_u{update:07d}\")\n",
    "            save_plot(HISTORY_CSV, PLOTS_DIR, EFFECTIVE_TOKENS_PER_UPDATE, save_tag=f\"alert_u{update:07d}\")\n",
    "            tps_bad = loss_bad = 0 # reset counters\n",
    "\n",
    "        # periodic logging\n",
    "        if update % LOG_EVERY == 0:\n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{loss_sm:.4f}\",\n",
    "                lr=f\"{optimizer.param_groups[0]['lr']:.2e}\",\n",
    "                tps=f\"{tps_sm:,.0f}\",\n",
    "                gpu=f\"{gpu_gb:.2f}GB\",\n",
    "                gnorm=f\"{float(gnorm):.2f}\",\n",
    "                scale=f\"{amp_sc:.0f}\",\n",
    "            )\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.reset_peak_memory_stats(device)\n",
    "        \n",
    "        # periodic evaluation\n",
    "        if update % EVAL_EVERY == 0:\n",
    "            val_loss, val_ppl = evaluate(model, valid_loader, device, amp_dtype, max_batches=15)\n",
    "            print(f\"\\n[upd {update}] val_loss={val_loss:.4f}  ppl={val_ppl:.2f}\")\n",
    "\n",
    "            # log validation row to CSV\n",
    "            log_val_row(HISTORY_CSV, update, val_loss, val_ppl, tokens_seen)\n",
    "\n",
    "            # periodic checkpoint (not every step, but spaced)\n",
    "            save_periodic(update, val_loss, every=PERIODIC_EVERY)\n",
    "            register_val_result(update, val_loss)\n",
    "\n",
    "            # update training/validation plot\n",
    "            save_plot(HISTORY_CSV, PLOTS_DIR, EFFECTIVE_TOKENS_PER_UPDATE, save_tag=f\"u{update:07d}\")\n",
    "\n",
    "        # increment global counter\n",
    "        start_update = update + 1\n",
    "\n",
    "    # final save (boundary)\n",
    "    save_checkpoint(CKPT_DIR, manifest, MANIFEST_PATH, model, optimizer, scheduler, scaler,\n",
    "                    start_update - 1, None, tag=f\"u{start_update-1:07d}\")\n",
    "    \n",
    "    # mark chunk completion in manifest\n",
    "    if tag.startswith(\"chunk\"):\n",
    "        manifest[\"chunks_completed\"] = int(manifest.get(\"chunks_completed\", 0)) + 1\n",
    "        MANIFEST_PATH.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "    print(f\"✅ Finished {tag}. Reached update {start_update-1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e400dcd3",
   "metadata": {},
   "source": [
    "##### **RUN THIS IF YOU'RE TESTING RESUME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a381adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_UPDATES   = 300 # before it was 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953db7a",
   "metadata": {},
   "source": [
    "#### **ONLY FOR RESUME TEST ↑**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33a03489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408723c83a714ceaa4073958a2ea2a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[pilot]:   0%|          | 0/15 [00:00<?, ?upd/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished pilot. Reached update 15.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4ebb0583b7430a89fff5748d1689f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[chunk1]:   0%|          | 0/142 [00:00<?, ?upd/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[upd 50] val_loss=8.0667  ppl=3186.71\n",
      "⭐ new best: 8.0667 @ upd 50\n",
      "🖼️ saved plot → ..\\pretrain_checkpoint_notebook\\plots\\loss_curve_u0000050.png\n",
      "\n",
      "[upd 100] val_loss=7.8978  ppl=2691.40\n",
      "💾 saved periodic checkpoint: u0000100\n",
      "⭐ new best: 7.8978 @ upd 100\n",
      "🖼️ saved plot → ..\\pretrain_checkpoint_notebook\\plots\\loss_curve_u0000100.png\n",
      "\n",
      "[upd 150] val_loss=7.6175  ppl=2033.39\n",
      "⭐ new best: 7.6175 @ upd 150\n",
      "🖼️ saved plot → ..\\pretrain_checkpoint_notebook\\plots\\loss_curve_u0000150.png\n",
      "✅ Finished chunk1. Reached update 157.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711c22b3745a43299e21a6764a5595f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[chunk2]:   0%|          | 0/142 [00:00<?, ?upd/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[upd 200] val_loss=7.6171  ppl=2032.59\n",
      "💾 saved periodic checkpoint: u0000200\n",
      "⭐ new best: 7.6171 @ upd 200\n",
      "🖼️ saved plot → ..\\pretrain_checkpoint_notebook\\plots\\loss_curve_u0000200.png\n",
      "\n",
      "[upd 250] val_loss=7.5308  ppl=1864.62\n",
      "⭐ new best: 7.5308 @ upd 250\n",
      "🖼️ saved plot → ..\\pretrain_checkpoint_notebook\\plots\\loss_curve_u0000250.png\n",
      "✅ Finished chunk2. Reached update 299.\n",
      "🏁 Full run plan completed (demo schedule).\n",
      "Artifacts:\n",
      " - History CSV: ..\\pretrain_checkpoint_notebook\\logs\\history.csv\n",
      " - Plots dir:   ..\\pretrain_checkpoint_notebook\\plots\n",
      " - Ckpts dir:   ..\\pretrain_checkpoint_notebook\\checkpoints\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# plan driver (demo)\n",
    "# how many updates for pilot phase\n",
    "pilot_updates = max(1, int(PILOT_FRAC * TOTAL_UPDATES))\n",
    "\n",
    "# how many updates per chunk (remainingupdates split into CHUNKS)\n",
    "chunk_updates = max(1, (TOTAL_UPDATES - pilot_updates) // CHUNKS)\n",
    "\n",
    "# pilot phase\n",
    "if start_update <= pilot_updates:\n",
    "    # if we haven't finished pilot yet, calculate remaining work\n",
    "    todo = pilot_updates - (start_update - 1)\n",
    "    train_for_updates(todo, tag=\"pilot\")\n",
    "else:\n",
    "    # skip if we're already done\n",
    "    print(\"Pilot already completed (resume detected).\")\n",
    "\n",
    "# chunked training phase\n",
    "for i in range(1, CHUNKS + 1):\n",
    "    # target update at the end of this chunk\n",
    "    target_end = pilot_updates + i * chunk_updates\n",
    "\n",
    "    \n",
    "    if start_update <= target_end:\n",
    "        # still work left in this chunk\n",
    "        todo = target_end - (start_update - 1)\n",
    "        train_for_updates(todo, tag=f\"chunk{i}\")\n",
    "    else:\n",
    "        # skip this chunk if already completed\n",
    "        print(f\"chunk{i} already completed (resume detected).\")\n",
    "\n",
    "# final logs\n",
    "print(\"🏁 Full run plan completed (demo schedule).\")\n",
    "\n",
    "print(\"Artifacts:\")\n",
    "print(\" - History CSV:\", HISTORY_CSV)\n",
    "print(\" - Plots dir:  \", PLOTS_DIR)\n",
    "print(\" - Ckpts dir:  \", CKPT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080a26c",
   "metadata": {},
   "source": [
    "##### IF YOU'RE TESTING RESUME, IT SHOULD SEE\n",
    "```\n",
    "Pilot already completed (resume detected).\n",
    "chunk1 already completed (resume detected).\n",
    "```\n",
    "something like that <br/>\n",
    "and your checkpoint should now have ```u0000299.pt``` in ```pretrain_results_test/checkpoints``` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77d784",
   "metadata": {},
   "source": [
    "#### **GO BACK TO \"11. Loading checkpoints\" TO TEST RESUME**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5dfb8e",
   "metadata": {},
   "source": [
    "# (TL;DR)\n",
    "what i've done for checkpoints: <br/>\n",
    "suppose chunk = 4, total = 1050 (pilot= 50) <br/>\n",
    "first run: <br/>\n",
    "pilot: 50 <br/>\n",
    "chunk 1: 250 <br/>\n",
    "chunk 2: 250 <br/>\n",
    "chunk 3: 250 <br/>\n",
    "chunk 4: 250 <br/>\n",
    " <br/>\n",
    "increase total = 2050 (suppose pilot= 50) <br/>\n",
    "assuming we've done up to 1000 from first run, now <br/>\n",
    "pilot: 50 <br/>\n",
    "chunk 1: 500 <br/>\n",
    "chunk 2: 500 <- we resume from somewhere in chunk 2 <br/>\n",
    "chunk 3: 500 <br/>\n",
    "chunk 4: 500 <br/>\n",
    " <br/>\n",
    "so, yeah. if you decided to rerun and the chunk is different, THAT'S EXPECTED <br/>\n",
    "since we're reallocating items for each chunk. <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2803e6b",
   "metadata": {},
   "source": [
    "under ```python_files``` folder, you'll see ```train.py``` <br/>\n",
    "that's basically \n",
    "- **everything in this notebook**\n",
    "- **some modification** for ```args``` <br/>\n",
    "- grabbing ```model.json``` from directory to use those configuration\n",
    "- now, ctx is configurable (for pretraining + another pretraining for better result)\n",
    "since we'll be training in terminal\n",
    "\n",
    "(P.S.) Hey, i've done some experiment with ```train.py``` and now, the new one takes less then a minutes! (used to take 10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6405e1",
   "metadata": {},
   "source": [
    "# UPDATES\n",
    "- **Fast attention kernels**: sdpa_ctx_fast() forces Flash/Mem-Efficient SDPA when available.\n",
    "- **Allocator & math speedups**: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True, TF32 enabled, cuDNN benchmark, optional torch.compile.\n",
    "- **AMP modes**: runtime switch --amp {fp16,bf16,none} with proper GradScaler handling.\n",
    "- **Better DataLoader**: worker seeding, pin_memory_device='cuda', prefetch_factor.\n",
    "- **Optimizer & decay**: AdamW (fused if possible), 2-group weight-decay (no decay on norms/bias/1D).\n",
    "- **Scheduler**: cosine with warmup (10%) + min_lr_ratio floor.\n",
    "- **Resume safety**: strip module prefixes, fuse old q/k/v weights → qkv on load, resize pos_emb if ctx changes, rebuild LR tail if scheduler state missing.\n",
    "- **Run integrity**: per run-dir config immutability (raises on mismatch).\n",
    "- **Checkpointing**: light vs full payloads + manifest.json; Top-K best tracking.\n",
    "- **Monitoring**: tokens/sec & loss EMAs, dataloader latency in tqdm, alerts on TPS drop or loss spike → immediate eval + save + plot.\n",
    "- **Plotting & logs**: dual-axis loss plots (updates & tokens), CSV history.csv with train/val rows.\n",
    "- **Plan execution**: pilot + chunks + tail schedule; per-N-update grad clipping; quick CUDA timing probe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dffdfdf",
   "metadata": {},
   "source": [
    "# DONE!\n",
    "if you went through every cell (including resume) and don't see any error <br/>\n",
    "you're good to go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
