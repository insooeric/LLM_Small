{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d221c0",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed3b91",
   "metadata": {},
   "source": [
    "**FINALLY!** <br/>\n",
    "let's now do pretraining <br/>\n",
    "we're gonna do small one <br/>\n",
    "currently the sequence will be: <br/>\n",
    "1. load config\n",
    "2. pilot (wormup)\n",
    "3. chunks (8 chunks by default)\n",
    "4. tail (left over ones)\n",
    "5. done!\n",
    "\n",
    "<br/>\n",
    "in the terminal, move to root directory <br/>\n",
    "drop <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ec296",
   "metadata": {},
   "source": [
    "```\n",
    "python -u .\\python_files\\train.py `\n",
    "   --run-dir \".\\pretrain_checkpoint_small\" `\n",
    "   --train-blocks \".\\materials\\train_blocks.npy\" `\n",
    "   --valid-blocks \".\\materials\\valid_blocks.npy\" `\n",
    "   --config \".\\configs\\model_config_124M.json\" `\n",
    "   --workers 6 --prefetch 6 --persistent `\n",
    "   --micro-bsz 12 --accum 6 `\n",
    "   --total-updates 20 --eval-every 10 --periodic-every 10 --log-every 10`\n",
    "   --torch-compile --compile-mode reduce-overhead `\n",
    "   --min-lr-ratio 0.05\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c2ed7",
   "metadata": {},
   "source": [
    "when it's done, run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befae222",
   "metadata": {},
   "source": [
    "```\n",
    "python -u .\\python_files\\train.py `\n",
    "   --run-dir \".\\pretrain_checkpoint_small\" `\n",
    "   --train-blocks \".\\materials\\train_blocks.npy\" `\n",
    "   --valid-blocks \".\\materials\\valid_blocks.npy\" `\n",
    "   --config \".\\configs\\model_config_124M.json\" `\n",
    "   --workers 6 --prefetch 6 --persistent `\n",
    "   --micro-bsz 12 --accum 6 `\n",
    "   --total-updates 35 --eval-every 10 --periodic-every 10 --log-every 10`\n",
    "   --torch-compile --compile-mode reduce-overhead `\n",
    "   --min-lr-ratio 0.05\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ba5fe",
   "metadata": {},
   "source": [
    "the only difference is now --total-updates is 35 (previously 20; increased for resuming test; extra 15 iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6d937",
   "metadata": {},
   "source": [
    "# (TL;DR)\n",
    "below is what i've done for the real one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc65a97",
   "metadata": {},
   "source": [
    "```\n",
    "python -u .\\python_files\\train.py `\n",
    "   --run-dir \".\\pretrain_checkpoint\" `\n",
    "   --train-blocks \".\\materials\\train_blocks.npy\" `\n",
    "   --valid-blocks \".\\materials\\valid_blocks.npy\" `\n",
    "   --config \".\\configs\\model_config_124M.json\" `\n",
    "   --workers 6 --prefetch 6 --persistent `\n",
    "   --micro-bsz 12 --accum 6 `\n",
    "   --total-updates 25000 --eval-every 1000 --periodic-every 1000 --log-every 200`\n",
    "   --amp bf16 `\n",
    "   --torch-compile --compile-mode reduce-overhead `\n",
    "   --min-lr-ratio 0.05\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83ae3b",
   "metadata": {},
   "source": [
    "i found the problem where there were a missmatch when resuming; <br/>\n",
    "hence, the train_loss (blue graph) was fluctuating so bad, and i couldn't figure it out the problem <br/>\n",
    "so, i decided to just run 25000 updates at once <br/>\n",
    "<br/>\n",
    "with RTX3060 laptop, it took around 10 days... (with 18s/upd)<br/>\n",
    "and i had problem with tokenizer, meaning i had to run everything back again <br/>\n",
    "so, i'm just gonna run with RTX3080 laptop <br/>\n",
    "<br/>\n",
    "Alright! that took around 21h 30m. <br/>\n",
    "still long but waaaaay better than 10 days lmao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3d217",
   "metadata": {},
   "source": [
    "# DONE!\n",
    "for time saving, we'll be using the one i've already done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
