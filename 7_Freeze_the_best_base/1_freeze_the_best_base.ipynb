{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3d67ff",
   "metadata": {},
   "source": [
    "# Freeze the best base\n",
    "What's the point? <br/>\n",
    "- fixing the starting weights to ensure any gains/bugs during SFT (supervised fine tuning) come from SFT, not drift in later-pretrain checkpoints.\n",
    "- pretrain can keep improving train loss while worsening valid. freezing the best and fully valid checkpoint prevents from accidently starting SFT from a worse step.\n",
    "- keep SFT separate from pretrain runs (so we don't overrite), and gives a stable baseline for later tasks (yeah, we'll do SFT/Lora and DPO/Lora later on)\n",
    "\n",
    "so, basically, this notebook will: <br/>\n",
    "1. pick a true base checkpoint out of multiple .pt in checkpoints (i just decided to work with best.pt since that's proven to be best looking over the log during pretraining. Else, when we iterate every .pt it's gonna take significant amount of time)\n",
    "2. evaluate best.pt with fp32\n",
    "3. export evaluation result to BEST_BASE_README.md (it's just for showing the result. it'll not going to be used later on)\n",
    "4. make a copy of best.pt as best_base.pt (starting point for SFT/Lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd7a36",
   "metadata": {},
   "source": [
    "# NOTE \n",
    "the whole thing with small pretrain took around an hour <br/>\n",
    "and with full pretrain took nearly 6 hours <br/> \n",
    "if you want to skip, just run `# NEVER RUN THIS IF YOU'VE DONE ABOVE` cell that's in the very bottom. <br/>\n",
    "it's gonna do the same thing except for detailed logs and others <br/>\n",
    "meaning we're going to completely trust that `best.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e5cd3",
   "metadata": {},
   "source": [
    "## **If you're testing full, turn this to true**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfull = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66837be",
   "metadata": {},
   "source": [
    "# Configure necessary stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58b8db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | Torch: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, shutil, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../python_files\")\n",
    "from Dummy_Model import DummyModel\n",
    "from npy_datasets import NpyTokensDataset\n",
    "\n",
    "if isfull:\n",
    "    RUN_DIR       = Path(\"../pretrain_checkpoint\")\n",
    "    VALID_BLOCKS  = Path(\"../materials/valid_blocks.npy\")\n",
    "else:\n",
    "    RUN_DIR       = Path(\"../pretrain_checkpoint_small\")\n",
    "    VALID_BLOCKS  = Path(\"../materials_small/valid_blocks.npy\")\n",
    "\n",
    "CONFIG_PATH   = Path(\"../configs/model_config_124M.json\")\n",
    "BEST_CKPT     = RUN_DIR / \"checkpoints\" / \"best.pt\"\n",
    "\n",
    "BEST_BASE_PT  = RUN_DIR / \"best_base.pt\"\n",
    "README_PATH   = RUN_DIR / \"BEST_BASE_README.md\"\n",
    "\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")  # faster matmuls on Ampere+, preserves FP32 numerics\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE, \"| Torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4aeaa",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "What's happening here is:\n",
    "1. W have rows of token numbers (a grid)\n",
    "2. Pick a window length T (in our case, 512 since that's context size)\n",
    "3. Cut each row into non-overlapping chunks of size T\n",
    "4. for each chunk, make: x = tokens[s: s+T], y = tokens[s+1: s+T+1]\n",
    "    - (Shift by 1 so the model learns the next token)\n",
    "5. number of samples: rows x floor((L-1)/T), where L is row length\n",
    "6. Order is fixed (no shuffle) -> reproducibe result\n",
    "7. Output type is int64 so it fits the model's embedding layer\n",
    "8. DataLoader just batches them (like 12 at a time) for faster eval\n",
    "\n",
    "for example: if T=4 <br/>\n",
    "Row:`[t0 t1 t2 t3 t4 t5 t6 t7 t8 t9]` <br/>\n",
    "if x: `[t0 t1 t2 t3]`, y: `[t1 t2 t3 t4]` <br/>\n",
    "=> x: `[t4 t5 t6 t7]`, y: `[t5 t6 t7 t8]` (the next tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b52500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid windows: 360053 | Seq len: 512 | Batch size: 12\n"
     ]
    }
   ],
   "source": [
    "class EvalTokensDataset(NpyTokensDataset):\n",
    "    def __len__(self):\n",
    "        if self._ndim == 1:\n",
    "            return max(0, (self._shape[0] - 1) // self.seq_len)\n",
    "        rows, L = self._shape\n",
    "        return max(0, rows * ((L - 1) // self.seq_len))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = self._arr\n",
    "        T = self.seq_len\n",
    "        if self._ndim == 1:\n",
    "            L = self._shape[0]\n",
    "            s = min(idx * T, L - T - 1)\n",
    "            x = arr[s:s+T].astype(np.int64, copy=False)\n",
    "            y = arr[s+1:s+T+1].astype(np.int64, copy=False)\n",
    "        else:\n",
    "            rows, L = self._shape\n",
    "            per_row = (L - 1) // T\n",
    "            r = idx % rows\n",
    "            k = (idx // rows) % per_row\n",
    "            s = min(k * T, L - T - 1)\n",
    "            row = arr[r]\n",
    "            x = row[s:s+T].astype(np.int64, copy=False)\n",
    "            y = row[s+1:s+T+1].astype(np.int64, copy=False)\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "# build dataloader\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "SEQ_LEN = int(cfg.get(\"context_length\", 512))\n",
    "\n",
    "# deterministic, full-valid dataset\n",
    "eval_ds = EvalTokensDataset(str(VALID_BLOCKS), SEQ_LEN)\n",
    "\n",
    "# since i'm working on windows, workers=0 to avoid multiprocessing spawn issues\n",
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE  = 12\n",
    "\n",
    "full_dl = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(NUM_WORKERS > 0)\n",
    ")\n",
    "\n",
    "print(f\"Valid windows: {len(eval_ds)} | Seq len: {SEQ_LEN} | Batch size: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652998f8",
   "metadata": {},
   "source": [
    "# Evaluation 2\n",
    "here's the plan. for evaluation:\n",
    "- first pass: fp16 + AMP with fp32 weights\n",
    "- second pass: fp32 with tf32 allowed\n",
    "\n",
    "so yeah. matmuls/convs will execute in tf32 for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19a0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Safe checkpoint loader (robust) ----------------\n",
    "import warnings, torch\n",
    "from pathlib import Path\n",
    "\n",
    "def safe_load_state_dict(ckpt_path: Path):\n",
    "    \"\"\"\n",
    "    Loads a model state_dict from common checkpoint layouts:\n",
    "    - plain state_dict\n",
    "    - {\"model\": state_dict}\n",
    "    - {\"state_dict\": state_dict}\n",
    "    - {\"module\": state_dict}  (DDP-style)\n",
    "    - {\"ema\": state_dict}     (if you saved EMA-only)\n",
    "    Tries weights_only=True first (PyTorch ≥2.4), falls back cleanly.\n",
    "    \"\"\"\n",
    "    def _torch_load(weights_only=True):\n",
    "        try:\n",
    "            return torch.load(ckpt_path, map_location=\"cpu\", weights_only=weights_only)\n",
    "        except TypeError:\n",
    "            # Old PyTorch without weights_only kwarg\n",
    "            return torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    obj = _torch_load(weights_only=True)\n",
    "\n",
    "    # If it's already a raw state_dict (tensor values), return as-is\n",
    "    if isinstance(obj, dict) and all(isinstance(k, str) for k in obj.keys()):\n",
    "        # Heuristics: dive into common containers\n",
    "        for key in (\"model\", \"state_dict\", \"module\", \"ema\", \"model_ema\"):\n",
    "            inner = obj.get(key, None)\n",
    "            if isinstance(inner, dict) and len(inner) > 0 and all(isinstance(k, str) for k in inner.keys()):\n",
    "                return inner\n",
    "        return obj  # looks like a proper state_dict already\n",
    "\n",
    "    # As a last resort, try loading without weights_only (older pickles)\n",
    "    obj = _torch_load(weights_only=False)\n",
    "    if isinstance(obj, dict):\n",
    "        for key in (\"model\", \"state_dict\", \"module\", \"ema\", \"model_ema\"):\n",
    "            inner = obj.get(key, None)\n",
    "            if isinstance(inner, dict):\n",
    "                return inner\n",
    "        return obj\n",
    "    raise RuntimeError(f\"Unrecognized checkpoint format at {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afbf5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Best-practice full-valid scorer ----------------\n",
    "import math, torch, torch.nn.functional as F\n",
    "from contextlib import nullcontext\n",
    "\n",
    "IGNORE_INDEX = -100  # keep consistent with your project\n",
    "\n",
    "def _extract_logits_and_loss(out):\n",
    "    \"\"\"Accepts (logits, loss), logits-only, or obj/dict with .logits/.loss.\"\"\"\n",
    "    logits, loss = None, None\n",
    "    if isinstance(out, (tuple, list)):\n",
    "        logits = out[0]\n",
    "        loss   = out[1] if len(out) > 1 else None\n",
    "    elif isinstance(out, dict):\n",
    "        logits = out.get(\"logits\", None)\n",
    "        loss   = out.get(\"loss\", None)\n",
    "    else:\n",
    "        logits = getattr(out, \"logits\", out)\n",
    "        loss   = getattr(out, \"loss\", None)\n",
    "    return logits, loss\n",
    "\n",
    "@torch.inference_mode()\n",
    "def forward_with_guaranteed_loss(model, xb, yb):\n",
    "    \"\"\"\n",
    "    Always returns (logits, loss). Prefers model(..., labels=yb).\n",
    "    Falls back to manual CE if loss is None. Respects IGNORE_INDEX.\n",
    "    \"\"\"\n",
    "    out = model(xb, labels=yb)  # explicit kwarg avoids positional API pitfalls\n",
    "    logits, loss = _extract_logits_and_loss(out)\n",
    "    if loss is None:\n",
    "        V = logits.size(-1)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, V),\n",
    "            yb.reshape(-1),\n",
    "            ignore_index=IGNORE_INDEX,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "    return logits, loss\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_model_full_valid(model: torch.nn.Module,\n",
    "                           loader,\n",
    "                           device: torch.device,\n",
    "                           precision: str = \"fp32\",     # {\"fp32\",\"amp_fp16\"}\n",
    "                           strict_fp32: bool = False,\n",
    "                           show_pbar: bool = True):\n",
    "    \"\"\"\n",
    "    Evaluates masked average loss (ignores targets == IGNORE_INDEX) and perplexity.\n",
    "    Restores TF32 flags and training mode afterwards.\n",
    "    \"\"\"\n",
    "    # preserve train/eval and TF32 flags\n",
    "    was_training = model.training\n",
    "    prev_matmul_tf32 = torch.backends.cuda.matmul.allow_tf32 if device.type == \"cuda\" else None\n",
    "    prev_cudnn_tf32  = torch.backends.cudnn.allow_tf32        if device.type == \"cuda\" else None\n",
    "\n",
    "    model.eval().to(device)\n",
    "\n",
    "    try:\n",
    "        # TF32 policy\n",
    "        if device.type == \"cuda\":\n",
    "            if precision == \"fp32\" and strict_fp32:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = False\n",
    "                torch.backends.cudnn.allow_tf32 = False\n",
    "            else:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        use_amp = (precision == \"amp_fp16\" and device.type == \"cuda\")\n",
    "        amp_ctx = torch.autocast(device_type=\"cuda\", dtype=torch.float16) if use_amp else nullcontext()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_kept = 0\n",
    "\n",
    "        it = loader\n",
    "        if show_pbar:\n",
    "            try:\n",
    "                from tqdm.auto import tqdm\n",
    "                it = tqdm(loader, total=len(loader),\n",
    "                          desc=f\"full-valid ({precision})\",\n",
    "                          unit=\"batch\", dynamic_ncols=True, leave=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        for xb, yb in it:\n",
    "            xb = xb.to(device, non_blocking=True).long()\n",
    "            yb = yb.to(device, non_blocking=True).long()\n",
    "\n",
    "            with amp_ctx:\n",
    "                _, loss = forward_with_guaranteed_loss(model, xb, yb)\n",
    "\n",
    "            kept = (yb != IGNORE_INDEX).sum().item()\n",
    "            if kept == 0:  # e.g., pure pretrain eval with no mask\n",
    "                kept = yb.numel()\n",
    "\n",
    "            total_loss += float(loss) * kept\n",
    "            total_kept += kept\n",
    "\n",
    "            if show_pbar and hasattr(it, \"set_postfix\"):\n",
    "                avg = total_loss / max(1, total_kept)\n",
    "                it.set_postfix(avg_loss=f\"{avg:.4f}\", ppl=f\"{math.exp(avg):.2f}\")\n",
    "\n",
    "        avg_loss = total_loss / max(1, total_kept)\n",
    "        ppl = math.exp(min(20.0, avg_loss))  # clamp to avoid inf on spikes\n",
    "        return avg_loss, ppl\n",
    "\n",
    "    finally:\n",
    "        # restore flags and mode\n",
    "        model.train(was_training)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.backends.cuda.matmul.allow_tf32 = prev_matmul_tf32\n",
    "            torch.backends.cudnn.allow_tf32 = prev_cudnn_tf32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56da58",
   "metadata": {},
   "source": [
    "# Full validation & rescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698fa1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501e27bdf1ea4a438e0da2c069bf1b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "full-valid (amp_fp16):   0%|          | 0/30005 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec11484098ac49c6938735b4af79bc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "full-valid (fp32):   0%|          | 0/30005 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[best.pt] full-valid results:\n",
      "  amp_fp16 — loss=4.896934  ppl=133.88\n",
      "  fp32     — loss=4.896926  ppl=133.88\n"
     ]
    }
   ],
   "source": [
    "# making sure that we do have best.pt \n",
    "assert BEST_CKPT.is_file(), f\"Missing BEST_CKPT: {BEST_CKPT}\"\n",
    "\n",
    "# build model and move to eval mode\n",
    "model = DummyModel(cfg).to(DEVICE).eval()\n",
    "\n",
    "# load serialized weights\n",
    "sd = safe_load_state_dict(BEST_CKPT)\n",
    "\n",
    "# enforce an exact key match to catch config/arch mismatches \n",
    "# load_state_dict returns (missing_keys, unexpected_keys) here\n",
    "# you should see nothing in the both brackets\n",
    "missing, unexpected = model.load_state_dict(sd, strict=True)\n",
    "assert not missing and not unexpected, f\"State mismatch — missing: {missing}, unexpected: {unexpected}\"\n",
    "\n",
    "# container for results across percisions\n",
    "results = {}\n",
    "\n",
    "# fast pass (AMP fp16 if cuda, otherwise fall back to fp32)\n",
    "prec = \"amp_fp16\" if DEVICE.type == \"cuda\" else \"fp32\"\n",
    "val_loss_fp16, val_ppl_fp16 = score_model_full_valid(model, full_dl, DEVICE, precision=prec, show_pbar=True)\n",
    "\n",
    "# on CPU, the key will be fp32 here and will be overwritten by gold pass below\n",
    "# but that's fine since both runs are fp32 in that case\n",
    "results[\"amp_fp16\" if DEVICE.type == \"cuda\" else \"fp32\"] = (float(val_loss_fp16), float(val_ppl_fp16))\n",
    "\n",
    "# gold pass (fp32)\n",
    "# set scrict_fp32=True if you want to forbid tf32 on Ampere+\n",
    "# but since i'm running in RTX3060 or RTX3080, i'm gonna set to false (cuz it's fast)\n",
    "val_loss_fp32, val_ppl_fp32 = score_model_full_valid(model, full_dl, DEVICE, precision=\"fp32\", strict_fp32=False, show_pbar=True)\n",
    "results[\"fp32\"] = (float(val_loss_fp32), float(val_ppl_fp32))\n",
    "\n",
    "print(\"\\n[best.pt] full-valid results:\")\n",
    "for k, (L, P) in results.items():\n",
    "    print(f\"  {k:<8} — loss={L:.6f}  ppl={P:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4b01c",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91932ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen best base at: ..\\pretrain_checkpoint\\best_base.pt\n",
      "📝 Wrote: ..\\pretrain_checkpoint\\BEST_BASE_README.md\n"
     ]
    }
   ],
   "source": [
    "# copy original .pt as canonical base (keeps training metadata layout)\n",
    "shutil.copy2(BEST_CKPT, BEST_BASE_PT)\n",
    "\n",
    "# write a tiny README (provenance + both metrics)\n",
    "loss_fp16, ppl_fp16 = results.get(\"amp_fp16\", results[\"fp32\"])\n",
    "loss_fp32, ppl_fp32 = results[\"fp32\"]\n",
    "\n",
    "readme = f\"\"\"# Best Base (Frozen)\n",
    "- Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- Source checkpoint: {BEST_CKPT.name}\n",
    "- Full-valid (amp_fp16): loss={loss_fp16:.6f}, ppl={ppl_fp16:.2f}\n",
    "- Full-valid (fp32)    : loss={loss_fp32:.6f}, ppl={ppl_fp32:.2f}\n",
    "- Seed: {SEED}\n",
    "- Context length: {SEQ_LEN}\n",
    "- Batch size (eval): {BATCH_SIZE}\n",
    "- Notes: Deterministic sliding-window evaluation; amp_fp16 for speed + fp32 for reproducibility.\n",
    "- Artifacts: {'best_base.pt'}\n",
    "\"\"\"\n",
    "with open(README_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(f\"✅ Frozen best base at: {BEST_BASE_PT}\")\n",
    "print(f\"📝 Wrote: {README_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c26c3",
   "metadata": {},
   "source": [
    "# NEVER RUN THIS IF YOU'VE DONE ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superfast: trust best.pt and freeze now (no evaluation)\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "assert BEST_CKPT.is_file(), f\"Missing BEST_CKPT: {BEST_CKPT}\"\n",
    "shutil.copy2(BEST_CKPT, BEST_BASE_PT)\n",
    "\n",
    "readme = f\"\"\"# Best Base (Frozen) — Quick Freeze (no eval)\n",
    "- Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- Source checkpoint: {BEST_CKPT.name}\n",
    "- Notes: Quick dev freeze (no deterministic validation run).\n",
    "- Artifacts: {'best_base.pt'}\n",
    "\"\"\"\n",
    "with open(README_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(f\"✅ Frozen best base at: {BEST_BASE_PT}\")\n",
    "print(f\"📝 Wrote: {README_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
