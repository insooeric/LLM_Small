{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4d1410",
   "metadata": {},
   "source": [
    "# Before we start\n",
    "here are the things we should think about:\n",
    "1. normalization\n",
    "    - we currently have multiples of parquet and datasets\n",
    "    - the thing is they have different shape\n",
    "        - if you take a look at `databricks-dolly-15k.jsonl` and `oasst1_train_1.parquet` for example: \n",
    "        - `databricks-dolly-15k.jsonl`: has `instruction` and `context` pairs for each row\n",
    "        - `oasst1_train_1.parquet`: has `message_id`, `parent_id`, `text`, etc.\n",
    "    - we should normalize those to `user` and `assistant` pairs for each row\n",
    "\n",
    "2. ctx\n",
    "    - the pretrain model uses context size of 512\n",
    "    - meaning, it accepts up to 512 words\n",
    "    - before sft, we will be using template and put those `user` and `assistant` for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf1d2c",
   "metadata": {},
   "source": [
    "# File name restriction\n",
    "later on, when we split by assistant token ratio <br/>\n",
    "WE READ BY `[filename]_train.jsonl` and _`[filename]_valid.jsonl` <br/>\n",
    "so, make sure every jsonl files under `train_jsonl` and `valid_jsonl` folder has according `_train.jsonl` and `_valid.jsonl` syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf81a39",
   "metadata": {},
   "source": [
    "# Reference\n",
    "all of those initial datasets (`jsonl` and `parquet`) are from [huggingface.co](https://huggingface.co/datasets)\n",
    "(except `info_train.jsonl` and `info_valid.jsonl`; I created from ChatGPT)\n",
    "\n",
    "- https://huggingface.co/datasets/allenai/ai2_arc/tree/main\n",
    "    - ARC-Easy\n",
    "        - train-00000-of-00001.parquet\n",
    "        - validation-00000-of-00001.parquet\n",
    "    - ARC-Challenge\n",
    "        - train-00000-of-00001.parquet\n",
    "        - validation-00000-of-00001.parquet\n",
    "\n",
    "- https://huggingface.co/datasets/databricks/databricks-dolly-15k/tree/main\n",
    "    - databricks-dolly-15k.jsonl\n",
    "\n",
    "- https://huggingface.co/datasets/openai/gsm8k/tree/main\n",
    "    - main\n",
    "        - train-00000-of-00001.parquet\n",
    "        - test-00000-of-00001.parquet\n",
    "\n",
    "- https://huggingface.co/datasets/EleutherAI/hendrycks_math/tree/main\n",
    "    - algebra\n",
    "        - train-00000-of-00001.parquet\n",
    "        - test-00000-of-00001.parquet\n",
    "    - counting_and_probability\n",
    "        - train-00000-of-00001.parquet\n",
    "        - test-00000-of-00001.parquet\n",
    "    - intermediate_algebra\n",
    "        - train-00000-of-00001.parquet\n",
    "        - test-00000-of-00001.parquet\n",
    "    - number_theory\n",
    "        - train-00000-of-00001.parquet\n",
    "        - test-00000-of-00001.parquet\n",
    "\n",
    "- https://huggingface.co/datasets/AI-MO/NuminaMath-CoT/tree/main\n",
    "    - data\n",
    "        - train-00000-of-00005.parquet\n",
    "        - train-00001-of-00005.parquet\n",
    "        - train-00002-of-00005.parquet\n",
    "        - train-00003-of-00005.parquet\n",
    "        - train-00004-of-00005.parquet\n",
    "        - test-00000-of-00001.parquet\n",
    "\n",
    "- https://huggingface.co/datasets/OpenAssistant/oasst1/tree/main\n",
    "    - data\n",
    "        - train-00000-of-00001-b42a775f407cee45.parquet\n",
    "        - validation-00000-of-00001-134b8fd0c89408b6.parquet\n",
    "\n",
    "- https://huggingface.co/datasets/Open-Orca/OpenOrca/tree/main\n",
    "    - 1M-GPT4-Augmented.parquet\n",
    "\n",
    "- https://huggingface.co/datasets/allenai/sciq/tree/main\n",
    "    - data\n",
    "        - train-00000-of-00001.parquet\n",
    "        - validation-00000-of-00001.parquet\n",
    "\n",
    "- https://huggingface.co/datasets/ChilleD/SVAMP/tree/main\n",
    "    - train.json\n",
    "    - test.json\n",
    "    \n",
    "- https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/tree/main\n",
    "    - data\n",
    "        - train_sft-00000-of-00003-a3ecf92756993583.parquet\n",
    "        - train_sft-00001-of-00003-0a1804bcb6ae68c6.parquet\n",
    "        - train_sft-00002-of-00003-ee46ed25cfae92c6.parquet\n",
    "        - test_sft-00000-of-00001-f7dfac4afe5b93f4.parquet"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
