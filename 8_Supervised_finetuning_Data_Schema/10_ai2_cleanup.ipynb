{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb46e00",
   "metadata": {},
   "source": [
    "# Cleanup AI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "from typing import Any, Dict, Iterable, Tuple, List\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    HAVE_PA = True\n",
    "except Exception:\n",
    "    HAVE_PA = False\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    HAVE_PD = True\n",
    "except Exception:\n",
    "    HAVE_PD = False\n",
    "\n",
    "BASE = Path(\"ai2\")\n",
    "TRAIN_GLOB = \"ai2_train_*.parquet\"\n",
    "VALID_GLOB = \"ai2_valid_*.parquet\"\n",
    "\n",
    "OUT_TRAIN = Path(\"./train_jsonl/ai2_train.jsonl\")\n",
    "OUT_VALID = Path(\"./valid_jsonl/ai2_valid.jsonl\")\n",
    "\n",
    "BATCH_SIZE = 8192\n",
    "DO_DEDUP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_clean_str(x: Any) -> str:\n",
    "    return str(x).strip()\n",
    "\n",
    "# parse answerKey\n",
    "# returns (status, label) where status ∈ {\"ok\",\"multi\",\"missing\",\"bad\"}\n",
    "# - \"ok\": single label like \"A\"..\"Z\" (case-insensitive)\n",
    "# - \"multi\": multiple labels present (e.g., \"A,B\" or [\"A\",\"B\"])\n",
    "# - \"missing\": None or empty\n",
    "# - \"bad\": unparseable\n",
    "def parse_answer_key(raw) -> Tuple[str, str | None]:\n",
    "    if raw is None:\n",
    "        return \"missing\", None\n",
    "\n",
    "    # list/tuple/set -> check cardinality\n",
    "    if isinstance(raw, (list, tuple, set)):\n",
    "        vals = [to_clean_str(x).upper() for x in raw if to_clean_str(x)]\n",
    "        labs = [re.match(r\"^[A-Z]$\", v).group(0) for v in vals if re.match(r\"^[A-Z]$\", v)]\n",
    "        uniq = sorted(set(labs))\n",
    "        if len(uniq) == 0:\n",
    "            return \"bad\", None\n",
    "        if len(uniq) > 1:\n",
    "            return \"multi\", None\n",
    "        return \"ok\", uniq[0]\n",
    "\n",
    "    s = to_clean_str(raw).upper()\n",
    "    if not s:\n",
    "        return \"missing\", None\n",
    "\n",
    "    # extract letter tokens; treat more than one as multi\n",
    "    letters = re.findall(r\"[A-Z]\", s)\n",
    "    uniq = sorted(set(letters))\n",
    "    if len(uniq) == 0:\n",
    "        return \"bad\", None\n",
    "    if len(uniq) > 1:\n",
    "        return \"multi\", None\n",
    "    return \"ok\", uniq[0]\n",
    "\n",
    "\n",
    "def extract_pair_strict(obj: Dict[str, Any]) -> Tuple[str, str] | Tuple[None, None, Dict[str,int]]:\n",
    "    counters = {\"missing_fields\": 0, \"bad_choices\": 0, \"num_multi\": 0, \"key_not_found\": 0}\n",
    "\n",
    "    if not all(k in obj for k in (\"question\", \"choices\", \"answerKey\")):\n",
    "        counters[\"missing_fields\"] += 1\n",
    "        return None, None, counters\n",
    "\n",
    "    q = to_clean_str(obj[\"question\"])\n",
    "    if not q:\n",
    "        counters[\"missing_fields\"] += 1\n",
    "        return None, None, counters\n",
    "\n",
    "    choices = obj.get(\"choices\", {})\n",
    "    labels = choices.get(\"label\")\n",
    "    texts  = choices.get(\"text\")\n",
    "    if not isinstance(labels, list) or not isinstance(texts, list) or len(labels) != len(texts) or len(labels) == 0:\n",
    "        counters[\"bad_choices\"] += 1\n",
    "        return None, None, counters\n",
    "\n",
    "    mapping = {}\n",
    "    for lab, txt in zip(labels, texts):\n",
    "        lab = to_clean_str(lab).upper()\n",
    "        txt = to_clean_str(txt)\n",
    "        if not re.match(r\"^[A-Z]$\", lab) or not txt:\n",
    "            counters[\"bad_choices\"] += 1\n",
    "            return None, None, counters\n",
    "        if lab in mapping:\n",
    "            # duplicate label -> treat as bad to avoid ambiguity\n",
    "            counters[\"bad_choices\"] += 1\n",
    "            return None, None, counters\n",
    "        mapping[lab] = txt\n",
    "\n",
    "    status, key = parse_answer_key(obj[\"answerKey\"])\n",
    "    if status == \"multi\":\n",
    "        counters[\"num_multi\"] += 1\n",
    "        return None, None, counters\n",
    "    if status != \"ok\" or key not in mapping:\n",
    "        counters[\"key_not_found\"] += 1\n",
    "        return None, None, counters\n",
    "\n",
    "    user = q\n",
    "    assistant = mapping[key]\n",
    "    return user, assistant, counters\n",
    "\n",
    "def build_jsonl_from_parquets(in_paths: List[Path], out_path: Path) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Convert AI2 parquet parts → normalized JSONL with progress bars.\n",
    "    STRICT as per extract_pair_strict().\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    total_rows = None\n",
    "    if HAVE_PA:\n",
    "        try:\n",
    "            total_rows = sum(pq.ParquetFile(p).metadata.num_rows for p in in_paths)\n",
    "        except Exception:\n",
    "            total_rows = None\n",
    "\n",
    "    kept = deduped = total_seen = 0\n",
    "    c_missing = c_bad_choices = c_multi = c_key_missing = 0\n",
    "\n",
    "    seen = set() if DO_DEDUP else None\n",
    "\n",
    "    desc = f\"writing {out_path.name}\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f, tqdm(total=total_rows, unit=\"rows\", desc=desc, leave=False) as pbar:\n",
    "\n",
    "        if HAVE_PA:\n",
    "            for p in in_paths:\n",
    "                pf = pq.ParquetFile(p)\n",
    "                for batch in pf.iter_batches(batch_size=BATCH_SIZE):\n",
    "                    rows = pa.Table.from_batches([batch]).to_pylist()\n",
    "                    for rec in rows:\n",
    "                        total_seen += 1\n",
    "                        res = extract_pair_strict(rec)\n",
    "                        if res[0] is None:\n",
    "                            # accumulate deltas\n",
    "                            d = res[2]\n",
    "                            c_missing     += d.get(\"missing_fields\", 0)\n",
    "                            c_bad_choices += d.get(\"bad_choices\", 0)\n",
    "                            c_multi       += d.get(\"num_multi\", 0)\n",
    "                            c_key_missing += d.get(\"key_not_found\", 0)\n",
    "                        else:\n",
    "                            user, assistant, _ = res\n",
    "                            if DO_DEDUP:\n",
    "                                h = (user, assistant)\n",
    "                                if h in seen:\n",
    "                                    deduped += 1\n",
    "                                    continue\n",
    "                                seen.add(h)\n",
    "                            f.write(json.dumps({\"user\": user, \"assistant\": assistant}, ensure_ascii=False) + \"\\n\")\n",
    "                            kept += 1\n",
    "                    pbar.update(len(rows))\n",
    "                    pbar.set_postfix(written=kept, deduped=deduped, missing=c_missing,\n",
    "                                     bad_choices=c_bad_choices, multi=c_multi, key_miss=c_key_missing)\n",
    "        elif HAVE_PD:\n",
    "            for p in in_paths:\n",
    "                df = pd.read_parquet(p)\n",
    "                recs = df.to_dict(orient=\"records\")\n",
    "                for rec in tqdm(recs, unit=\"rows\", desc=desc, leave=False):\n",
    "                    total_seen += 1\n",
    "                    res = extract_pair_strict(rec)\n",
    "                    if res[0] is None:\n",
    "                        d = res[2]\n",
    "                        c_missing     += d.get(\"missing_fields\", 0)\n",
    "                        c_bad_choices += d.get(\"bad_choices\", 0)\n",
    "                        c_multi       += d.get(\"num_multi\", 0)\n",
    "                        c_key_missing += d.get(\"key_not_found\", 0)\n",
    "                    else:\n",
    "                        user, assistant, _ = res\n",
    "                        if DO_DEDUP:\n",
    "                            h = (user, assistant)\n",
    "                            if h in seen:\n",
    "                                deduped += 1\n",
    "                                continue\n",
    "                            seen.add(h)\n",
    "                        f.write(json.dumps({\"user\": user, \"assistant\": assistant}, ensure_ascii=False) + \"\\n\")\n",
    "                        kept += 1\n",
    "        else:\n",
    "            raise RuntimeError(\"Neither pyarrow nor pandas is available to read parquet.\")\n",
    "\n",
    "    return {\n",
    "        \"total_rows_in_files\": int(total_rows) if total_rows is not None else total_seen,\n",
    "        \"seen_rows\": total_seen,\n",
    "        \"written\": kept,\n",
    "        \"deduped\": deduped,\n",
    "        \"dropped_missing_fields\": c_missing,\n",
    "        \"dropped_bad_choices\": c_bad_choices,\n",
    "        \"num_multi\": c_multi,\n",
    "        \"key_not_found_or_bad\": c_key_missing,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train parts:\n",
      " - ai2\\ai2_train_1.parquet\n",
      " - ai2\\ai2_train_2.parquet\n",
      "\n",
      "Valid parts:\n",
      " - ai2\\ai2_valid_1.parquet\n",
      " - ai2\\ai2_valid_2.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06264a4b86d644d4a2f27c821d7c9c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "writing ai2_train.jsonl:   0%|          | 0/3370 [00:00<?, ?rows/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305fbf2d61e7431f853a9d3ee352f71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "writing ai2_valid.jsonl:   0%|          | 0/869 [00:00<?, ?rows/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Done ---\n",
      "Train: {'total_rows_in_files': 3370, 'seen_rows': 3370, 'written': 3244, 'deduped': 1, 'dropped_missing_fields': 0, 'dropped_bad_choices': 125, 'num_multi': 0, 'key_not_found_or_bad': 0}\n",
      "Valid: {'total_rows_in_files': 869, 'seen_rows': 869, 'written': 844, 'deduped': 0, 'dropped_missing_fields': 0, 'dropped_bad_choices': 25, 'num_multi': 0, 'key_not_found_or_bad': 0}\n"
     ]
    }
   ],
   "source": [
    "train_parts = sorted(BASE.glob(TRAIN_GLOB), key=lambda p: p.name)\n",
    "valid_parts = sorted(BASE.glob(VALID_GLOB), key=lambda p: p.name)\n",
    "\n",
    "print(\"Train parts:\")\n",
    "for p in train_parts: print(\" -\", p)\n",
    "print(\"\\nValid parts:\")\n",
    "for p in valid_parts: print(\" -\", p)\n",
    "\n",
    "train_stats = build_jsonl_from_parquets(train_parts, OUT_TRAIN)\n",
    "valid_stats = build_jsonl_from_parquets(valid_parts, OUT_VALID)\n",
    "\n",
    "print(\"\\n--- Done ---\")\n",
    "print(\"Train:\", train_stats)\n",
    "print(\"Valid:\", valid_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b1b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ai2_train.jsonl (first 3) ---\n",
      "{\"user\": \"Which factor will most likely cause a person to develop a fever?\", \"assistant\": \"a bacterial population in the bloodstream\"}\n",
      "{\"user\": \"Lichens are symbiotic organisms made of green algae and fungi. What do the green algae supply to the fungi in this symbiotic relationship?\", \"assistant\": \"food\"}\n",
      "{\"user\": \"When a switch is used in an electrical circuit, the switch can\", \"assistant\": \"stop and start the flow of current.\"}\n",
      "\n",
      "--- ai2_valid.jsonl (first 3) ---\n",
      "{\"user\": \"Which technology was developed most recently?\", \"assistant\": \"cellular telephone\"}\n",
      "{\"user\": \"A student hypothesizes that algae are producers. Which question will best help the student determine if this is correct?\", \"assistant\": \"Do algae use sunlight to make food?\"}\n",
      "{\"user\": \"Soccer players use their muscle systems to kick a ball into a goal. What organ system coordinates the muscles?\", \"assistant\": \"The nervous system\"}\n"
     ]
    }
   ],
   "source": [
    "def peek_jsonl(path: Path, k: int = 3):\n",
    "    print(f\"\\n--- {path.name} (first {k}) ---\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, ln in enumerate(f):\n",
    "            if i >= k: break\n",
    "            print(ln.rstrip())\n",
    "\n",
    "peek_jsonl(OUT_TRAIN, 3)\n",
    "peek_jsonl(OUT_VALID, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fbd9ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d022915d86744407be4552eb3e13cd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scan ai2_train.jsonl: 0lines [00:00, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai2_train.jsonl: scanned 3244 | bad_json=0 nonstring=0 short_user(<5)=0 short_asst(<1)=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae263799b524e6c8b92e3b9dfd73965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scan ai2_valid.jsonl: 0lines [00:00, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai2_valid.jsonl: scanned 844 | bad_json=0 nonstring=0 short_user(<5)=0 short_asst(<1)=0\n"
     ]
    }
   ],
   "source": [
    "def sanity_check_jsonl(path: Path, max_lines: int | None = None):\n",
    "    n = bad_json = nonstring = short_user = short_asst = 0\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, ln in enumerate(tqdm(f, unit=\"lines\", desc=f\"scan {path.name}\", leave=False)):\n",
    "            if (max_lines is not None) and (i >= max_lines): break\n",
    "            try:\n",
    "                obj = json.loads(ln)\n",
    "            except Exception:\n",
    "                bad_json += 1; continue\n",
    "            u, a = obj.get(\"user\",\"\"), obj.get(\"assistant\",\"\")\n",
    "            if not isinstance(u, str) or not isinstance(a, str):\n",
    "                nonstring += 1; continue\n",
    "            if len(u) < 5: short_user += 1\n",
    "            if len(a) < 1: short_asst += 1\n",
    "            n += 1\n",
    "    print(f\"{path.name}: scanned {n} | bad_json={bad_json} nonstring={nonstring} short_user(<5)={short_user} short_asst(<1)={short_asst}\")\n",
    "\n",
    "sanity_check_jsonl(OUT_TRAIN)\n",
    "sanity_check_jsonl(OUT_VALID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
