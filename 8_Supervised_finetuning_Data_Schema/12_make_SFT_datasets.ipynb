{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88af29f1",
   "metadata": {},
   "source": [
    "# Configure tokenizers & stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de621a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[decode] present 9/9\n",
      "[prompt] present 2/2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re, numpy as np\n",
    "from tokenizers import ByteLevelBPETokenizer, AddedToken\n",
    "\n",
    "VOCAB  = Path(\"../bpe/bpe_model-vocab.json\")\n",
    "MERGES = Path(\"../bpe/bpe_model-merges.txt\")\n",
    "\n",
    "MAX_LEN          = 512\n",
    "PAD_ID_EXPECTED  = 0 # must match tokenizer.json\n",
    "END_ID_EXPECTED  = 3 # must match tokenizer.json\n",
    "\n",
    "# here's the thing\n",
    "# if you look at `tokenizer_test.ipynb`, you'll that whole system prompt is 87 tokens\n",
    "# so, we need to reserve at least that many tokens for the system prompt\n",
    "# i just set it to 90 (+3 for safety)\n",
    "SYSTEM_RESERVE   = 90   # tokens reserved for system\n",
    "# also, if you look at `checking_info.ipynb`, you'll see that the longest one has 87 tokens\n",
    "# so, we'll reserve 70 tokens for info\n",
    "INFO_RESERVE     = 70   # tokens reserved for info\n",
    "# that leaves us with 512 - 90 - 70 = 352 tokens for the user + assistant\n",
    "\n",
    "# we want to ensure that the assistant always has at least 48 tokens\n",
    "MIN_ASSISTANT    = 48   # hard minimum tokens for assistant span\n",
    "\n",
    "# build the two tokenizer *views*:\n",
    "# - bpe_decode: for decoding + recognizing specials\n",
    "# - bpe_prompt: for encoding prompts (ONLY PAD and END are special)\n",
    "bpe_decode = ByteLevelBPETokenizer(str(VOCAB), str(MERGES), lowercase=False, add_prefix_space=True)\n",
    "bpe_prompt = ByteLevelBPETokenizer(str(VOCAB), str(MERGES), lowercase=False, add_prefix_space=True)\n",
    "\n",
    "# to verify we do have those special tokens\n",
    "SPECIALS_DECODE = [\n",
    "    \"<|PAD|>\", \"<|START|>\", \"<|END|>\", \"<|SYSTEM|>\",\n",
    "    \"<|INFOSTART|>\", \"<|INFOEND|>\", \"<|USER|>\", \"<|ASSISTANT|>\", \"<|UNKNOWN|>\"\n",
    "]\n",
    "SPECIALS_PROMPT = [\"<|PAD|>\", \"<|END|>\"]\n",
    "\n",
    "def report_specials(tok, specials, name):\n",
    "    ids = {t: tok.token_to_id(t) for t in specials}\n",
    "    missing = [t for t, i in ids.items() if i is None]\n",
    "    print(f\"[{name}] present {len(specials)-len(missing)}/{len(specials)}\")\n",
    "    if missing:\n",
    "        print(\"  missing:\", missing)\n",
    "    return ids, missing\n",
    "\n",
    "SID, missing_decode = report_specials(bpe_decode, SPECIALS_DECODE, \"decode\")\n",
    "SPID, missing_prompt = report_specials(bpe_prompt, SPECIALS_PROMPT, \"prompt\")\n",
    "\n",
    "assert not missing_decode, f\"Decode tokenizer missing specials: {missing_decode}\"\n",
    "assert not missing_prompt, f\"Prompt tokenizer missing specials: {missing_prompt}\"\n",
    "\n",
    "PAD_ID  = SID[\"<|PAD|>\"]; END_ID = SID[\"<|END|>\"]\n",
    "assert PAD_ID == PAD_ID_EXPECTED and END_ID == END_ID_EXPECTED, (\n",
    "    f\"PAD/END ids mismatch: PAD={PAD_ID} vs {PAD_ID_EXPECTED}, END={END_ID} vs {END_ID_EXPECTED}\"\n",
    ")\n",
    "\n",
    "# helpers\n",
    "def tok_len(txt: str) -> int:\n",
    "    return len(bpe_prompt.encode(txt, add_special_tokens=False).ids)\n",
    "\n",
    "def enc(txt: str):\n",
    "    return bpe_prompt.encode(txt, add_special_tokens=False).ids\n",
    "\n",
    "def dec(ids):\n",
    "    return bpe_decode.decode([int(t) for t in ids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38350f7",
   "metadata": {},
   "source": [
    "# For TQDM STUFFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5aa8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress bar setup (put once near the top)\n",
    "from tqdm import tqdm as TQDM\n",
    "if hasattr(TQDM, \"monitor_interval\"):\n",
    "    TQDM.monitor_interval = 0 # no background monitor thread\n",
    "\n",
    "TQDM_KW = dict(dynamic_ncols=True)\n",
    "BARFMT  = \"{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b07b6",
   "metadata": {},
   "source": [
    "# System prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e3a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ok. that's the template we'll use for sft training (we'll use the same by the time we generate output)\n",
    "# tmi: that's the one with 87 tokens\n",
    "SYSTEM_PROMPT_DEFAULT = (\n",
    "    \"Be a helpful, concise assistant with a light, friendly tone. \"\n",
    "    \"Answer directly in 1–3 sentences. Don’t use steps or bullet lists unless the user asks. \"\n",
    "    \"Use the content between INFOSTART/INFOEND only as context. Do not mention it, ‘memory,’ or any internal tags. \"\n",
    "    \"Avoid speculation and say when unsure. Keep replies safe, accurate, and on-topic.\"\n",
    ")\n",
    "\n",
    "# later on, we'll look at an SFT ids file to detect whether training used a leading space/newline after <|ASSISTANT|>\n",
    "SFT_IDS = Path(\"../final_npy/train_input_ids.npy\")\n",
    "def detect_assist_prefix(ids_path: Path, sample_k: int = 256) -> str:\n",
    "    if not ids_path.exists():\n",
    "        return \" \"\n",
    "    arr = np.load(ids_path, mmap_mode=\"r\")\n",
    "    spaces = newlines = letters = 0\n",
    "    for i in range(min(sample_k, arr.shape[0])):\n",
    "        row = [int(t) for t in arr[i] if int(t) != PAD_ID]\n",
    "        txt = bpe_decode.decode(row)\n",
    "        j = txt.find(\"<|ASSISTANT|>\")\n",
    "        if j == -1: \n",
    "            continue\n",
    "        ch = txt[j+len(\"<|ASSISTANT|>\"): j+len(\"<|ASSISTANT|>\")+1]\n",
    "        if ch == \" \":      spaces += 1\n",
    "        elif ch == \"\\n\":   newlines += 1\n",
    "        elif ch:           letters += 1\n",
    "    if spaces >= max(newlines, letters): return \" \"\n",
    "    if newlines > 0: return \"\\n\"\n",
    "    return \" \"\n",
    "\n",
    "ASSIST_PREFIX = detect_assist_prefix(SFT_IDS)  # default to a single space if unknown\n",
    "\n",
    "def build_template_text(system_text: str, info_text: str, user_text: str) -> str:\n",
    "    return (\n",
    "        \"<|START|><|SYSTEM|>\" + system_text + \"\\n\"\n",
    "        \"<|INFOSTART|>\" + (info_text or \"\") + \"<|INFOEND|>\\n\"\n",
    "        \"<|USER|>\" + user_text.strip() + \"\\n\"\n",
    "        \"<|ASSISTANT|>\" + ASSIST_PREFIX\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1f264",
   "metadata": {},
   "source": [
    "# Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eccb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "\n",
    "RUT_PAT = re.compile(r\"(?i)\\bstep\\s*\\d+|identify the (given )?sentence|the sentence is\")\n",
    "\n",
    "# returns available budgets (token counts) for user and assistant when packing to MAX_LEN\n",
    "# strategy:\n",
    "#   1) Reserve SYSTEM_RESERVE and INFO_RESERVE (hard caps)\n",
    "#   2) Build the static template and measure its tokenized length (without user/assistant)\n",
    "#   3) Leave 1 token for END\n",
    "#   4) Enforce MIN_ASSISTANT\n",
    "#   5) Allocate remaining to user\n",
    "def compute_budgets(system_text: str, info_text: str, user_text: str) -> dict:\n",
    "    # clamp system/info to reserves (skip if they exceed 512 tokens total)\n",
    "    sys_ids  = enc((system_text or \"\")[:])[:SYSTEM_RESERVE]\n",
    "    info_ids = enc((info_text  or \"\")[:])[:INFO_RESERVE]\n",
    "\n",
    "    # template skeleton around user/assistant (we’ll measure tokens contributed by tags + SYSTEM/INFO shells)\n",
    "    # for length, we must recompute using the same tokenizer behavior used in packing\n",
    "    # build a \"header\" with SYSTEM/INFO lines and USER/ASSISTANT tags but with EMPTY user/assistant bodies\n",
    "    header_txt = (\n",
    "        \"<|START|><|SYSTEM|>\" + bpe_decode.decode(sys_ids) + \"\\n\"\n",
    "        \"<|INFOSTART|>\" + bpe_decode.decode(info_ids) + \"<|INFOEND|>\\n\"\n",
    "        \"<|USER|>\" + \"\" + \"\\n\"\n",
    "        \"<|ASSISTANT|>\" + ASSIST_PREFIX\n",
    "    )\n",
    "    header_len = tok_len(header_txt)\n",
    "\n",
    "    # total tokens available for user_text + assistant_text + END\n",
    "    remain = MAX_LEN - header_len - 1  # save 1 for END token\n",
    "\n",
    "    # we recently set MIN_ASSISTANT to 48\n",
    "    # so, it will ignore user if the template + reserves already nearly fills the row\n",
    "    # i.e. if remain < MIN_ASSISTANT, then user_budget=0 and assistant_budget=remain\n",
    "    if remain < MIN_ASSISTANT:\n",
    "        return dict(user_budget=0, assistant_budget=max(0, remain))\n",
    "    \n",
    "    # guarantee assistant minimum\n",
    "    user_budget = max(0, remain - MIN_ASSISTANT)\n",
    "    assistant_budget = MIN_ASSISTANT\n",
    "    return dict(user_budget=user_budget, assistant_budget=assistant_budget)\n",
    "\n",
    "def truncate_to_budget(text: str, budget: int) -> list[int]:\n",
    "    if budget <= 0:\n",
    "        return []\n",
    "    # token level truncation preserves leading spaces behavior\n",
    "    ids = enc(text)\n",
    "    return ids[:budget]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f23e2",
   "metadata": {},
   "source": [
    "# Row & Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af29c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packing row. WE ARE NOT TRUNCATING. skip if it wouldn't fit 512\n",
    "def strip_any_end(text: str) -> str:\n",
    "    return re.sub(r\"<\\|END\\|>\\s*$\", \"\", text)\n",
    "\n",
    "def row_to_ids_and_mask(system_text: str, info_text: str, user_text: str, assistant_text: str):\n",
    "    # cleanup + scaffold filter\n",
    "    assistant_text = strip_any_end(assistant_text or \"\")\n",
    "    if RUT_PAT.search(assistant_text):\n",
    "        return None\n",
    "\n",
    "    # tokenize FULL fields (no slicing!) to measure true lengths\n",
    "    sys_ids_full   = enc(system_text or SYSTEM_PROMPT_DEFAULT)\n",
    "    info_ids_full  = enc(info_text  or \"\")\n",
    "    user_ids_full  = enc(user_text  or \"\")\n",
    "    asst_ids_full  = enc(assistant_text or \"\")\n",
    "\n",
    "    # enforce fixed reserves for system/info (SKIP if they exceed reserves)\n",
    "    if len(sys_ids_full) > SYSTEM_RESERVE:\n",
    "        return None\n",
    "    if len(info_ids_full) > INFO_RESERVE:\n",
    "        return None\n",
    "\n",
    "    sys_ids  = sys_ids_full\n",
    "    info_ids = info_ids_full\n",
    "\n",
    "    # compute header length with EXACT same tokenizer behavior\n",
    "    header_txt = (\n",
    "        \"<|START|><|SYSTEM|>\" + bpe_decode.decode(sys_ids) + \"\\n\"\n",
    "        \"<|INFOSTART|>\" + bpe_decode.decode(info_ids) + \"<|INFOEND|>\\n\"\n",
    "        \"<|USER|>\" + bpe_decode.decode(user_ids_full) + \"\\n\"\n",
    "        \"<|ASSISTANT|>\" + ASSIST_PREFIX\n",
    "    )\n",
    "    header_len = tok_len(header_txt)\n",
    "\n",
    "    # total remaining budget for assistant + END\n",
    "    remain = MAX_LEN - header_len - 1  # keep 1 for END\n",
    "\n",
    "    # must leave room for at least MIN_ASSISTANT (strict)\n",
    "    if remain < MIN_ASSISTANT:\n",
    "        return None\n",
    "\n",
    "    # strict budgets: user must already fit; assistant must fit into remain\n",
    "    # we already built header with full user_ids, so the check is simply on asst length.\n",
    "    if len(asst_ids_full) > remain:\n",
    "        return None\n",
    "\n",
    "    # build final sequence\n",
    "    ids = []\n",
    "    ids += enc(\"<|START|><|SYSTEM|>\") + sys_ids + enc(\"\\n\")\n",
    "    ids += enc(\"<|INFOSTART|>\") + info_ids + enc(\"<|INFOEND|>\\n\")\n",
    "    ids += enc(\"<|USER|>\") + user_ids_full + enc(\"\\n\")\n",
    "    ids += enc(\"<|ASSISTANT|>\") + enc(ASSIST_PREFIX)\n",
    "    ids += asst_ids_full\n",
    "    ids += [END_ID]\n",
    "\n",
    "    # safety: must fit exactly within 512 (pad if shorter, drop if longer)\n",
    "    if len(ids) > MAX_LEN:\n",
    "        return None\n",
    "    pad_len = MAX_LEN - len(ids)\n",
    "    mask = np.zeros(len(ids) + pad_len, dtype=np.uint8)\n",
    "\n",
    "    # mask from first assistant token (after tag+prefix) through END (inclusive)\n",
    "    prefix_txt = (\n",
    "        \"<|START|><|SYSTEM|>\" + bpe_decode.decode(sys_ids) + \"\\n\"\n",
    "        \"<|INFOSTART|>\" + bpe_decode.decode(info_ids) + \"<|INFOEND|>\\n\"\n",
    "        \"<|USER|>\" + bpe_decode.decode(user_ids_full) + \"\\n\"\n",
    "        \"<|ASSISTANT|>\" + ASSIST_PREFIX\n",
    "    )\n",
    "    start_mask = len(enc(prefix_txt))\n",
    "    end_mask   = len(ids) - 1  # END index\n",
    "\n",
    "    mask[:len(ids)] = 0\n",
    "    mask[start_mask:end_mask+1] = 1\n",
    "\n",
    "    if pad_len:\n",
    "        ids  = ids + [PAD_ID]*pad_len  # right pad to 512\n",
    "\n",
    "    # stats\n",
    "    asst_len = end_mask - start_mask  # not counting END\n",
    "    stats = dict(\n",
    "        total=len(ids),\n",
    "        user_len=len(user_ids_full),\n",
    "        assistant_len=asst_len,\n",
    "        includes_END=True,\n",
    "        truncated=False,\n",
    "    )\n",
    "    return np.array(ids, dtype=np.int32), mask.astype(np.uint8), stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3fea0f",
   "metadata": {},
   "source": [
    "# Audit NPY files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c74b4",
   "metadata": {},
   "source": [
    "this is for sanity check <br/>\n",
    "around the end, by the time we do sanity check, we'll use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed2b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_npys(ids_path: Path, mask_path: Path, sample_k: int = 5000):\n",
    "    ids = np.load(ids_path, mmap_mode=\"r\")\n",
    "    msk = np.load(mask_path, mmap_mode=\"r\")\n",
    "    N, T = ids.shape\n",
    "    take = min(sample_k, N)\n",
    "\n",
    "    bad_order = bad_multi_end = bad_mask_after = 0\n",
    "    boundary_mismatch = 0\n",
    "    short_asst = rut_hits = 0\n",
    "\n",
    "    rng = TQDM(range(take), desc=f\"[AUDIT*] {ids_path.name}\", total=take, bar_format=BARFMT, **TQDM_KW)\n",
    "    for i in rng:\n",
    "        row_tok = ids[i]\n",
    "        row_msk = msk[i].astype(bool)\n",
    "\n",
    "        # quick END checks\n",
    "        nonpad = (row_tok != PAD_ID)\n",
    "        last_idx = T - 1 - np.argmax(nonpad[::-1])\n",
    "        end_count = int((row_tok == END_ID).sum())\n",
    "        if end_count != 1: bad_multi_end += 1\n",
    "        if row_tok[last_idx] != END_ID: bad_order += 1\n",
    "\n",
    "        # no mask after END\n",
    "        if row_msk[last_idx+1:].any(): bad_mask_after += 1\n",
    "\n",
    "        # assistant start from mask (first 1)\n",
    "        if not row_msk.any(): continue # should not happen\n",
    "        a_tok = int(np.argmax(row_msk)) # first supervised index\n",
    "        e_tok = last_idx\n",
    "\n",
    "        # semantic boundary check: ensure decoded text around a_tok shows we're right after the tag+prefix\n",
    "        # decode a small window around the suspected boundary\n",
    "        lo = max(0, a_tok - 32); hi = min(T, a_tok + 32)\n",
    "        window = bpe_decode.decode([int(t) for t in row_tok[lo:hi] if int(t) != PAD_ID])\n",
    "\n",
    "        # we expect \"...<|ASSISTANT|>{ASSIST_PREFIX}<first assistant chars>...\"\n",
    "        expect = \"<|ASSISTANT|>\" + ASSIST_PREFIX\n",
    "        ok = expect in window\n",
    "        if not ok: boundary_mismatch += 1\n",
    "\n",
    "        # ruts / short length\n",
    "        asst_len = (e_tok - a_tok)\n",
    "        short_asst += int(asst_len <= 3)\n",
    "        if RUT_PAT.search(window): rut_hits += 1\n",
    "\n",
    "        if i and i % 5000 == 0:\n",
    "            rng.set_postfix_str(f\"boundary_mismatch={boundary_mismatch}, short<=3={short_asst}, ruts={rut_hits}\")\n",
    "\n",
    "    print(f\"[AUDIT*] rows checked={take}/{N}\")\n",
    "    print(f\"  bad_order={bad_order}  multi_or_zero_END={bad_multi_end}  mask_after_END={bad_mask_after}\")\n",
    "    print(f\"  boundary_mismatch (tag+prefix not visible near start) = {boundary_mismatch}\")\n",
    "    print(f\"  assistant_len<=3: {short_asst} ({short_asst/take:.2%}), rut_hits: {rut_hits} ({rut_hits/take:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd563fd2",
   "metadata": {},
   "source": [
    "# Each JSONL to NPY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ad56d",
   "metadata": {},
   "source": [
    "we're going to do the following: <br/>\n",
    "1. convert each jsonl to npy\n",
    "2. strip those npy by ratio\n",
    "    - chat:math:science = 7:2:1 (+ entire info datasets)\n",
    "    - those ratio are for assistant tokens that are being masked\n",
    "    - we split by assistant tokens since that's the crutial one since our current babbling model will be tuned by learning those assistant tokens\n",
    "3. concatinate those npy files and masked files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d3911",
   "metadata": {},
   "source": [
    "# Convert each JSONL to NPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each JSONL to its own NPY pair (input_ids + loss_mask)\n",
    "# with strict filtering (skip rows that don't fit exactly)\n",
    "from pathlib import Path\n",
    "import json, numpy as np, re, os\n",
    "try:\n",
    "    from tqdm.auto import tqdm as TQDM\n",
    "except Exception:\n",
    "    from tqdm import tqdm as TQDM\n",
    "\n",
    "def iter_lines_hybrid(p: Path, desc=\"READ\"):\n",
    "    total_bytes = os.stat(p).st_size\n",
    "    line_count = 0\n",
    "    with p.open(\"rb\") as fbin, TQDM(total=total_bytes, unit=\"B\", unit_scale=True,\n",
    "                                    desc=f\"[{desc}] {p.name}\", leave=False,\n",
    "                                    bar_format=BARFMT, **TQDM_KW) as t:\n",
    "        for raw in fbin:\n",
    "            t.update(len(raw))\n",
    "            line = raw.decode(\"utf-8\", \"ignore\")\n",
    "            if line.strip():\n",
    "                line_count += 1\n",
    "                if line_count % 1000 == 0:\n",
    "                    t.set_postfix_str(f\"lines={line_count:,}\")\n",
    "            yield line\n",
    "\n",
    "def _label_from_filename(path: Path) -> str:\n",
    "    name = path.name.lower()\n",
    "    # pull dataset key before _train/_valid\n",
    "    m = re.match(r\"^(.*)_(train|valid)(?:\\.\\w+)?$\", name)\n",
    "    key = m.group(1) if m else path.stem.lower()\n",
    "\n",
    "    # explicit keys\n",
    "    if key == \"info\" or \"info\" in key:\n",
    "        return \"info\"\n",
    "\n",
    "    # chat datasets\n",
    "    if key in {\"ultrachat\", \"openorca\", \"dolly\", \"oasst1\"}:\n",
    "        return \"chat\"\n",
    "\n",
    "    # math datasets\n",
    "    if key in {\"gsm8k\", \"hendrycks_math\", \"numinamath\", \"svamp\"}:\n",
    "        return \"math\"\n",
    "\n",
    "    # science datasets\n",
    "    if key in {\"sciq\", \"ai2\"}:\n",
    "        return \"science\"\n",
    "\n",
    "    # heuristic fallbacks (keep your originals)\n",
    "    if \"chat\" in name:    return \"chat\"\n",
    "    if \"math\" in name:    return \"math\"\n",
    "    if \"science\" in name: return \"science\"\n",
    "    if re.search(r\"\\b(science|physics|chem|bio)\\b\", name): return \"science\"\n",
    "    if re.search(r\"\\b(math|algebra|calc|equation)\\b\", name): return \"math\"\n",
    "    return \"chat\"\n",
    "\n",
    "\n",
    "def build_npys_per_file_with_labels(src_dir: Path, out_dir: Path, *, system_text: str = SYSTEM_PROMPT_DEFAULT, rng_seed: int = 1234):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    files = sorted([p for p in Path(src_dir).glob(\"*.jsonl\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .jsonl in {src_dir}\")\n",
    "\n",
    "    meta = []\n",
    "    file_iter = TQDM(files, desc=f\"[SCAN] {src_dir.name}\", unit=\"file\",\n",
    "                    total=len(files), bar_format=BARFMT, **TQDM_KW)\n",
    "\n",
    "\n",
    "    for p in file_iter:\n",
    "        ids_buf, mask_buf, kept_stats = [], [], []\n",
    "        dropped_scaffold = dropped_over = dropped_other = 0\n",
    "        kept = 0\n",
    "\n",
    "        for ln in iter_lines_hybrid(p, desc=\"READ\"):\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            try:\n",
    "                row = json.loads(ln)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            info = (row.get(\"info\") or \"\").strip()\n",
    "            user = (row.get(\"user\") or \"\").strip()\n",
    "            asst = (row.get(\"assistant\") or \"\").strip()\n",
    "            if not user or not asst:\n",
    "                continue\n",
    "\n",
    "            packed = row_to_ids_and_mask(system_text, info, user, asst)\n",
    "            if packed is None:\n",
    "                if RUT_PAT.search(asst or \"\"): dropped_scaffold += 1\n",
    "                else: dropped_over += 1\n",
    "                continue\n",
    "\n",
    "            ids, msk, st = packed\n",
    "            if len(ids) != MAX_LEN or len(msk) != MAX_LEN:\n",
    "                dropped_other += 1; continue\n",
    "\n",
    "            ids_buf.append(ids); mask_buf.append(msk); kept_stats.append(st); kept += 1\n",
    "\n",
    "        if kept == 0:\n",
    "            TQDM.write(f\"[SKIP] {p.name}: kept 0 rows after strict filtering.\")\n",
    "            continue\n",
    "\n",
    "        ids_arr  = np.stack(ids_buf,  axis=0)\n",
    "        mask_arr = np.stack(mask_buf, axis=0)\n",
    "\n",
    "        # deterministic per-file shuffle\n",
    "        rng = np.random.default_rng(rng_seed)\n",
    "        perm = rng.permutation(ids_arr.shape[0])\n",
    "        ids_arr, mask_arr = ids_arr[perm], mask_arr[perm]\n",
    "\n",
    "        lbl = _label_from_filename(p)\n",
    "        labels_arr = np.array([lbl]*ids_arr.shape[0], dtype=object)\n",
    "\n",
    "        out_ids   = out_dir / f\"{p.stem}.input_ids.npy\"\n",
    "        out_mask  = out_dir / f\"{p.stem}.loss_mask.npy\"\n",
    "        out_lbls  = out_dir / f\"{p.stem}.labels.npy\"\n",
    "        np.save(out_ids,  ids_arr)\n",
    "        np.save(out_mask, mask_arr)\n",
    "        np.save(out_lbls, labels_arr)\n",
    "\n",
    "        asst_lens = np.array([s[\"assistant_len\"] for s in kept_stats], dtype=np.int32)\n",
    "        ones_pct  = float(mask_arr.sum()) / mask_arr.size\n",
    "        TQDM.write(\n",
    "            f\"[WRITE] {p.name:25s} → kept={kept:,}  scaf_drop={dropped_scaffold:,}  over_drop={dropped_over:,}  other={dropped_other:,}  \"\n",
    "            f\"p50={np.percentile(asst_lens,50):.1f}  ones%={ones_pct:.2%}  out=({out_ids.name}, {out_mask.name}, {out_lbls.name})\"\n",
    "        )\n",
    "\n",
    "        meta.append(dict(file=p, label=lbl, out_ids=out_ids, out_mask=out_mask, out_labels=out_lbls, kept=kept))\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7295c96",
   "metadata": {},
   "source": [
    "# Concatinate NPY files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e14f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gather_label_pool(perfile_meta: list[dict], want_label: str):\n",
    "    ids_list, msk_list = [], []\n",
    "    for m in perfile_meta:\n",
    "        if m[\"label\"] != want_label:\n",
    "            continue\n",
    "        ids = np.load(m[\"out_ids\"],  mmap_mode=\"r\")\n",
    "        msk = np.load(m[\"out_mask\"], mmap_mode=\"r\")\n",
    "        ids_list.append(ids); msk_list.append(msk)\n",
    "    if not ids_list:\n",
    "        return np.zeros((0, MAX_LEN), dtype=np.int32), np.zeros((0, MAX_LEN), dtype=np.uint8)\n",
    "    return np.concatenate(ids_list, axis=0), np.concatenate(msk_list, axis=0)\n",
    "\n",
    "# sort by decending assistant length and take until ~target_tokens\n",
    "# deterministic via seed\n",
    "# returns indices into ids/msk\n",
    "def _select_by_token_budget(ids: np.ndarray, msk: np.ndarray, target_tokens: int, *, seed: int) -> np.ndarray:\n",
    "    if ids.shape[0] == 0 or target_tokens <= 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    tok = msk.sum(axis=1) - 1\n",
    "    order = np.argsort(-tok) # long answers first (fewer examples needed to hit budget)\n",
    "    # small randomization within same lengths to avoid bias\n",
    "    uniq_lens = {}\n",
    "    for idx in order:\n",
    "        L = int(tok[idx])\n",
    "        uniq_lens.setdefault(L, []).append(idx)\n",
    "    shuffled = []\n",
    "    for L, arr in uniq_lens.items():\n",
    "        a = np.array(arr, dtype=np.int64)\n",
    "        rng.shuffle(a)\n",
    "        shuffled.append(a)\n",
    "    order = np.concatenate(shuffled, axis=0) if shuffled else np.array([], dtype=np.int64)\n",
    "\n",
    "    picked, acc = [], 0\n",
    "    for i in order:\n",
    "        L = int(tok[i])\n",
    "        if acc + L > target_tokens and acc > 0:\n",
    "            break\n",
    "        picked.append(i); acc += L\n",
    "        if acc >= target_tokens:\n",
    "            break\n",
    "    return np.array(picked, dtype=np.int64)\n",
    "\n",
    "def build_final_four_from_perfile(\n",
    "    train_meta: list[dict],\n",
    "    valid_meta: list[dict],\n",
    "    *,\n",
    "    ratios={\"chat\":7, \"math\":2, \"science\":1}, # assistant token ratios\n",
    "    keep_all_info_train=True,\n",
    "    valid_fraction_noninfo=0.10,\n",
    "    seed=2345,\n",
    "    out_dir=Path(\"../final_npy\"),\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # pools per split\n",
    "    def _split_from_meta(meta, split_name):\n",
    "        # keep all info rows\n",
    "        info_ids, info_msk = _gather_label_pool(meta, \"info\")\n",
    "\n",
    "        # build non-info pools\n",
    "        chat_ids, chat_msk = _gather_label_pool(meta, \"chat\")\n",
    "        math_ids, math_msk = _gather_label_pool(meta, \"math\")\n",
    "        sci_ids,  sci_msk  = _gather_label_pool(meta, \"science\")\n",
    "\n",
    "        # total non-info tokens available (assistant tokens)\n",
    "        def total_tok(msk):\n",
    "            return int((msk.sum(axis=1) - 1).clip(min=0).sum())\n",
    "\n",
    "        avail = {\n",
    "            \"chat\": total_tok(chat_msk),\n",
    "            \"math\": total_tok(math_msk),\n",
    "            \"science\": total_tok(sci_msk),\n",
    "        }\n",
    "        TQDM.write(f\"[{split_name}] non-info token availability: {avail}\")\n",
    "\n",
    "        # budget for VALID (non-info only), TRAIN gets the rest (+ all info if train)\n",
    "        if split_name == \"valid\":\n",
    "            target_total_noninfo = int(round(sum(avail.values()) * valid_fraction_noninfo))\n",
    "        else:\n",
    "            # TRAIN will be built implicitly as \"all non-info minus VALID selection\"\n",
    "            target_total_noninfo = None  # not used\n",
    "\n",
    "        # compute per-label VALID token targets via ratios, capped by availability\n",
    "        def targets_for_valid(avail_tokens, ratios):\n",
    "            if target_total_noninfo is None or target_total_noninfo <= 0:\n",
    "                return {\"chat\":0,\"math\":0,\"science\":0}\n",
    "            rsum = sum(ratios.values())\n",
    "            raw = {k: int(round(target_total_noninfo * (ratios[k]/rsum))) for k in ratios}\n",
    "            # clamp by availability\n",
    "            for k in raw:\n",
    "                raw[k] = min(raw[k], avail_tokens[k])\n",
    "            # fix rounding to match total\n",
    "            diff = target_total_noninfo - sum(raw.values())\n",
    "            if diff != 0:\n",
    "                order = sorted(ratios.keys(), key=lambda k: -ratios[k])\n",
    "                j = 0\n",
    "                while diff != 0 and order:\n",
    "                    k = order[j % len(order)]\n",
    "                    cap = avail_tokens[k]\n",
    "                    if diff > 0 and raw[k] < cap:\n",
    "                        raw[k] += 1; diff -= 1\n",
    "                    elif diff < 0 and raw[k] > 0:\n",
    "                        raw[k] -= 1; diff += 1\n",
    "                    j += 1\n",
    "            return raw\n",
    "\n",
    "        valid_targets = targets_for_valid(avail, ratios)\n",
    "        if split_name == \"valid\":\n",
    "            TQDM.write(f\"[{split_name}] token targets (chat:math:science) = {valid_targets}\")\n",
    "\n",
    "        # choose VALID rows (non-info) by token budgets\n",
    "        if split_name == \"valid\":\n",
    "            vidx_chat = _select_by_token_budget(chat_ids, chat_msk, valid_targets[\"chat\"], seed=seed+1)\n",
    "            vidx_math = _select_by_token_budget(math_ids, math_msk, valid_targets[\"math\"], seed=seed+2)\n",
    "            vidx_sci  = _select_by_token_budget(sci_ids,  sci_msk,  valid_targets[\"science\"], seed=seed+3)\n",
    "\n",
    "            valid_ids = np.concatenate([\n",
    "                chat_ids[vidx_chat], math_ids[vidx_math], sci_ids[vidx_sci]\n",
    "            ], axis=0)\n",
    "            valid_msk = np.concatenate([\n",
    "                chat_msk[vidx_chat], math_msk[vidx_math], sci_msk[vidx_sci]\n",
    "            ], axis=0)\n",
    "\n",
    "            # TRAIN gets remaining + all info\n",
    "            rem_chat = np.setdiff1d(np.arange(chat_ids.shape[0]), vidx_chat, assume_unique=False)\n",
    "            rem_math = np.setdiff1d(np.arange(math_ids.shape[0]), vidx_math, assume_unique=False)\n",
    "            rem_sci  = np.setdiff1d(np.arange(sci_ids.shape[0]),  vidx_sci,  assume_unique=False)\n",
    "\n",
    "            train_ids_noninfo = np.concatenate([chat_ids[rem_chat], math_ids[rem_math], sci_ids[rem_sci]], axis=0)\n",
    "            train_msk_noninfo = np.concatenate([chat_msk[rem_chat], math_msk[rem_math], sci_msk[rem_sci]], axis=0)\n",
    "\n",
    "            # true by default since I intended to include all info rows\n",
    "            if keep_all_info_train:\n",
    "                train_ids = np.concatenate([info_ids, train_ids_noninfo], axis=0)\n",
    "                train_msk = np.concatenate([info_msk, train_msk_noninfo], axis=0)\n",
    "            else:\n",
    "                train_ids, train_msk = train_ids_noninfo, train_msk_noninfo\n",
    "\n",
    "            # final shuffle\n",
    "            rng = np.random.default_rng(seed+42)\n",
    "            if train_ids.shape[0]:\n",
    "                perm = rng.permutation(train_ids.shape[0])\n",
    "                train_ids, train_msk = train_ids[perm], train_msk[perm]\n",
    "            if valid_ids.shape[0]:\n",
    "                perm = rng.permutation(valid_ids.shape[0])\n",
    "                valid_ids, valid_msk = valid_ids[perm], valid_msk[perm]\n",
    "\n",
    "            return train_ids, train_msk, valid_ids, valid_msk\n",
    "\n",
    "        else:\n",
    "            # if we’re building TRAIN directly (without knowing VALID), just keep all (plus info),\n",
    "            # but we actually call this function only for VALID split above and reuse the remainder as TRAIN.\n",
    "            raise RuntimeError(\"Internal: call this with split_name='valid' only.\")\n",
    "\n",
    "    # build VALID from VALID meta, then TRAIN from TRAIN meta by mirroring the same process:\n",
    "    # for TRAIN/VALID as two independent directories, we need to:\n",
    "    #  - compute VALID selection from VALID directory (its own files)\n",
    "    #  - compute TRAIN selection from TRAIN directory (its own files), but we still need the 7:2:1\n",
    "    #    on the TRAIN side. we're gonna keep all info_train.jsonl and do the ratio on *non-info*; we'll do that here by pretending\n",
    "    #    valid_fraction_noninfo applies within each split directory independently.\n",
    "\n",
    "    # VALID set\n",
    "    TQDM.write(\"▶ Building VALID (token-ratio on non-info; no info forced)\")\n",
    "    # for VALID we don't keep info; we just do the selection directly from its own meta\n",
    "    # we call the helper that expects 'valid' to return both train and valid for that directory; we only need valid_* here\n",
    "    _train_ids_dummy, _train_msk_dummy, valid_ids, valid_msk = _split_from_meta(valid_meta, split_name=\"valid\")\n",
    "\n",
    "    # TRAIN set\n",
    "    TQDM.write(\"▶ Building TRAIN (token-ratio on non-info; keep all info_* from train_meta)\")\n",
    "    train_ids, train_msk, _valid_ids_dummy, _valid_msk_dummy = _split_from_meta(train_meta, split_name=\"valid\")\n",
    "\n",
    "    # save four files\n",
    "    np.save(out_dir / \"train_input_ids.npy\",  train_ids)\n",
    "    np.save(out_dir / \"train_loss_mask.npy\",  train_msk)\n",
    "    np.save(out_dir / \"valid_input_ids.npy\",  valid_ids)\n",
    "    np.save(out_dir / \"valid_loss_mask.npy\",  valid_msk)\n",
    "\n",
    "    # small report\n",
    "    def _tok_sum(msk): return int((msk.sum(axis=1) - 1).clip(min=0).sum())\n",
    "    TQDM.write(f\"[SAVE] TRAIN rows={train_ids.shape[0]:,}  tokens={_tok_sum(train_msk):,}\")\n",
    "    TQDM.write(f\"[SAVE] VALID rows={valid_ids.shape[0]:,}  tokens={_tok_sum(valid_msk):,}\")\n",
    "\n",
    "    return (out_dir / \"train_input_ids.npy\", out_dir / \"train_loss_mask.npy\",\n",
    "            out_dir / \"valid_input_ids.npy\", out_dir / \"valid_loss_mask.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac4726",
   "metadata": {},
   "source": [
    "# Orchestrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7be4f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Per-file build: TRAIN\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8325a7289f4e40bbbb501ce048fddf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[SCAN] train_jsonl:   0%|           | 0/11 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbef7a34e2654974b0da3dda9d889c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] ai2_train.jsonl:   0%|           | 0.00/564k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] ai2_train.jsonl           → kept=3,244  scaf_drop=0  over_drop=0  other=0  p50=5.0  ones%=1.45%  out=(ai2_train.input_ids.npy, ai2_train.loss_mask.npy, ai2_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8722e7aa65884fa895949ef91a3475c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] dolly_train.jsonl:   0%|           | 0.00/11.7M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] dolly_train.jsonl         → kept=12,658  scaf_drop=12  over_drop=1,576  other=0  p50=44.0  ones%=13.19%  out=(dolly_train.input_ids.npy, dolly_train.loss_mask.npy, dolly_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6007bd522043a68e7b59c32f6cb0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] gsm8k_train.jsonl:   0%|           | 0.00/5.40M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] gsm8k_train.jsonl         → kept=7,299  scaf_drop=1  over_drop=173  other=0  p50=130.0  ones%=27.44%  out=(gsm8k_train.input_ids.npy, gsm8k_train.loss_mask.npy, gsm8k_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a472b01a3022489e9626c062f1c1275c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] hendrycks_math_train.jsonl:   0%|           | 0.00/3.45M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] hendrycks_math_train.jsonl → kept=3,644  scaf_drop=0  over_drop=1,035  other=0  p50=140.0  ones%=29.63%  out=(hendrycks_math_train.input_ids.npy, hendrycks_math_train.loss_mask.npy, hendrycks_math_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa28a2c5968043c59ce215f853942b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] info_train.jsonl:   0%|           | 0.00/1.01M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] info_train.jsonl          → kept=3,000  scaf_drop=0  over_drop=0  other=0  p50=7.0  ones%=2.01%  out=(info_train.input_ids.npy, info_train.loss_mask.npy, info_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c6ce3cef5a475d8f8a3f5b5bfd7951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] numinamath_train.jsonl:   0%|           | 0.00/1.30G [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] numinamath_train.jsonl    → kept=308,895  scaf_drop=23,381  over_drop=520,466  other=0  p50=208.0  ones%=40.03%  out=(numinamath_train.input_ids.npy, numinamath_train.loss_mask.npy, numinamath_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb60431ef9c84629a0759f8e46b3e3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] oasst1_train.jsonl:   0%|           | 0.00/3.89M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] oasst1_train.jsonl        → kept=2,750  scaf_drop=18  over_drop=714  other=0  p50=143.0  ones%=30.26%  out=(oasst1_train.input_ids.npy, oasst1_train.loss_mask.npy, oasst1_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62a0716d75b4a90933681efcf0cb56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] openorca_train.jsonl:   0%|           | 0.00/1.58G [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] openorca_train.jsonl      → kept=499,315  scaf_drop=112,307  over_drop=323,909  other=0  p50=70.0  ones%=18.26%  out=(openorca_train.input_ids.npy, openorca_train.loss_mask.npy, openorca_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a9af2fa8114b85b9c639727c55c16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] sciq_train.jsonl:   0%|           | 0.00/6.32M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] sciq_train.jsonl          → kept=10,058  scaf_drop=1  over_drop=422  other=0  p50=80.0  ones%=19.51%  out=(sciq_train.input_ids.npy, sciq_train.loss_mask.npy, sciq_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3775b73fa846d5af08340b3e41f456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] svamp_train.jsonl:   0%|           | 0.00/154k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] svamp_train.jsonl         → kept=700  scaf_drop=0  over_drop=0  other=0  p50=13.0  ones%=3.07%  out=(svamp_train.input_ids.npy, svamp_train.loss_mask.npy, svamp_train.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac66fac8fe54495a5c594d0164b8f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] ultrachat_train.jsonl:   0%|           | 0.00/1.21G [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] ultrachat_train.jsonl     → kept=377,249  scaf_drop=6,478  over_drop=274,036  other=0  p50=222.0  ones%=40.24%  out=(ultrachat_train.input_ids.npy, ultrachat_train.loss_mask.npy, ultrachat_train.labels.npy)\n",
      "▶ Per-file build: VALID\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8367953b6f540f99a70fc51ce7786c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[SCAN] valid_jsonl:   0%|           | 0/11 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30713887b85947b09d92e0e27b5894d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] ai2_valid.jsonl:   0%|           | 0.00/150k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] ai2_valid.jsonl           → kept=844  scaf_drop=0  over_drop=0  other=0  p50=5.0  ones%=1.43%  out=(ai2_valid.input_ids.npy, ai2_valid.loss_mask.npy, ai2_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2dbe710a2e44bc94ca301b5d7c1804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] dolly_valid.jsonl:   0%|           | 0.00/629k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] dolly_valid.jsonl         → kept=667  scaf_drop=1  over_drop=82  other=0  p50=44.0  ones%=13.32%  out=(dolly_valid.input_ids.npy, dolly_valid.loss_mask.npy, dolly_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eae3ee6e13b430ea8043b2aedc4741a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] gsm8k_valid.jsonl:   0%|           | 0.00/972k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] gsm8k_valid.jsonl         → kept=1,288  scaf_drop=0  over_drop=31  other=0  p50=135.0  ones%=28.17%  out=(gsm8k_valid.input_ids.npy, gsm8k_valid.loss_mask.npy, gsm8k_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8722842830d4ac4b47dab56eb11b484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] hendrycks_math_valid.jsonl:   0%|           | 0.00/2.19M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] hendrycks_math_valid.jsonl → kept=2,435  scaf_drop=1  over_drop=668  other=0  p50=138.0  ones%=29.00%  out=(hendrycks_math_valid.input_ids.npy, hendrycks_math_valid.loss_mask.npy, hendrycks_math_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acaeec3314e45d2822459a5436d4155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] info_valid.jsonl:   0%|           | 0.00/33.2k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] info_valid.jsonl          → kept=100  scaf_drop=0  over_drop=0  other=0  p50=7.0  ones%=1.99%  out=(info_valid.input_ids.npy, info_valid.loss_mask.npy, info_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e510fa926a84ce89655e87d74e89c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] numinamath_valid.jsonl:   0%|           | 0.00/148k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] numinamath_valid.jsonl    → kept=42  scaf_drop=4  over_drop=54  other=0  p50=217.0  ones%=41.90%  out=(numinamath_valid.input_ids.npy, numinamath_valid.loss_mask.npy, numinamath_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6c9a37e81442849d77361883055e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] oasst1_valid.jsonl:   0%|           | 0.00/204k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] oasst1_valid.jsonl        → kept=152  scaf_drop=1  over_drop=35  other=0  p50=129.5  ones%=29.72%  out=(oasst1_valid.input_ids.npy, oasst1_valid.loss_mask.npy, oasst1_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3daf92e2634b81895ce2e328c242c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] openorca_valid.jsonl:   0%|           | 0.00/82.9M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] openorca_valid.jsonl      → kept=26,335  scaf_drop=5,844  over_drop=17,060  other=0  p50=71.0  ones%=18.31%  out=(openorca_valid.input_ids.npy, openorca_valid.loss_mask.npy, openorca_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da601c855c0a4a8db13fe1478286b7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] sciq_valid.jsonl:   0%|           | 0.00/533k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] sciq_valid.jsonl          → kept=851  scaf_drop=1  over_drop=35  other=0  p50=82.0  ones%=19.40%  out=(sciq_valid.input_ids.npy, sciq_valid.loss_mask.npy, sciq_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1724217474848b0b3b5692d0dc04cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] svamp_valid.jsonl:   0%|           | 0.00/66.0k [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] svamp_valid.jsonl         → kept=300  scaf_drop=0  over_drop=0  other=0  p50=13.0  ones%=3.17%  out=(svamp_valid.input_ids.npy, svamp_valid.loss_mask.npy, svamp_valid.labels.npy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85076484226d4915960eb16549597840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[READ] ultrachat_valid.jsonl:   0%|           | 0.00/134M [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] ultrachat_valid.jsonl     → kept=42,461  scaf_drop=721  over_drop=29,967  other=0  p50=221.0  ones%=40.11%  out=(ultrachat_valid.input_ids.npy, ultrachat_valid.loss_mask.npy, ultrachat_valid.labels.npy)\n",
      "▶ Token-ratio combine (7:2:1 by assistant tokens; keep all info_train)\n",
      "▶ Building VALID (token-ratio on non-info; no info forced)\n",
      "[valid] non-info token availability: {'chat': 11186901, 'math': 557155, 'science': 89024}\n",
      "[valid] token targets (chat:math:science) = {'chat': 842969, 'math': 251315, 'science': 89024}\n",
      "▶ Building TRAIN (token-ratio on non-info; keep all info_* from train_meta)\n",
      "[valid] non-info token availability: {'chat': 124801624, 'math': 64581301, 'science': 1015204}\n",
      "[valid] token targets (chat:math:science) = {'chat': 13772258, 'math': 4252351, 'science': 1015204}\n",
      "[SAVE] TRAIN rows=1,161,960  tokens=171,386,497\n",
      "[SAVE] VALID rows=5,209  tokens=1,182,854\n"
     ]
    }
   ],
   "source": [
    "TRAIN_JSONL_DIR = Path(\"./train_jsonl\")\n",
    "VALID_JSONL_DIR = Path(\"./valid_jsonl\")\n",
    "\n",
    "PERFILE_TRAIN = Path(\"../final_npy/perfile_train\")\n",
    "PERFILE_VALID = Path(\"../final_npy/perfile_valid\")\n",
    "FINAL_DIR     = Path(\"../final_npy\")\n",
    "\n",
    "print(\"▶ Per-file build: TRAIN\")\n",
    "train_meta = build_npys_per_file_with_labels(TRAIN_JSONL_DIR, PERFILE_TRAIN, system_text=SYSTEM_PROMPT_DEFAULT, rng_seed=2345)\n",
    "\n",
    "print(\"▶ Per-file build: VALID\")\n",
    "valid_meta = build_npys_per_file_with_labels(VALID_JSONL_DIR, PERFILE_VALID, system_text=SYSTEM_PROMPT_DEFAULT, rng_seed=3456)\n",
    "\n",
    "print(\"▶ Token-ratio combine (7:2:1 by assistant tokens; keep all info_train)\")\n",
    "train_ids_p, train_msk_p, valid_ids_p, valid_msk_p = build_final_four_from_perfile(\n",
    "    train_meta, valid_meta,\n",
    "    ratios={\"chat\":7, \"math\":2, \"science\":1},\n",
    "    keep_all_info_train=True,\n",
    "    valid_fraction_noninfo=0.10,\n",
    "    seed=2345,\n",
    "    out_dir=FINAL_DIR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8468b3",
   "metadata": {},
   "source": [
    "wait... science is 0... <- This is fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a16d05",
   "metadata": {},
   "source": [
    "# NPY sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d923d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ===\n",
      "[shape] ids=(1161960, 512) mask=(1161960, 512)\n",
      "[END token] ok=1,161,960/1,161,960 | only_one=1,161,960 | end_is_last=1,161,960\n",
      "[mask] no ones after END ✔\n",
      "[assistant span] p50=141.0  p75=230.0  p90=281.0  <=3 tokens=2.27%\n",
      "[padding] PAD only in tail ✔\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63a7cdb470d4c4a86c8634c00db8ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[AUDIT*] train_input_ids.npy:   0%|           | 0/1161960 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUDIT*] rows checked=1161960/1161960\n",
      "  bad_order=0  multi_or_zero_END=0  mask_after_END=0\n",
      "  boundary_mismatch (tag+prefix not visible near start) = 0\n",
      "  assistant_len<=3: 26387 (2.27%), rut_hits: 72 (0.01%)\n",
      "=== TRAIN OK ===\n",
      "\n",
      "=== VALID ===\n",
      "[shape] ids=(5209, 512) mask=(5209, 512)\n",
      "[END token] ok=5,209/5,209 | only_one=5,209 | end_is_last=5,209\n",
      "[mask] no ones after END ✔\n",
      "[assistant span] p50=284.0  p75=343.0  p90=352.0  <=3 tokens=4.40%\n",
      "[padding] PAD only in tail ✔\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ce3a69652c490c9adb77d4e60fa0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[AUDIT*] valid_input_ids.npy:   0%|           | 0/5209 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUDIT*] rows checked=5209/5209\n",
      "  bad_order=0  multi_or_zero_END=0  mask_after_END=0\n",
      "  boundary_mismatch (tag+prefix not visible near start) = 0\n",
      "  assistant_len<=3: 229 (4.40%), rut_hits: 0 (0.00%)\n",
      "=== VALID OK ===\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def _nfmt(x): \n",
    "    return f\"{int(x):,}\"\n",
    "\n",
    "def sanity_check_pair(ids_path: Path, mask_path: Path, *, name: str):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    ids = np.load(ids_path,  mmap_mode=\"r\")\n",
    "    msk = np.load(mask_path, mmap_mode=\"r\")\n",
    "\n",
    "    # shapes & dtypes\n",
    "    N, T = ids.shape\n",
    "    print(f\"[shape] ids={ids.shape} mask={msk.shape}\")\n",
    "    assert ids.shape == msk.shape, \"ids/mask shape mismatch\"\n",
    "    assert T == MAX_LEN, f\"seq len must be {MAX_LEN}\"\n",
    "    assert ids.dtype in (np.int32, np.int64), f\"ids dtype unexpected: {ids.dtype}\"\n",
    "    assert msk.dtype in (np.uint8, np.int8, np.int32), f\"mask dtype unexpected: {msk.dtype}\"\n",
    "\n",
    "    # mask is strictly 0/1\n",
    "    uniq = np.unique(msk)\n",
    "    assert set(uniq.tolist()) <= {0,1}, f\"mask has values beyond 0/1: {uniq}\"\n",
    "\n",
    "    # last non-PAD must be END; exactly one END per row\n",
    "    # (fast vectorized: find last non-PAD index, check it's END_ID)\n",
    "    nonpad = (ids != PAD_ID)\n",
    "    # guard for fully-padded (this should never happen...)\n",
    "    fully_pad = (~nonpad.any(axis=1))\n",
    "    assert not fully_pad.any(), \"some rows are fully PAD\"\n",
    "\n",
    "    last_idx = ids.shape[1] - 1 - np.argmax(nonpad[:, ::-1], axis=1)\n",
    "    last_tok = ids[np.arange(N), last_idx]\n",
    "    only_one_end = (ids == END_ID).sum(axis=1) == 1\n",
    "    end_is_last  = (last_tok == END_ID)\n",
    "    ok_end = only_one_end & end_is_last\n",
    "    bad_rows = (~ok_end).sum()\n",
    "    print(f\"[END token] ok={_nfmt(ok_end.sum())}/{_nfmt(N)} | only_one={_nfmt(only_one_end.sum())} | end_is_last={_nfmt(end_is_last.sum())}\")\n",
    "    assert bad_rows == 0, f\"{bad_rows} rows fail END checks\"\n",
    "\n",
    "    # mask must be zero strictly after END\n",
    "    tail_mask_sum = msk[np.arange(N)[:,None], np.clip(last_idx+1, 0, T-1)[:,None]].sum()  # just a touch\n",
    "    # vector check: any 1s after last_idx?\n",
    "    after_end_any = (msk * (np.arange(T)[None,:] > last_idx[:,None])).any(axis=1)\n",
    "    assert (~after_end_any).all(), f\"{after_end_any.sum()} rows have mask after END\"\n",
    "    print(\"[mask] no ones after END ✔\")\n",
    "\n",
    "    # assistant length stats from mask (mask counting includes END)\n",
    "    asst_len_incl_end = msk.sum(axis=1)\n",
    "    asst_len_excl_end = asst_len_incl_end - 1\n",
    "    p50 = float(np.percentile(asst_len_excl_end, 50))\n",
    "    p75 = float(np.percentile(asst_len_excl_end, 75))\n",
    "    p90 = float(np.percentile(asst_len_excl_end, 90))\n",
    "    short = (asst_len_excl_end <= 3).mean()\n",
    "    print(f\"[assistant span] p50={p50:.1f}  p75={p75:.1f}  p90={p90:.1f}  <=3 tokens={short:.2%}\")\n",
    "\n",
    "    # quick pad locality: PADs should only appear at the end\n",
    "    # (heuristic: find first PAD; ensure all later tokens are PAD)\n",
    "    first_pad = np.argmax((ids == PAD_ID), axis=1)  # returns 0 when first is PAD; handle no-PAD rows:\n",
    "    no_pad_rows = ~ (ids == PAD_ID).any(axis=1)\n",
    "    if no_pad_rows.any():\n",
    "        first_pad[no_pad_rows] = T  # sentinel: no PADs\n",
    "    pad_tail_ok = (ids == PAD_ID).sum(axis=1) == (T - first_pad)\n",
    "    assert pad_tail_ok.all(), f\"{(~pad_tail_ok).sum()} rows have PAD(s) before END region\"\n",
    "    print(\"[padding] PAD only in tail ✔\")\n",
    "\n",
    "    # deep audit\n",
    "    audit_npys(ids_path, mask_path, sample_k=N)\n",
    "\n",
    "    print(f\"=== {name} OK ===\")\n",
    "\n",
    "# run & create files\n",
    "train_ids_p = Path(\"../final_npy/train_input_ids.npy\")\n",
    "train_msk_p = Path(\"../final_npy/train_loss_mask.npy\")\n",
    "valid_ids_p = Path(\"../final_npy/valid_input_ids.npy\")\n",
    "valid_msk_p = Path(\"../final_npy/valid_loss_mask.npy\")\n",
    "\n",
    "sanity_check_pair(train_ids_p, train_msk_p, name=\"TRAIN\")\n",
    "sanity_check_pair(valid_ids_p, valid_msk_p, name=\"VALID\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
