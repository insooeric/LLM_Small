{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39eee45",
   "metadata": {},
   "source": [
    "# Cleanup Ultrachat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a91c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 69288 Columns: ['prompt', 'prompt_id', 'messages']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = Path(\"./\")\n",
    "PARQUET_IN = ROOT / \"train_parquet\" / \"ultrachat_train_1.parquet\" # (this is for testing since i was tryna resolve an error)\n",
    "OUT_JSONL = ROOT / \"train_jsonl\" / (PARQUET_IN.stem + \".jsonl\")\n",
    "OUT_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_parquet(PARQUET_IN, engine=\"pyarrow\")\n",
    "print(\"Rows:\", len(df), \"Columns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcf37b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_parse_messages(x):\n",
    "    # pyArrow scalar -> python\n",
    "    if hasattr(x, \"as_py\"):\n",
    "        try:\n",
    "            x = x.as_py()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # NumPy ndarray -> list\n",
    "    if isinstance(x, np.ndarray):\n",
    "        # structured array?\n",
    "        if x.dtype.names:  # ('content','role')\n",
    "            out = []\n",
    "            names = list(x.dtype.names)\n",
    "            for rec in x:\n",
    "                vals = rec.tolist() if hasattr(rec, \"tolist\") else list(rec)\n",
    "                d = dict(zip(names, vals))\n",
    "                out.append({\"role\": str(d.get(\"role\", \"\")).strip(),\n",
    "                            \"content\": str(d.get(\"content\", \"\")).strip()})\n",
    "            return out\n",
    "        # plain object array -> tolist()\n",
    "        x = x.tolist()\n",
    "\n",
    "    # already a list?\n",
    "    if isinstance(x, list):\n",
    "        # list of tuples? try to coerce\n",
    "        if x and isinstance(x[0], tuple):\n",
    "            out = []\n",
    "            for t in x:\n",
    "                if len(t) == 2:\n",
    "                    a, b = t\n",
    "                    a, b = (\"\" if a is None else str(a)), (\"\" if b is None else str(b))\n",
    "                    if b.lower() in {\"user\",\"assistant\"}:\n",
    "                        out.append({\"role\": b.strip().lower(), \"content\": a.strip()})\n",
    "                    elif a.lower() in {\"user\",\"assistant\"}:\n",
    "                        out.append({\"role\": a.strip().lower(), \"content\": b.strip()})\n",
    "            return out\n",
    "        # assume list of dicts\n",
    "        return x\n",
    "\n",
    "    # bytes -> string\n",
    "    if isinstance(x, (bytes, bytearray)):\n",
    "        try:\n",
    "            x = x.decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    # string -> json / ast\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"{\") and s.endswith(\"}\")):\n",
    "            try:\n",
    "                val = json.loads(s)\n",
    "                if isinstance(val, list):\n",
    "                    return val\n",
    "            except Exception:\n",
    "                try:\n",
    "                    val = ast.literal_eval(s)\n",
    "                    if isinstance(val, list):\n",
    "                        return val\n",
    "                except Exception:\n",
    "                    return []\n",
    "        return []\n",
    "\n",
    "    # sast resort: ast on str(x)\n",
    "    try:\n",
    "        val = ast.literal_eval(str(x))\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def messages_to_pairs(messages):\n",
    "    \"\"\"Pair strictly in order: user → assistant; drop incomplete tails/empties.\"\"\"\n",
    "    pairs, last_user = [], None\n",
    "    for m in messages or []:\n",
    "        if not isinstance(m, dict):\n",
    "            continue\n",
    "        role = (m.get(\"role\") or \"\").strip().lower()\n",
    "        content = (m.get(\"content\") or \"\").strip()\n",
    "        if not content:\n",
    "            continue\n",
    "        if role == \"user\":\n",
    "            last_user = content\n",
    "        elif role == \"assistant\" and last_user is not None:\n",
    "            pairs.append({\"user\": last_user, \"assistant\": content})\n",
    "            last_user = None\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7989d37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0: type=ndarray  msgs=6  pairs=3\n",
      "  sample: What are some useful tips for organizing my closet and keepi -> 1. Start by decluttering: Go through your clothes and access\n",
      "row 1: type=ndarray  msgs=6  pairs=3\n",
      "  sample: How does sitting for prolonged periods of time affect overal -> Sitting for prolonged periods of time can have negative impa\n",
      "row 2: type=ndarray  msgs=8  pairs=4\n",
      "  sample: Can you summarize the benefits of 3D printing for surgeons a -> Surgeons at the VA Puget Sound Health Care System have great\n",
      "row 3: type=ndarray  msgs=6  pairs=3\n",
      "  sample: Write a Java program that creates a singly linked list of in -> Here is one possible implementation of a singly linked list \n",
      "row 4: type=ndarray  msgs=14  pairs=7\n",
      "  sample: Offer a guide to making homemade shakshuka with tomato sauce -> Ingredients: - 1 tablespoon olive oil - 1 onion, chopped - 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(5, len(df))):\n",
    "    raw = df.iloc[i][\"messages\"] if \"messages\" in df.columns else None\n",
    "    msgs = maybe_parse_messages(raw)\n",
    "    ps = messages_to_pairs(msgs)\n",
    "    print(f\"row {i}: type={type(raw).__name__}  msgs={len(msgs)}  pairs={len(ps)}\")\n",
    "    if ps:\n",
    "        print(\"  sample:\", ps[0][\"user\"][:60].replace(\"\\n\",\" \"), \"->\", ps[0][\"assistant\"][:60].replace(\"\\n\",\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452377c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows ultrachat_train_1.parquet: 100%|██████████| 69288/69288 [00:04<00:00, 17125.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Wrote 219527 pairs → C:\\Users\\insoo\\Documents\\Personal\\Projects\\LLM\\LLM-dummy-documentation\\8_Supervised_finetuning_Data_Schema\\train_jsonl\\ultrachat_train_1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_pairs = 0\n",
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=f\"Rows {PARQUET_IN.name}\"):\n",
    "        msgs = maybe_parse_messages(getattr(row, \"messages\", None))\n",
    "        for pair in messages_to_pairs(msgs):\n",
    "            json.dump(pair, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "            total_pairs += 1\n",
    "\n",
    "print(f\"✅ Done. Wrote {total_pairs} pairs → {OUT_JSONL.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f03049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows ultrachat_train_1.parquet: 100%|██████████| 69288/69288 [00:04<00:00, 17004.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ultrachat_train_1.parquet: wrote 219527 pairs → train_jsonl\\ultrachat_train_1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows ultrachat_train_2.parquet: 100%|██████████| 69288/69288 [00:04<00:00, 16352.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ultrachat_train_2.parquet: wrote 219225 pairs → train_jsonl\\ultrachat_train_2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows ultrachat_train_3.parquet: 100%|██████████| 69289/69289 [00:04<00:00, 16318.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ultrachat_train_3.parquet: wrote 219011 pairs → train_jsonl\\ultrachat_train_3.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows ultrachat_valid_1.parquet: 100%|██████████| 23110/23110 [00:01<00:00, 15946.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ultrachat_valid_1.parquet: wrote 73149 pairs → valid_jsonl\\ultrachat_valid_1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FILES = [\n",
    "    ROOT / \"train_parquet\" / \"ultrachat_train_1.parquet\",\n",
    "    ROOT / \"train_parquet\" / \"ultrachat_train_2.parquet\",\n",
    "    ROOT / \"train_parquet\" / \"ultrachat_train_3.parquet\",\n",
    "    ROOT / \"valid_parquet\" / \"ultrachat_valid_1.parquet\",\n",
    "]\n",
    "\n",
    "for p in FILES:\n",
    "    out_path = (ROOT / (\"train_jsonl\" if \"train_\" in p.name else \"valid_jsonl\") / (p.stem + \".jsonl\"))\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.read_parquet(p, engine=\"pyarrow\")\n",
    "    total_pairs = 0\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for row in tqdm(df.itertuples(index=False), total=len(df), desc=f\"Rows {p.name}\"):\n",
    "            msgs = maybe_parse_messages(getattr(row, \"messages\", None))\n",
    "            for pair in messages_to_pairs(msgs):\n",
    "                json.dump(pair, f, ensure_ascii=False)\n",
    "                f.write(\"\\n\")\n",
    "                total_pairs += 1\n",
    "    print(f\"✅ {p.name}: wrote {total_pairs} pairs → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d041e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shards:\n",
      "  - ultrachat_train_1.jsonl\n",
      "  - ultrachat_train_2.jsonl\n",
      "  - ultrachat_train_3.jsonl\n",
      "Valid shards:\n",
      "  - ultrachat_valid_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = Path(\"./\")\n",
    "\n",
    "TRAIN_DIR = ROOT / \"train_jsonl\"\n",
    "VALID_DIR = ROOT / \"valid_jsonl\"\n",
    "\n",
    "# find ultrachat shards\n",
    "def sort_key(p):\n",
    "    m = re.search(r\"ultrachat_(?:train|valid)_(\\d+)\", p.stem)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "TRAIN_FILES = sorted(TRAIN_DIR.glob(\"ultrachat_train_*.jsonl\"), key=sort_key)\n",
    "VALID_FILES = sorted(VALID_DIR.glob(\"ultrachat_valid_*.jsonl\"), key=sort_key)\n",
    "\n",
    "OUT_TRAIN = TRAIN_DIR / \"ultrachat_train.jsonl\"\n",
    "OUT_VALID = VALID_DIR / \"ultrachat_valid.jsonl\"\n",
    "\n",
    "print(\"Train shards:\")\n",
    "for p in TRAIN_FILES: print(\"  -\", p.name)\n",
    "print(\"Valid shards:\")\n",
    "for p in VALID_FILES: print(\"  -\", p.name)\n",
    "\n",
    "OUT_TRAIN.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_VALID.parent.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90544a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(path: Path) -> int:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def merge_jsonls(in_files, out_file):\n",
    "    total = sum(count_lines(p) for p in in_files)\n",
    "    kept = skipped = 0\n",
    "    with out_file.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        pbar = tqdm(total=total, desc=f\"Merging → {out_file.name}\")\n",
    "        for src in in_files:\n",
    "            with src.open(\"r\", encoding=\"utf-8\") as fin:\n",
    "                for line in fin:\n",
    "                    pbar.update(1)\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        skipped += 1; continue\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                    except Exception:\n",
    "                        skipped += 1; continue\n",
    "                    u = (obj.get(\"user\") or \"\").strip()\n",
    "                    a = (obj.get(\"assistant\") or \"\").strip()\n",
    "                    if not (u and a):\n",
    "                        skipped += 1; continue\n",
    "                    json.dump({\"user\": u, \"assistant\": a}, fout, ensure_ascii=False)\n",
    "                    fout.write(\"\\n\")\n",
    "                    kept += 1\n",
    "        pbar.close()\n",
    "    print(f\"✅ {out_file.name}: kept {kept}, skipped {skipped}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da2ddc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging → ultrachat_train.jsonl: 100%|██████████| 657763/657763 [00:17<00:00, 37098.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ultrachat_train.jsonl: kept 657763, skipped 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging → ultrachat_valid.jsonl: 100%|██████████| 73149/73149 [00:01<00:00, 37368.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ultrachat_valid.jsonl: kept 73149, skipped 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_FILES:\n",
    "    merge_jsonls(TRAIN_FILES, OUT_TRAIN)\n",
    "else:\n",
    "    print(\"No UltraChat train shards found in\", TRAIN_DIR)\n",
    "\n",
    "if VALID_FILES:\n",
    "    merge_jsonls(VALID_FILES, OUT_VALID)\n",
    "else:\n",
    "    print(\"No UltraChat valid shards found in\", VALID_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b150a5a",
   "metadata": {},
   "source": [
    "then remove all ultrachat_train/valid_#number.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
