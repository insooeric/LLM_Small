{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd04d965",
   "metadata": {},
   "source": [
    "# Before SFT training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812378ba",
   "metadata": {},
   "source": [
    "## DIAGRAM\n",
    "**(picture is a bit small. you may have to open the picture separately)**\n",
    "<br/><br/>\n",
    "<img src=\"./sft_diagram.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5ff2e",
   "metadata": {},
   "source": [
    "that's going to be our overall plan. <br/>\n",
    "so, similar as pretraining. we're going to splice the whole thing pilot -> 8 chunks -> tail (remaining) <br/>\n",
    "for training, we will be adding SFT with LoRA and EMA <br/>\n",
    "**(TL;DR)** <br/>\n",
    "in the diagram, i've ommited some of the features to fit into the picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3ca47",
   "metadata": {},
   "source": [
    "# Before SFT training\n",
    "we'll be updating 15000 times with <br/>\n",
    "- 5% for pilot (15000 * 0.05 = 750)\n",
    "- 8 chunks ((15000 - 750) // 8 = 1781)\n",
    "- remaining 2 for tail (15000 - (750 + 8 * 1781) = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084af27",
   "metadata": {},
   "source": [
    "# In training loop\n",
    "before we talk about **LoRA** and **EMA**, let's talk about fine-tuning <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e594d",
   "metadata": {},
   "source": [
    "# Problem\n",
    "fine-tuning a full transformer with billions of parameters is\n",
    "- heavy: requires huge GPU memory\n",
    "- slow: every weight gets updated\n",
    "- risky: can overwrite useful pretrained knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efda7d8",
   "metadata": {},
   "source": [
    "# **LoRA (Low-Rank Adaptation)**\n",
    "## idea\n",
    "- instead of updating the entire weight matrix W, LoRA freezes the original W and ony learns a low-rank update <br/>\n",
    "\n",
    "let's say we have a large weight matrix W, where the shape is d x k and contains real number <br/>\n",
    "then, LoRA introduces two smaller trainable matrices: <br/>\n",
    "- W = A ⋅ B <br/>\n",
    "\n",
    "where shape of A is d x r <br/>\n",
    "and shape of B is r x k <br/>\n",
    "where r is a small rank (like 4, 8, 16, ...) <br/>\n",
    "(quick check, shape of A ⋅ B is (d x r) ⋅ (r x k), where matrices multiplecation results d x k since r cancels out each other) <br/>\n",
    "so the effective weight during training is: <br/>\n",
    "\n",
    "### Formula\n",
    "\n",
    "**W' = W + α ⋅ (A ⋅ B)** <br/>\n",
    "\n",
    "intuitively, <br/>\n",
    "- W stays frozen (keeps pretrained knowledge) <br/>\n",
    "- A and B are tiny and trainable\n",
    "- α is a scaling factor (--lora-alpha in train_sft.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dece8",
   "metadata": {},
   "source": [
    "Consequently\n",
    "- huge parameter savings since instead of trraining the full d x k matrix, we train only r x (d + k)\n",
    "- memory efficient since gradient/optimizer states only for LoRA layers\n",
    "- pluggable since we can add LoRA to specific modules (in our case qkv, out_proj, mlp.fc, mlp.proj)\n",
    "- composable since multiple LoRA adapters can be swapped in/out on top of the same base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b502afd",
   "metadata": {},
   "source": [
    "in a part of my execution and train_sft.py: <br/>\n",
    "```\n",
    "    --lora --lora-r 16 --lora-alpha 32 `\n",
    "    --lora-targets \"attn.qkv,attn.out_proj,mlp.fc,mlp.proj\" `\n",
    "```\n",
    "```\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_drop,\n",
    "        target_modules=targets,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "```\n",
    "\n",
    "that is:\n",
    "- `--lora-r 16`: rank of the low-rank decomposition\n",
    "- `--lora-alpha 32`: scaling factor for the LoRA updates\n",
    "- `--lora-targets \"attn.qkv,attn.out_proj,mlp.fc,mlp.proj\"`: applying LoRA only to attention projections and MLP layers (the big matrices)\n",
    "- `bias=\"none\"`: keep that W matrix frozen\n",
    "\n",
    "So, the base model stays mostly frozen, and only a tiny fraction (the LoRA matrices) is fine-tuned <br>\n",
    "that's shown in console as `trainable params: 2,359,296 || all params: 133,888,512 || trainable%: 1.7621` when we run the code <br/>\n",
    "as you see, we're training small number of params: `trainable params: 2,359,296`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c311a",
   "metadata": {},
   "source": [
    "### Why I used it? \n",
    "the biggest reason was training speed went better <br/>\n",
    "with out LoRA & EMA (we'll talk about this later) it used to be 180s/update<br/>\n",
    "and with LoRA only, it went down to 60s/update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c08d2",
   "metadata": {},
   "source": [
    "# LoRA in train_sft.py\n",
    "## ARGS\n",
    "```\n",
    "ap.add_argument(\"--lora\", action=\"store_true\")\n",
    "ap.add_argument(\"--lora-r\", type=int, default=16)\n",
    "ap.add_argument(\"--lora-alpha\", type=int, default=32)\n",
    "ap.add_argument(\"--lora-drop\", type=float, default=0.05)\n",
    "ap.add_argument(\"--lora-targets\", type=str, default=\"attn.qkv,attn.out_proj,mlp.fc,mlp.proj\")\n",
    "ap.add_argument(\"--lora-merge\", action=\"store_true\")\n",
    "ap.add_argument(\"--load-lora-from\", type=str, default=\"\")\n",
    "```\n",
    "\n",
    "so, yeah. other tags:\n",
    "- `--lora`: makes sure that we're using LoRA\n",
    "- `--lora-merge`: in W' = W + α ⋅ (A ⋅ B), it's that `+` sign, so that we end up with single .pt\n",
    "- `--load-lora-from`: only load LoRA adapter weights on top of base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b4721",
   "metadata": {},
   "source": [
    "## Base model Wrapper\n",
    "```\n",
    "base_model = DummyModel(cfg).to(device)\n",
    "\n",
    "if args.load_lora_from:\n",
    "    model = PeftModel.from_pretrained(base_model, args.load_lora_from, is_trainable=True).to(device)\n",
    "else:\n",
    "    targets = [t.strip() for t in args.lora_targets.split(\",\") if t.strip()]\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_drop,\n",
    "        target_modules=targets, bias=\"none\", task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    model = peft_get_peft_model(base_model, lora_cfg).to(device)\n",
    "```\n",
    "either load an existing LoRA adapter or wrap with a new LoRA config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda62e5f",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "```\n",
    "if args.lora:\n",
    "    trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(trainable, lr=BASE_LR, betas=(0.9, 0.95), eps=1e-8, fused=True or False)\n",
    "```\n",
    "if `--lora` is set, optimizer collects trainable parameters (which are the LoRA adapters) and builds AdamW just for them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096391e",
   "metadata": {},
   "source": [
    "## Save\n",
    "```\n",
    "base_sd = base_model.state_dict()\n",
    "if isinstance(model, PeftModel):\n",
    "    base_sd = _strip_lora_from_base_sd(base_sd)\n",
    "\n",
    "payload = {\"model\": base_sd, ...}\n",
    "if isinstance(model, PeftModel):\n",
    "    payload[\"lora_state\"] = get_peft_model_state_dict(model)\n",
    "torch.save(payload, path)\n",
    "```\n",
    "base weights goes under `model` but LoRA adapters are stored in `lora_state`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fddd14",
   "metadata": {},
   "source": [
    "## Load\n",
    "```\n",
    "load_state_dict_safely(model, {\"model\": raw_sd})\n",
    "if isinstance(model, PeftModel) and \"lora_state\" in ckpt:\n",
    "    set_peft_model_state_dict(model, ckpt[\"lora_state\"]) \n",
    "```\n",
    "restores base first, then restore `lora_state`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f19a7",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "```\n",
    "total = sum(p.numel() for n,p in model.named_parameters() if \".lora_\" in n)\n",
    "...\n",
    "probe_loss.backward()\n",
    "any_grad = any((p.grad is not None and p.grad.abs().sum().item() > 0)\n",
    "               for n,p in model.named_parameters() if \".lora_\" in n)\n",
    "print(\"LoRA grads flowing:\", any_grad)\n",
    "```\n",
    "after resuming, it runs a tiny forward/backward to ensure LoRA parameters receive gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e103b",
   "metadata": {},
   "source": [
    "## EMA can be LoRA-only\n",
    "```\n",
    "ema = EMA(model, beta=(args.ema ** args.ema_every), trainable_only=args.ema_trainable_only)\n",
    "```\n",
    "the EMA class supports trainable-only averaging (so, it EMA-averages LoRA adapters, not the frozen base) when `--ema-trainable-only` is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9773c",
   "metadata": {},
   "source": [
    "## Saving LoRA adapters as standalone directories\n",
    "```\n",
    "best_dir = CKPT_DIR / \"lora_best\"; model.save_pretrained(str(best_dir))\n",
    "best_ema_dir = CKPT_DIR / \"lora_best_ema\"; model.save_pretrained(str(best_ema_dir))\n",
    "final_dir = CKPT_DIR / \"lora_final\"; model.save_pretrained(str(final_dir))\n",
    "```\n",
    "\n",
    "on new best/latest/final, it writes adapters folder so that we can reuse it in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6e68d",
   "metadata": {},
   "source": [
    "# **EMA (Exponential Moving Average)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f8a10",
   "metadata": {},
   "source": [
    "## What's EMA? \n",
    "- EMA keeps a smoothed copy of the model's weight while training instead of just relying on the noisy parameters being updated each step\n",
    "\n",
    "following is the example of with/without EMA smoothing <br/>\n",
    "\n",
    "<img src=\"./ema_example1.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7714e",
   "metadata": {},
   "source": [
    "### Formula \n",
    "**θ_EMA ​= β ⋅ θ_EMA ​+ (1 − β) ⋅ θ**\n",
    "- θ: current trainable weights (in our case, LoRA params)\n",
    "- θ_EMA = shadow weights (EMA copy)\n",
    "- β: smoothing factor; usually close to 1 (i've used `--ema 0.9999`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7219c",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "- stability: raw weights can oscillate update-to-update, which EMA smooths that out\n",
    "- better evaluation: model evaluated with EMA weights often have lower val_loss and perplexity\n",
    "- regularization effect: prevents overfitting spikes in late training\n",
    "- safety net: if the live weights diverge temprarily, the EMA copy drifts more slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6399f",
   "metadata": {},
   "source": [
    "### Why I used it?\n",
    "EMA actually slows down the training <br/>\n",
    "but it outputs better results <br/>\n",
    "\n",
    "during the development, before implementing EMA, it used to be 30s/upd <br/>\n",
    "after implementing EMA, it is 50s/upd <br/>\n",
    "\n",
    "the quality of output: <br/>\n",
    "i don't know if this is caused by implementing EMA <br/>\n",
    "but i did small fine tuning (around 8k, validation loss ≈6.7) <br/>\n",
    "and for the question `\"Hello, my name is Eric. Who are you?\"` <br/>\n",
    "answers (trimmed): <br/>\n",
    "without EMA (forgot validation loss): `\", , , paradox   .              \"` <br/>\n",
    "with EMA(≈6.7): `\",          ,    helpful, me.         is name\"` <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3b1c4",
   "metadata": {},
   "source": [
    "# EMA in train_sft.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69e5cd",
   "metadata": {},
   "source": [
    "## class EMA\n",
    "```\n",
    "class EMA:\n",
    "    def __init__(self, model, beta=0.9999, trainable_only=False):\n",
    "        self.beta = beta\n",
    "        base = _unwrap_model_for_state_dict(model)\n",
    "        self.params = [(n, p) for n, p in base.named_parameters()\n",
    "                       if p.dtype.is_floating_point and (p.requires_grad if trainable_only else True)]\n",
    "        with torch.no_grad():\n",
    "            self.shadow = {n: p.detach().clone() for n, p in self.params}\n",
    "            self.back = {}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for n, p in self.params:\n",
    "            self.shadow[n].mul_(self.beta).add_(p.data, alpha=1 - self.beta)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def swap_in(self, model):\n",
    "        self.back = {n: p.detach().clone() for n, p in self.params}\n",
    "        for n, p in self.params:\n",
    "            p.data.copy_(self.shadow[n])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def swap_out(self, model):\n",
    "        for n, p in self.params:\n",
    "            p.data.copy_(self.back[n])\n",
    "        self.back = {}\n",
    "```\n",
    "\n",
    "tracks a shadow copy of parameters and updates thhem with exponential formula each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7eeb6",
   "metadata": {},
   "source": [
    "## ARGS\n",
    "```\n",
    "    ap.add_argument(\"--ema\", type=float, default=0.9999)\n",
    "    ap.add_argument(\"--ema-trainable-only\", action=\"store_true\")\n",
    "    ap.add_argument(\"--ema-every\", type=int, default=1)\n",
    "```\n",
    "\n",
    "- `--ema 0.9999`: set smoothing factor β close to 1\n",
    "- `--ema-trainable-only`: restrics EMA to LoRA params\n",
    "- `--ema-every`: update every k steps, so effective β per step is β^k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc0798",
   "metadata": {},
   "source": [
    "## Construction\n",
    "```\n",
    "ema = EMA(\n",
    "    model,\n",
    "    beta=(args.ema ** args.ema_every),   # smoothing factor (compounded if updating every k steps)\n",
    "    trainable_only=args.ema_trainable_only\n",
    ") if args.ema and args.ema > 0 else None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991a3c9",
   "metadata": {},
   "source": [
    "## Baseline evaluation uses EMA weights\n",
    "```\n",
    "if ema: ema.swap_in(model)\n",
    "val0, ppl0 = evaluate(model, valid_loader, device, amp_dtype, max_batches=5)\n",
    "if ema: ema.swap_out(model)\n",
    "```\n",
    "right after restart/start, we temporarily evaluate with EMA weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232e512",
   "metadata": {},
   "source": [
    "## During training\n",
    "```\n",
    "if ema and (update % args.ema_every == 0):\n",
    "    ema.update(model)\n",
    "```\n",
    "\n",
    "in `def train_for_updates(num_updates, tag):` after `optimizer.step()` <br/>\n",
    "update EMA on schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04584140",
   "metadata": {},
   "source": [
    "## Quick validation using EMA\n",
    "```\n",
    "if args.quick_every > 0 and update % args.quick_every == 0:\n",
    "    if ema: ema.swap_in(model)\n",
    "    vq, pq = evaluate(model, valid_loader, device, amp_dtype, max_batches=10)\n",
    "    if ema: ema.swap_out(model)\n",
    "    print(f\"[quick] upd {update}  val={vq:.4f}  ppl={pq:.2f}  (EMA)\")\n",
    "```\n",
    "\n",
    "every `--quick-every` updates, it: swap in to EMA -> evaluate -> swap out from EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53c16a",
   "metadata": {},
   "source": [
    "## Alert path (TPS drop/loss spike) uses EMA\n",
    "```\n",
    "if ema: ema.swap_in(model)\n",
    "val_loss, val_ppl = evaluate(model, valid_loader, device, amp_dtype, max_batches=None)\n",
    "if ema: ema.swap_out(model)\n",
    "```\n",
    "\n",
    "When triggering an immediate eval/checkpoint, same thing.\n",
    "it swap in to EMA -> evaluate -> swap out from EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e149e7",
   "metadata": {},
   "source": [
    "## Full validation & checkpoint uses EMA\n",
    "```\n",
    "if ema: ema.swap_in(model)\n",
    "val_loss, val_ppl = evaluate(model, valid_loader, device, amp_dtype, max_batches=None)\n",
    "if ema: ema.swap_out(model)\n",
    "```\n",
    "\n",
    "every `--eval-every` updates, same thing as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f74bb",
   "metadata": {},
   "source": [
    "# Saving special EMA checkpoints / adapters\n",
    "```\n",
    "if ema:\n",
    "    ema.swap_in(model)\n",
    "    save_checkpoint(CKPT_DIR, manifest, MANIFEST_PATH, model, optimizer, scheduler, scaler,\n",
    "                    update, val_loss, tag=\"best_ema\")\n",
    "    if hasattr(model, \"save_pretrained\"):\n",
    "        best_ema_dir = CKPT_DIR / \"lora_best_ema\"\n",
    "        model.save_pretrained(str(best_ema_dir))\n",
    "        print(f\"💾 saved LoRA (EMA) adapter → {best_ema_dir}\")\n",
    "    ema.swap_out(model)\n",
    "```\n",
    "\n",
    "when we hit new best, it does the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b871f",
   "metadata": {},
   "source": [
    "# END\n",
    "those are major updates i've done from train.py to train_sft.py <br/>\n",
    "i've done some tweaks to decrease the training time <br/>\n",
    "checkout `2_SFT_tweeks.ipyng`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-dummy-documentation (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
